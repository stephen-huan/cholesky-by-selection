
@article{behnel2011cython,
  title = {Cython: {{The Best}} of {{Both Worlds}}},
  shorttitle = {Cython},
  author = {Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
  year = {2011},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {13},
  number = {2},
  pages = {31--39},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2010.118},
  abstract = {Cython is a Python language extension that allows explicit type declarations and is compiled directly to C. As such, it addresses Python's large overhead for numerical loops and the difficulty of efficiently using existing C and Fortran code, which Cython can interact with natively.},
  keywords = {Computer programs,Cython,numerics,Programming,Python,Runtime,scientific computing,Sparse matrices,Syntactics},
  file = {/home/stephenhuan/Zotero/storage/VYHMHDPL/Behnel et al. - 2011 - Cython The Best of Both Worlds.pdf;/home/stephenhuan/Zotero/storage/2EILYWGV/5582062.html}
}

@misc{chalupka2012framework,
  title = {A {{Framework}} for {{Evaluating Approximation Methods}} for {{Gaussian Process Regression}}},
  author = {Chalupka, Krzysztof and Williams, Christopher K. I. and Murray, Iain},
  year = {2012},
  month = nov,
  number = {arXiv:1205.6326},
  eprint = {1205.6326},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1205.6326},
  abstract = {Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n\^2) space and O(n\^3) time for a dataset of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/stephenhuan/Zotero/storage/DLCZTN7I/Chalupka et al. - 2012 - A Framework for Evaluating Approximation Methods f.pdf;/home/stephenhuan/Zotero/storage/T4WCSCFK/1205.html}
}

@article{clark2018greedy,
  title = {Greedy {{Sensor Placement}} with {{Cost Constraints}}},
  author = {Clark, Emily and Askham, Travis and Brunton, Steven L. and Kutz, J. Nathan},
  year = {2018},
  month = may,
  journal = {arXiv:1805.03717 [math]},
  eprint = {1805.03717},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {The problem of optimally placing sensors under a cost constraint arises naturally in the design of industrial and commercial products, as well as in scientific experiments. We consider a relaxation of the full optimization formulation of this problem and then extend a well-established QR-based greedy algorithm for the optimal sensor placement problem without cost constraints. We demonstrate the effectiveness of this algorithm on data sets related to facial recognition, climate science, and fluid mechanics. This algorithm is scalable and often identifies sparse sensors with near optimal reconstruction performance, while dramatically reducing the overall cost of the sensors. We find that the cost-error landscape varies by application, with intuitive connections to the underlying physics. Additionally, we include experiments for various pre-processing techniques and find that a popular technique based on the singular value decomposition is often sub-optimal.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/home/stephenhuan/Zotero/storage/RERP7Q62/Clark et al. - 2018 - Greedy Sensor Placement with Cost Constraints.pdf;/home/stephenhuan/Zotero/storage/J62LJAED/1805.html}
}

@article{cohn1996neural,
  title = {Neural {{Network Exploration Using Optimal Experiment Design}}},
  author = {Cohn, David A.},
  year = {1996},
  month = aug,
  journal = {Neural Networks},
  volume = {9},
  number = {6},
  pages = {1071--1083},
  issn = {08936080},
  doi = {10.1016/0893-6080(95)00137-9},
  abstract = {Consider the problem of learning input/output mappings through exploration, e.g. learning the kinematics or dynamics of a robotic manipulator. If actions are expensive and computation is cheap, then we should explore by selecting a trajectory through the input space which gives us the most amount of information in the fewest number of steps. I discuss how results from the field of optimal experiment design may be used to guide such exploration, and demonstrate its use on a simple kinematics problem.},
  langid = {english},
  file = {/home/stephenhuan/Zotero/storage/JDJDIJ7F/Cohn - 1996 - Neural Network Exploration Using Optimal Experimen.pdf}
}

@article{eguchi2006interpreting,
  title = {Interpreting {{Kullback}}\textendash{{Leibler}} Divergence with the {{Neyman}}\textendash{{Pearson}} Lemma},
  author = {Eguchi, Shinto and Copas, John},
  year = {2006},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  series = {Special {{Issue}} Dedicated to {{Prof}}. {{Fujikoshi}}},
  volume = {97},
  number = {9},
  pages = {2034--2040},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2006.03.007},
  abstract = {Kullback\textendash Leibler divergence and the Neyman\textendash Pearson lemma are two fundamental concepts in statistics. Both are about likelihood ratios: Kullback\textendash Leibler divergence is the expected log-likelihood ratio, and the Neyman\textendash Pearson lemma is about error rates of likelihood ratio tests. Exploring this connection gives another statistical interpretation of the Kullback\textendash Leibler divergence in terms of the loss of power of the likelihood ratio test when the wrong distribution is used for one of the hypotheses. In this interpretation, the standard non-negativity property of the Kullback\textendash Leibler divergence is essentially a restatement of the optimal property of likelihood ratios established by the Neyman\textendash Pearson lemma. The asymmetry of Kullback\textendash Leibler divergence is overviewed in information geometry.},
  langid = {english},
  keywords = {Exponential connection,Information geometry,Maximum likelihood,Mixture connection,ROC curve,Testing hypotheses},
  file = {/home/stephenhuan/Zotero/storage/IBNYWAQ7/Eguchi and Copas - 2006 - Interpreting Kullbackâ€“Leibler divergence with the .pdf;/home/stephenhuan/Zotero/storage/Q3QIVJPW/S0047259X06000868.html}
}

@misc{gramacy2014local,
  title = {Local {{Gaussian}} Process Approximation for Large Computer Experiments},
  author = {Gramacy, Robert B. and Apley, Daniel W.},
  year = {2014},
  month = oct,
  number = {arXiv:1303.0383},
  eprint = {1303.0383},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {We provide a new approach to approximate emulation of large computer experiments. By focusing expressly on desirable properties of the predictive equations, we derive a family of local sequential design schemes that dynamically define the support of a Gaussian process predictor based on a local subset of the data. We further derive expressions for fast sequential updating of all needed quantities as the local designs are built-up iteratively. Then we show how independent application of our local design strategy across the elements of a vast predictive grid facilitates a trivially parallel implementation. The end result is a global predictor able to take advantage of modern multicore architectures, providing a nonstationary modeling feature as a bonus. We demonstrate our method on two examples utilizing designs with thousands of data points, and compare to the method of compactly supported covariances.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/4LSXJG23/Gramacy and Apley - 2014 - Local Gaussian process approximation for large com.pdf}
}

@misc{gramacy2015speeding,
  title = {Speeding up Neighborhood Search in Local {{Gaussian}} Process Prediction},
  author = {Gramacy, Robert B. and Haaland, Benjamin},
  year = {2015},
  month = jan,
  number = {arXiv:1409.0074},
  eprint = {1409.0074},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {Recent implementations of local approximate Gaussian process models have pushed computational boundaries for non-linear, non-parametric prediction problems, particularly when deployed as emulators for computer experiments. Their flavor of spatially independent computation accommodates massive parallelization, meaning that they can handle designs two or more orders of magnitude larger than previously. However, accomplishing that feat can still require massive computational horsepower. Here we aim to ease that burden. We study how predictive variance is reduced as local designs are built up for prediction. We then observe how the exhaustive and discrete nature of an important search subroutine involved in building such local designs may be overly conservative. Rather, we suggest that searching the space radially, i.e., continuously along rays emanating from the predictive location of interest, is a far thriftier alternative. Our empirical work demonstrates that ray-based search yields predictors with accuracy comparable to exhaustive search, but in a fraction of the time\textemdash for many problems bringing a supercomputer implementation back onto the desktop.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/AUJNMY26/Gramacy and Haaland - 2015 - Speeding up neighborhood search in local Gaussian .pdf}
}

@article{guinness2018permutation,
  title = {Permutation and {{Grouping Methods}} for {{Sharpening Gaussian Process Approximations}}},
  author = {Guinness, Joseph},
  year = {2018},
  month = oct,
  journal = {Technometrics},
  volume = {60},
  number = {4},
  eprint = {1609.05372},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {415--429},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2018.1437476},
  abstract = {Vecchia's approximate likelihood for Gaussian process parameters depends on how the observations are ordered, which can be viewed as a deficiency because the exact likelihood is permutation-invariant. This article takes the alternative standpoint that the ordering of the observations can be tuned to sharpen the approximations. Advantageously chosen orderings can drastically improve the approximations, and in fact, completely random orderings often produce far more accurate approximations than default coordinate-based orderings do. In addition to the permutation results, automatic methods for grouping calculations of components of the approximation are introduced, having the result of simultaneously improving the quality of the approximation and reducing its computational burden. In common settings, reordering combined with grouping reduces Kullback-Leibler divergence from the target model by a factor of 80 and computation time by a factor of 2 compared to ungrouped approximations with default ordering. The claims are supported by theory and numerical results with comparisons to other approximations, including tapered covariances and stochastic partial differential equation approximations. Computational details are provided, including efficiently finding the orderings and ordered nearest neighbors, and profiling out linear mean parameters and using the approximations for prediction and conditional simulation. An application to space-time satellite data is presented.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/TCT93VG4/Guinness - 2018 - Permutation and Grouping Methods for Sharpening Ga.pdf;/home/stephenhuan/Zotero/storage/U94S5XUJ/1609.html}
}

@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Computational science,Computer science,Software,Solar physics},
  file = {/home/stephenhuan/Zotero/storage/9AQD5GZT/Harris et al. - 2020 - Array programming with NumPy.pdf;/home/stephenhuan/Zotero/storage/2GETN8XH/s41586-020-2649-2.html}
}

@inproceedings{herbrich2002fast,
  title = {Fast {{Sparse Gaussian Process Methods}}: {{The Informative Vector Machine}}},
  shorttitle = {Fast {{Sparse Gaussian Process Methods}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Herbrich, Ralf and Lawrence, Neil and Seeger, Matthias},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  file = {/home/stephenhuan/Zotero/storage/BMFNUXUY/Herbrich et al. - 2002 - Fast Sparse Gaussian Process Methods The Informat.pdf}
}

@article{hunter2007matplotlib,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  year = {2007},
  month = may,
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems},
  keywords = {application development,Computer languages,Equations,Graphical user interfaces,Graphics,Image generation,Interpolation,Operating systems,Packaging,Programming profession,Python,scientific programming,scripting languages,User interfaces},
  file = {/home/stephenhuan/Zotero/storage/D6UMNCJ8/4160265.html}
}

@misc{kang2021correlationbased,
  title = {Correlation-Based Sparse Inverse {{Cholesky}} Factorization for Fast {{Gaussian-process}} Inference},
  author = {Kang, Myeongjong and Katzfuss, Matthias},
  year = {2021},
  month = dec,
  number = {arXiv:2112.14591},
  eprint = {2112.14591},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {Gaussian processes are widely used as priors for unknown functions in statistics and machine learning. To achieve computationally feasible inference for large datasets, a popular approach is the Vecchia approximation, which is an ordered conditional approximation of the data vector that implies a sparse Cholesky factor of the precision matrix. The ordering and sparsity pattern are typically determined based on Euclidean distance of the inputs or locations corresponding to the data points. Here, we propose instead to use a correlation-based distance metric, which implicitly applies the Vecchia approximation in a suitable transformed input space. The correlation-based algorithm can be carried out in quasilinear time in the size of the dataset, and so it can be applied even for iterative inference on unknown parameters in the correlation structure. The Euclidean- and correlation-based Vecchia approximations are equivalent for strictly decreasing isotropic covariances, but the correlation-based approach has two advantages for more complex settings: It can result in more accurate approximations, and it offers a simple, automatic strategy that can be applied to any covariance, even when Euclidean distance is not applicable. We demonstrate these advantages in several settings, including anisotropic, nonstationary, multivariate, and spatio-temporal processes. We also illustrate our method on multivariate spatio-temporal temperature fields produced by a regional climate model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/XYJF5FIN/Kang and Katzfuss - 2021 - Correlation-based sparse inverse Cholesky factoriz.pdf}
}

@article{kaporin1990alternative,
  title = {An Alternative Approach to Estimating the Convergence Rate of the {{CG}} Method},
  author = {Kaporin, I. E.},
  year = {1990},
  journal = {Numerical Methods and Software, Yu. A. Kuznetsov, ed., Dept. of Numerical Mathematics, USSR Academy of Sciences, Moscow},
  pages = {55--72}
}

@article{katzfuss2021general,
  title = {A {{General Framework}} for {{Vecchia Approximations}} of {{Gaussian Processes}}},
  author = {Katzfuss, Matthias and Guinness, Joseph},
  year = {2021},
  month = feb,
  journal = {Statistical Science},
  volume = {36},
  number = {1},
  pages = {124--141},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/19-STS755},
  abstract = {Gaussian processes (GPs) are commonly used as models for functions, time series, and spatial fields, but they are computationally infeasible for large datasets. Focusing on the typical setting of modeling data as a GP plus an additive noise term, we propose a generalization of the Vecchia (J. Roy. Statist. Soc. Ser. B 50 (1988) 297\textendash 312) approach as a framework for GP approximations. We show that our general Vecchia approach contains many popular existing GP approximations as special cases, allowing for comparisons among the different methods within a unified framework. Representing the models by directed acyclic graphs, we determine the sparsity of the matrices necessary for inference, which leads to new insights regarding the computational properties. Based on these results, we propose a novel sparse general Vecchia approximation, which ensures computational feasibility for large spatial datasets but can lead to considerable improvements in approximation accuracy over Vecchia's original approach. We provide several theoretical results and conduct numerical comparisons. We conclude with guidelines for the use of Vecchia approximations in spatial statistics.},
  keywords = {computational complexity,covariance approximation,directed acyclic graphs,large datasets,Sparsity,spatial statistics},
  file = {/home/stephenhuan/Zotero/storage/GF9ZFIUE/Katzfuss and Guinness - 2021 - A General Framework for Vecchia Approximations of .pdf;/home/stephenhuan/Zotero/storage/2GDUMKXU/19-STS755.html}
}

@misc{katzfuss2022scalable,
  title = {Scalable {{Bayesian}} Transport Maps for High-Dimensional Non-{{Gaussian}} Spatial Fields},
  author = {Katzfuss, Matthias and Sch{\"a}fer, Florian},
  year = {2022},
  month = feb,
  number = {arXiv:2108.04211},
  eprint = {2108.04211},
  eprinttype = {arxiv},
  primaryclass = {stat},
  institution = {{arXiv}},
  abstract = {A multivariate distribution can be described by a triangular transport map from the target distribution to a simple reference distribution. We propose Bayesian nonparametric inference on the transport map by modeling its components using Gaussian processes. This enables regularization and and uncertainty quantification of the map estimation, while still resulting in a closed-form and invertible posterior map. We then focus on inferring the distribution of a nonstationary spatial field from a small number of replicates. We develop specific transport-map priors that are highly flexible and are motivated by the behavior of a large class of stochastic processes. Our approach is scalable to high-dimensional fields due to data-dependent sparsity and parallel computations. We also discuss extensions, including Dirichlet process mixtures for flexible marginals. We present numerical results to demonstrate the accuracy, scalability, and usefulness of our methods, including an analysis of non-Gaussian climate-model output.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/8NHVURKB/Katzfuss and SchÃ¤fer - 2022 - Scalable Bayesian transport maps for high-dimensio.pdf;/home/stephenhuan/Zotero/storage/ITSD2DJJ/2108.html}
}

@article{krause2008nearoptimal,
  title = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}: {{Theory}}, {{Efficient Algorithms}} and {{Empirical Studies}}},
  shorttitle = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}},
  author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
  year = {2008},
  month = jun,
  journal = {The Journal of Machine Learning Research},
  volume = {9},
  pages = {235--284},
  issn = {1532-4435},
  abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
  file = {/home/stephenhuan/Zotero/storage/V2QNNH9L/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf}
}

@inproceedings{krause2015more,
  title = {A {{More Efficient Rank-one Covariance Matrix Update}} for {{Evolution Strategies}}},
  booktitle = {Proceedings of the 2015 {{ACM Conference}} on {{Foundations}} of {{Genetic Algorithms XIII}}},
  author = {Krause, Oswin and Igel, Christian},
  year = {2015},
  month = jan,
  series = {{{FOGA}} '15},
  pages = {129--136},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2725494.2725496},
  abstract = {Learning covariance matrices of Gaussian distributions is at the heart of most variable-metric randomized algorithms for continuous optimization. If the search space dimensionality is high, updating the covariance or its factorization is computationally expensive. Therefore, we adopt an algorithm from numerical mathematics for rank-one updates of Cholesky factors. Our methods results in a quadratic time covariance matrix update scheme with minimal memory requirements. The numerically stable algorithm leads to triangular Cholesky factors. Systems of linear equations where the linear transformation is defined by a triangular matrix can be solved in quadratic time. This can be exploited to avoid the additional iterative update of the inverse Cholesky factor required in some covariance matrix adaptation algorithms proposed in the literature. When used together with the (1+1)-CMA-ES and the multi-objective CMA-ES, the new method leads to a memory reduction by a factor of almost four and a faster covariance matrix update. The numerical stability and runtime improvements are demonstrated on a set of benchmark functions.},
  isbn = {978-1-4503-3434-1},
  keywords = {cholesky factorization,cma-es,covariance matrix adaptation,rank-one update},
  file = {/home/stephenhuan/Zotero/storage/XAC3J7RD/Krause and Igel - 2015 - A More Efficient Rank-one Covariance Matrix Update.pdf}
}

@article{kyng2016approximate,
  title = {Approximate {{Gaussian Elimination}} for {{Laplacians}}: {{Fast}}, {{Sparse}}, and {{Simple}}},
  shorttitle = {Approximate {{Gaussian Elimination}} for {{Laplacians}}},
  author = {Kyng, Rasmus and Sachdeva, Sushant},
  year = {2016},
  month = may,
  journal = {arXiv:1605.02353 [cs]},
  eprint = {1605.02353},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We show how to perform sparse approximate Gaussian elimination for Laplacian matrices. We present a simple, nearly linear time algorithm that approximates a Laplacian by a matrix with a sparse Cholesky factorization, the version of Gaussian elimination for symmetric matrices. This is the first nearly linear time solver for Laplacian systems that is based purely on random sampling, and does not use any graph theoretic constructions such as low-stretch trees, sparsifiers, or expanders. The crux of our analysis is a novel concentration bound for matrix martingales where the differences are sums of conditionally independent variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/stephenhuan/Zotero/storage/Z4NRXCLK/Kyng and Sachdeva - 2016 - Approximate Gaussian Elimination for Laplacians F.pdf;/home/stephenhuan/Zotero/storage/2NG6PYBS/1605.html}
}

@article{lecun1998gradientbased,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/home/stephenhuan/Zotero/storage/TVLD6BT3/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/home/stephenhuan/Zotero/storage/6WM3JJHE/726791.html}
}

@article{liu2020when,
  title = {When {{Gaussian Process Meets Big Data}}: {{A Review}} of {{Scalable GPs}}},
  shorttitle = {When {{Gaussian Process Meets Big Data}}},
  author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {11},
  pages = {4405--4423},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2957109},
  abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
  keywords = {Big data,Complexity theory,Computational modeling,Gaussian process regression (GPR),Ground penetrating radar,Kernel,local approximations,Predictive models,scalability,Scalability,sparse approximations,Sparse representation},
  file = {/home/stephenhuan/Zotero/storage/NXLKKUBL/Liu et al. - 2020 - When Gaussian Process Meets Big Data A Review of .pdf;/home/stephenhuan/Zotero/storage/25YP8RG7/8951257.html}
}

@article{marzouk2016introduction,
  title = {An Introduction to Sampling via Measure Transport},
  author = {Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  year = {2016},
  journal = {arXiv:1602.05023 [math, stat]},
  eprint = {1602.05023},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  pages = {1--41},
  doi = {10.1007/978-3-319-11259-6_23-1},
  abstract = {We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling---i.e., a transport map---between a complex "target" probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available, and second, when the target distribution is known only through a finite collection of samples. We show that in both settings the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization--based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to "Gaussianize" complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization--based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation--based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Statistics - Computation,Statistics - Methodology},
  file = {/home/stephenhuan/Zotero/storage/CD4D23JI/Marzouk et al. - 2016 - An introduction to sampling via measure transport.pdf;/home/stephenhuan/Zotero/storage/9B67627H/1602.html}
}

@misc{mutny2022experimental,
  title = {Experimental {{Design}} for {{Linear Functionals}} in {{Reproducing Kernel Hilbert Spaces}}},
  author = {Mutn{\'y}, Mojm{\'i}r and Krause, Andreas},
  year = {2022},
  month = may,
  number = {arXiv:2205.13627},
  eprint = {2205.13627},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {Optimal experimental design seeks to determine the most informative allocation of experiments to infer an unknown statistical quantity. In this work, we investigate the optimal design of experiments for estimation of linear functionals in reproducing kernel Hilbert spaces (RKHSs). This problem has been extensively studied in the linear regression setting under an estimability condition, which allows estimating parameters without bias. We generalize this framework to RKHSs, and allow for the linear functional to be only approximately inferred, i.e., with a fixed bias. This scenario captures many important modern applications, such as estimation of gradient maps, integrals, and solutions to differential equations. We provide algorithms for constructing bias-aware designs for linear functionals. We derive non-asymptotic confidence sets for fixed and adaptive designs under sub-Gaussian noise, enabling us to certify estimation with bounded error with high probability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory},
  file = {/home/stephenhuan/Zotero/storage/E23P9WUF/MutnÃ½ and Krause - 2022 - Experimental Design for Linear Functionals in Repr.pdf}
}

@article{pedregosa2011scikitlearn,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/home/stephenhuan/Zotero/storage/75JAG5CQ/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{quinonero-candela2005unifying,
  title = {A {{Unifying View}} of {{Sparse Approximate Gaussian Process Regression}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {65},
  pages = {1939--1959},
  issn = {1533-7928},
  abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
  file = {/home/stephenhuan/Zotero/storage/B9V6RFVE/QuiÃ±onero-Candela and Rasmussen - 2005 - A Unifying View of Sparse Approximate Gaussian Pro.pdf}
}

@book{rasmussen2006gaussian,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/home/stephenhuan/Zotero/storage/JFJMRP5T/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@book{rue2005gaussian,
  title = {Gaussian {{Markov Random Fields}}: {{Theory}} and {{Applications}}},
  shorttitle = {Gaussian {{Markov Random Fields}}},
  author = {Rue, Havard and Held, Leonhard},
  year = {2005},
  month = feb,
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/9780203492024},
  abstract = {Gaussian Markov Random Field (GMRF) models are most widely used in spatial statistics - a very active area of research in which few up-to-date reference works are available. This is the first book on the subject that provides a unified framework of GMRFs with particular emphasis on the computational aspects. This book includes extensive case-studie},
  isbn = {978-0-429-20882-9}
}

@article{schafer2020compression,
  title = {Compression, Inversion, and Approximate {{PCA}} of Dense Kernel Matrices at near-Linear Computational Complexity},
  author = {Sch{\"a}fer, Florian and Sullivan, T. J. and Owhadi, Houman},
  year = {2020},
  month = oct,
  journal = {arXiv:1706.02205 [cs, math]},
  eprint = {1706.02205},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Dense kernel matrices \$\textbackslash Theta \textbackslash in \textbackslash mathbb\{R\}\^\{N \textbackslash times N\}\$ obtained from point evaluations of a covariance function \$G\$ at locations \$\textbackslash\{ x\_\{i\} \textbackslash\}\_\{1 \textbackslash leq i \textbackslash leq N\} \textbackslash subset \textbackslash mathbb\{R\}\^\{d\}\$ arise in statistics, machine learning, and numerical analysis. For covariance functions that are Green's functions of elliptic boundary value problems and homogeneously-distributed sampling points, we show how to identify a subset \$S \textbackslash subset \textbackslash\{ 1 , \textbackslash dots , N \textbackslash\}\^2\$, with \$\textbackslash\# S = O ( N \textbackslash log (N) \textbackslash log\^\{d\} ( N /\textbackslash epsilon ) )\$, such that the zero fill-in incomplete Cholesky factorisation of the sparse matrix \$\textbackslash Theta\_\{ij\} 1\_\{( i, j ) \textbackslash in S\}\$ is an \$\textbackslash epsilon\$-approximation of \$\textbackslash Theta\$. This factorisation can provably be obtained in complexity \$O ( N \textbackslash log( N ) \textbackslash log\^\{d\}( N /\textbackslash epsilon) )\$ in space and \$O ( N \textbackslash log\^\{2\}( N ) \textbackslash log\^\{2d\}( N /\textbackslash epsilon) )\$ in time, improving upon the state of the art for general elliptic operators; we further present numerical evidence that \$d\$ can be taken to be the intrinsic dimension of the data set rather than that of the ambient space. The algorithm only needs to know the spatial configuration of the \$x\_\{i\}\$ and does not require an analytic representation of \$G\$. Furthermore, this factorization straightforwardly provides an approximate sparse PCA with optimal rate of convergence in the operator norm. Hence, by using only subsampling and the incomplete Cholesky factorization, we obtain, at nearly linear complexity, the compression, inversion and approximate PCA of a large class of covariance matrices. By inverting the order of the Cholesky factorization we also obtain a solver for elliptic PDE with complexity \$O ( N \textbackslash log\^\{d\}( N /\textbackslash epsilon) )\$ in space and \$O ( N \textbackslash log\^\{2d\}( N /\textbackslash epsilon) )\$ in time, improving upon the state of the art for general elliptic operators.},
  archiveprefix = {arXiv},
  keywords = {65F30; 42C40; 65F50; 65N55; 65N75; 60G42; 68Q25; 68W40,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/home/stephenhuan/Zotero/storage/5INEG2VG/SchÃ¤fer et al. - 2020 - Compression, inversion, and approximate PCA of den.pdf;/home/stephenhuan/Zotero/storage/KVKESGZB/1706.html}
}

@article{schafer2021sparse,
  title = {Sparse {{Cholesky}} Factorization by {{Kullback-Leibler}} Minimization},
  author = {Sch{\"a}fer, Florian and Katzfuss, Matthias and Owhadi, Houman},
  year = {2021},
  month = oct,
  journal = {arXiv:2004.14455 [cs, math, stat]},
  eprint = {2004.14455},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We propose to compute a sparse approximate inverse Cholesky factor \$L\$ of a dense covariance matrix \$\textbackslash Theta\$ by minimizing the Kullback-Leibler divergence between the Gaussian distributions \$\textbackslash mathcal\{N\}(0, \textbackslash Theta)\$ and \$\textbackslash mathcal\{N\}(0, L\^\{-\textbackslash top\} L\^\{-1\})\$, subject to a sparsity constraint. Surprisingly, this problem has a closed-form solution that can be computed efficiently, recovering the popular Vecchia approximation in spatial statistics. Based on recent results on the approximate sparsity of inverse Cholesky factors of \$\textbackslash Theta\$ obtained from pairwise evaluation of Green's functions of elliptic boundary-value problems at points \$\textbackslash\{x\_\{i\}\textbackslash\}\_\{1 \textbackslash leq i \textbackslash leq N\} \textbackslash subset \textbackslash mathbb\{R\}\^\{d\}\$, we propose an elimination ordering and sparsity pattern that allows us to compute \$\textbackslash epsilon\$-approximate inverse Cholesky factors of such \$\textbackslash Theta\$ in computational complexity \$\textbackslash mathcal\{O\}(N \textbackslash log(N/\textbackslash epsilon)\^d)\$ in space and \$\textbackslash mathcal\{O\}(N \textbackslash log(N/\textbackslash epsilon)\^\{2d\})\$ in time. To the best of our knowledge, this is the best asymptotic complexity for this class of problems. Furthermore, our method is embarrassingly parallel, automatically exploits low-dimensional structure in the data, and can perform Gaussian-process regression in linear (in \$N\$) space complexity. Motivated by the optimality properties of our methods, we propose methods for applying it to the joint covariance of training and prediction points in Gaussian-process regression, greatly improving stability and computational cost. Finally, we show how to apply our method to the important setting of Gaussian processes with additive noise, sacrificing neither accuracy nor computational complexity.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Computation},
  file = {/home/stephenhuan/Zotero/storage/CRD5BVVU/SchÃ¤fer et al. - 2021 - Sparse Cholesky factorization by Kullback-Leibler .pdf;/home/stephenhuan/Zotero/storage/IWBR5KXD/2004.html}
}

@inproceedings{seeger2003fast,
  title = {Fast {{Forward Selection}} to {{Speed Up Sparse Gaussian Process Regression}}},
  booktitle = {In {{Workshop}} on {{AI}} and {{Statistics}} 9},
  author = {Seeger, Matthias and Williams, Christopher K. I.},
  year = {2003},
  abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection motivated by active learning. We show how a large number of hyperparameters can be adjusted automatically by maximizing the marginal likelihood of the training data. Our method is essentially as fast as an equivalent one which selects the "support" patterns at random, yet has the potential to outperform random selection on hard curve fitting tasks and at the very least leads to a more stable behaviour of first-level inference which makes the subsequent gradient-based optimization of hyperparameters much easier. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods.},
  file = {/home/stephenhuan/Zotero/storage/MUVJGZ6P/Seeger and Williams - 2003 - Fast Forward Selection to Speed Up Sparse Gaussian.pdf;/home/stephenhuan/Zotero/storage/VJALLGBH/summary.html}
}

@inproceedings{smola2000sparse,
  title = {Sparse {{Greedy Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Smola, Alex and Bartlett, Peter},
  year = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  file = {/home/stephenhuan/Zotero/storage/L5EKJK8D/Smola and Bartlett - 2000 - Sparse Greedy Gaussian Process Regression.pdf}
}

@article{spantini2018inference,
  title = {Inference via Low-Dimensional Couplings},
  author = {Spantini, Alessio and Bigoni, Daniele and Marzouk, Youssef},
  year = {2018},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {19},
  number = {1},
  pages = {2639--2709},
  issn = {1532-4435},
  abstract = {We investigate the low-dimensional structure of deterministic transformations between random variables, i.e., transport maps between probability measures. In the context of statistics and machine learning, these transformations can be used to couple a tractable "reference" measure (e.g., a standard Gaussian) with a target measure of interest. Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map. Yet characterizing such a map--e.g., representing and evaluating it--grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings, induced by transport maps that are sparse and/or decomposable. Our analysis not only facilitates the construction of transformations in high-dimensional settings, but also suggests new inference methodologies for continuous non-Gaussian graphical models. For instance, in the context of nonlinear state-space models, we describe new variational algorithms for filtering, smoothing, and sequential parameter inference. These algorithms can be understood as the natural generalization--to the non-Gaussian case--of the square-root Rauch-Tung-Striebel Gaussian smoother.},
  keywords = {graphical models,joint parameter and state estimation,sparsity,state-space models,transport map,variational inference},
  file = {/home/stephenhuan/Zotero/storage/5G26SH3N/Spantini et al. - 2018 - Inference via low-dimensional couplings.pdf}
}

@article{stein2002screening,
  title = {The Screening Effect in {{Kriging}}},
  author = {Stein, Michael L.},
  year = {2002},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {30},
  number = {1},
  pages = {298--323},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1015362194},
  abstract = {When predicting the value of a stationary random field at a location x in some region in which one has a large number of observations, it may be difficult to compute the optimal predictor. One simple way to reduce the computational burden is to base the predictor only on those observations nearest to x. As long as the number of observations used in the predictor is sufficiently large, one might generally expect the best predictor based on these observations to be nearly optimal relative to the best predictor using all observations. Indeed, this phenomenon has been empirically observed in numerous circumstances and is known as the screening effect in the geostatistical literature. For linear predictors, when observations are on a regular grid, this work proves that there generally is a screening effect as the grid becomes increasingly dense. This result requires that, at high frequencies, the spectral density of the random field not decay faster than algebraically and not vary too quickly. Examples demonstrate that there may be no screening effect if these conditions on the spectral density are violated.},
  keywords = {60G25,62M20,62M40,asymptotics,best linear prediction,Random field,regular variation,self-affine,Self-similar},
  file = {/home/stephenhuan/Zotero/storage/FT6CMLRT/Stein - 2002 - The screening effect in Kriging.pdf;/home/stephenhuan/Zotero/storage/GGXC4YK4/1015362194.html}
}

@article{stein20112010,
  title = {2010 {{Rietz}} Lecture: {{When}} Does the Screening Effect Hold?},
  shorttitle = {2010 {{Rietz}} Lecture},
  author = {Stein, Michael L.},
  year = {2011},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {6},
  pages = {2795--2819},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/11-AOS909},
  abstract = {When using optimal linear prediction to interpolate point observations of a mean square continuous stationary spatial process, one often finds that the interpolant mostly depends on those observations located nearest to the predictand. This phenomenon is called the screening effect. However, there are situations in which a screening effect does not hold in a reasonable asymptotic sense, and theoretical support for the screening effect is limited to some rather specialized settings for the observation locations. This paper explores conditions on the observation locations and the process model under which an asymptotic screening effect holds. A series of examples shows the difficulty in formulating a general result, especially for processes with different degrees of smoothness in different directions, which can naturally occur for spatial-temporal processes. These examples lead to a general conjecture and two special cases of this conjecture are proven. The key condition on the process is that its spectral density should change slowly at high frequencies. Models not satisfying this condition of slow high-frequency change should be used with caution.},
  keywords = {60G25,62M15,62M30,fixed-domain asymptotics,kriging,Spaceâ€“time process,spectral analysis},
  file = {/home/stephenhuan/Zotero/storage/FTG5T79S/Stein - 2011 - 2010 Rietz lecture When does the screening effect.pdf;/home/stephenhuan/Zotero/storage/4SLZTSFR/11-AOS909.html}
}

@article{tropp2006algorithms,
  title = {Algorithms for Simultaneous Sparse Approximation. {{Part I}}: {{Greedy}} Pursuit},
  shorttitle = {Algorithms for Simultaneous Sparse Approximation. {{Part I}}},
  author = {Tropp, Joel A. and Gilbert, Anna C. and Strauss, Martin J.},
  year = {2006},
  month = mar,
  journal = {Signal Processing},
  volume = {86},
  number = {3},
  pages = {572--588},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2005.05.030},
  abstract = {A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection.},
  langid = {english},
  file = {/home/stephenhuan/Zotero/storage/DH9HWWNA/Tropp et al. - 2006 - Algorithms for simultaneous sparse approximation. .pdf}
}

@article{tropp2007signal,
  title = {Signal {{Recovery From Random Measurements Via Orthogonal Matching Pursuit}}},
  author = {Tropp, Joel A. and Gilbert, Anna C.},
  year = {2007},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {53},
  number = {12},
  pages = {4655--4666},
  issn = {1557-9654},
  doi = {10.1109/TIT.2007.909108},
  abstract = {This paper demonstrates theoretically and empirically that a greedy algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover a signal with \$m\$ nonzero entries in dimension \$d\$ given \$ \textbackslash rm O(m \textbackslash ln d)\$ random linear measurements of that signal. This is a massive improvement over previous results, which require \$\textbackslash rm O(m\^2)\$ measurements. The new results for OMP are comparable with recent results for another approach called Basis Pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.},
  keywords = {Algorithms,approximation,basis pursuit,Blood,compressed sensing,Compressed sensing,Greedy algorithms,group testing,Matching pursuit algorithms,Mathematics,orthogonal matching pursuit,Performance evaluation,Reliability theory,Signal processing,signal recovery,sparse approximation,Testing,Vectors},
  file = {/home/stephenhuan/Zotero/storage/9XKDSFHX/Tropp and Gilbert - 2007 - Signal Recovery From Random Measurements Via Ortho.pdf;/home/stephenhuan/Zotero/storage/AIPX7H8X/4385788.html}
}

@article{vecchia1988estimation,
  title = {Estimation and {{Model Identification}} for {{Continuous Spatial Processes}}},
  author = {Vecchia, A. V.},
  year = {1988},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {50},
  number = {2},
  pages = {297--312},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1988.tb01729.x},
  abstract = {Formal parameter estimation and model identification procedures for continuous domain spatial processes are introduced. The processes are assumed to be adequately described by a linear model with residuals that follow a second-order stationary Gaussian random field and data are assumed to consist of noisy observations of the process at arbitrary sampling locations. A general class of two-dimensional rational spectral density functions with elliptic contours is used to model the spatial covariance function. An iterative estimation procedure alleviates many of the computational difficulties of conventional maximum likelihood estimation for non-lattice data. The procedure is applied to several generated data sets and to an actual ground-water data set.},
  langid = {english},
  keywords = {anisotropy,gaussian process,measurement error,non-lattice data,spatial correlation,spatial regression},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1988.tb01729.x},
  file = {/home/stephenhuan/Zotero/storage/TFBI2YZI/Vecchia - 1988 - Estimation and Model Identification for Continuous.pdf;/home/stephenhuan/Zotero/storage/5UGP2IT8/j.2517-6161.1988.tb01729.html}
}

@article{virtanen2020scipy,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Biophysical chemistry,Computational biology and bioinformatics,Technology},
  file = {/home/stephenhuan/Zotero/storage/YSJMPS3T/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf}
}

@inproceedings{wada2013gaussian,
  title = {Gaussian {{Process Regression}} with {{Dynamic Active Set}} and {{Its Application}} to {{Anomaly Detection}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Data Science}} ({{ICDATA}})},
  author = {Wada, Toshikazu and Matsumura, Yuki and Maeda, Shunji and Shibuya, Hisae},
  year = {2013},
  pages = {7},
  abstract = {Gaussian Process Regression (GPR) can be defined as a linear regression in high-dimensional space, where low-dimensional input vectors are projected by a nonlinear high-dimensional mapping. Same as other kernel based methods, kernel function is introduced instead of computing the mapping directly. This regression can be regarded as an example based regression by identifying the kernel function with the similarity measure of two vectors. Based on this interpretation, we show that GPR can be accelerated and its memory consumption can be reduced while keeping the accuracy by dynamically forming the active set depending on the given input vector, where active set is the set of examples used for the regression. We call this method Dynamic Active Set (DAS). Based on DAS, we can extend the standard GPR, which estimates a scalar output with variance, to a regression method to estimate multidimensional output with covariance matrix. We applied our method to anomaly detection on real power plant and confirmed that it can detect prefault phenomena four days before actual fault alarm.},
  langid = {english},
  file = {/home/stephenhuan/Zotero/storage/3KQGV78M/Wada et al. - Gaussian Process Regression with Dynamic Active Se.pdf}
}

@article{yeremin2000factorized,
  title = {Factorized Sparse Approximate Inverse Preconditionings. {{III}}. {{Iterative}} Construction of Preconditioners},
  author = {Yeremin, A. Y. and Kolotilina, L. Y. and Nikishin, A. A.},
  year = {2000},
  month = sep,
  journal = {Journal of Mathematical Sciences},
  volume = {101},
  number = {4},
  pages = {3237--3254},
  issn = {1573-8795},
  doi = {10.1007/BF02672769},
  abstract = {This paper presents new results of the theoretical study of factorized sparse approximate inverse (FSAI) preconditionings. In particular, the effect of the a posteriori Jacobi scaling and the possibility of constructing FSAI preconditioners iteratively are analyzed. A simple stopping criterion for the termination of local iterations in constructing approximate FSAI preconditioners using the PCG method is proposed. The results of numerical experiments with 3D finite-element problems from linear elasticity are presented. Bibliography21 titles.},
  langid = {english},
  keywords = {Conjugate Gradient Method,Diagonal Entry,Frobenius Norm,Local Iteration,Sparsity Pattern},
  file = {/home/stephenhuan/Zotero/storage/IU3N22JS/Yeremin et al. - 2000 - Factorized sparse approximate inverse precondition.pdf}
}


