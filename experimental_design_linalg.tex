% SIAM Article Template

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf 
\hypersetup{
  pdftitle={m-calculus},
  pdfauthor={S Huan, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  experimental design for linear algebra 
\end{abstract}

% REQUIRED
\begin{keywords}
   \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}
\cite{schafer2017compression} test citation

\section{Greedy selection for directed inference}

\subsection{Conditional \textit{k}-th nearest neighbors}

Consider the simple regression algorithm \( k \)th-nearest neighbors (\( k
\)-NN). Given a training set \( X_\text{Tr} = \{ \vec{x}_1, \dots, \vec{x}_n
\} \) and corresponding labels \( \vec{y}_\text{Tr} = \{ y_1, \dots, y_n \}
\), the goal is to estimate the unknown label \( y_\text{Pr} \) of some unseen
prediction point \( \vec{x}_\text{Pr} \) Stated informally, the \( k \)-NN
approach is to select the \( k \) points in \( X_\text{Tr} \) \emph{most
informative} about \( \vec{x}_\text{Pr} \) and combine their results.

\begin{algorithm}
  \caption{Idealized \( k \)-NN regression}
  Given \( (X_\text{Tr}, \vec{y}_\text{Tr}) = 
        \{ (\vec{x}_1, y_1), \dots, (\vec{x}_n, y_n) \} \) 
  and \( \vec{x}_\text{Pr} \)
  \begin{enumerate}
    \item Select the \( k \) points in \( X_\text{Tr} \)
      most informative about \( \vec{x}_\text{Pr} \)
    \item Combine the labels of the selected points to generate a prediction
  \end{enumerate}
\end{algorithm}

One specific approach is intuitively, points close to \( \vec{x}_\text{Pr}
\) should be similar to it. So we select the \( k \) closest points in \(
X_\text{Tr} \) to \( \vec{x}_\text{Pr} \) and pool their labels (e.g., by
averaging).

\begin{algorithm}
  \caption{\( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dots, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) closest to \( \vec{x}_\text{Pr} \) 
    \item Compute \( \vec{y}_\text{Pr} \) by 
      \( \frac{1}{k} \sum_{j = 1}^k y_{i_j} \)
  \end{enumerate}
\end{algorithm}

However, we can generalize the notion of ``closest'' with the \emph{kernel
trick}, by using an arbitrary kernel function to measure similarity. For
example, commonly used kernels like the Gaussian kernel and Mat{\'e}rn family
of covariance functions are \emph{isotropic}; they depend only on the distance
between the two vectors. If such isotropic kernels monotonically decrease with
distance, then selecting points based on the largest kernel similarity recovers
\( k \)-NN. However, kernels need not be isotropic in general --- they just
need to capture some sense of ``similarity'', motivating kernel \( k \)-NN.

\todo{not sure whether ``stationary'' or
``isotropic'' are the right word(s) to use here}

\begin{algorithm}
  \caption{Kernel \( k \)-NN regression}
  Given kernel function \( K(\vec{x}, \vec{y}) \) 
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dots, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) most similar to \( \vec{x}_\text{Pr} \)
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by similarity
  \end{enumerate}
\end{algorithm}

Although the kernel \( k \)-NN approach is more general than its normed
counterpart, it still suffers from a fundamental issue. Suppose the closest
point to \( \vec{x}_\text{Pr} \) has many duplicates in the training set. Then
the algorithm will select the same point multiple times, even though in some
sense the duplicate point has stopped giving additional information about the
prediction point. In order to fix this issue, we should be selecting new points
\emph{conditional} on the points we've already selected. This preserves the
idealized algorithm of selecting points based on the information they tell us
about the prediction point --- once we've selected a point, conditioning on
it reduces the information similar points tells us, encouraging diverse point
selection.

\begin{algorithm}
  \caption{\textcolor{lightblue}{Conditional} kernel \( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dots, \vec{x}_{i_k} 
      \}  \subseteq X_\text{Tr} \) most \textcolor{lightblue}{informative} to
      \( \vec{x}_\text{Pr} \) \\ \textcolor{lightblue}{after conditioning on
      all points selected beforehand}
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by
      \textcolor{lightblue}{information}
  \end{enumerate}
\end{algorithm}

In order to make the notions of conditioning and information precise, we need a
specific framework. Kernel methods lead naturally to Gaussian processes, whose
covariance matrices naturally result from kernel functions and allows us to use
the rigorous statistical and information-theoretic notions of conditioning and
information.

\todo{mention sensor placement/spatial statistics perspective/literature}

\subsection{Sparse Gaussian process regression}
\label{subsec:gp_reg}

A \emph{Gaussian process} is a prior distribution over functions, such that
for any finite set of points, the corresponding function over the points
is distributed according to a multivariate Gaussian. In order to generate
such a distribution over an uncountable number of points consistently, a
Gaussian process is specified by a \emph{mean function} \( \mu(\vec{x}) \) and
\emph{covariance function} or \emph{kernel function} \( K(\vec{x}, \vec{y})
\). For any finite set of points \( X = \{ \vec{x}_1, \dots, \vec{x}_n \}
\), \( f(X) \sim \mathcal{N}(\vec{\mu}, \Theta) \), where \( \vec{\mu}_i =
\mu(\vec{x}_i) \) and \( \Theta_{ij} = K(\vec{x}_i, \vec{x}_j) \).

In order to compute a prediction at \( \vec{x}_\text{Pr} \), we can
simply condition the desired prediction \( \vec{y}_\text{Pr} \) on the
observed outputs and compute the conditional expectation. We can also
find the conditional variance, which will quantify the uncertainty of
our prediction. If we block our covariance matrix
\( \Theta = 
  \begin{pmatrix} 
    \Theta_{\text{Tr}, \text{Tr}} & \Theta_{\text{Tr}, \text{Pr}} \\
    \Theta_{\text{Pr}, \text{Tr}} & \Theta_{\text{Pr}, \text{Pr}}
  \end{pmatrix}
\) 
where \( \Theta_{\text{Tr}, \text{Tr}}, \Theta_{\text{Pr}, \text{Pr}},
\Theta_{\text{Tr}, \text{Pr}}, \Theta_{\text{Pr}, \text{Tr}} \) are the
covariance matrices of the training data, testing data, and training and test
data respectively, then the conditional expectation and covariance are:
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &= 
    \vec{\mu}_\text{Pr} + 
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1} 
    (\vec{y}_\text{Tr} - \vec{\mu}_\text{Tr}) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &=
    \Theta_{\text{Pr}, \text{Pr}} - 
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1} 
    \Theta_{\text{Tr}, \text{Pr}}
\end{align}
Note that calculating the posterior mean and variance requires inverting the
training covariance matrix \( \Theta_{\text{Tr}, \text{Tr}} \), which costs
\( \mathcal{O}(N^3) \), where \( N \) is the number of training points. This
scaling is prohibitive for large datasets, so many \emph{sparse} Gaussian
process regression techniques have been proposed. These methods often focus
on selecting a subset of the training data that is most informative about the
prediction points, which naturally aligns with our \( k \)-NN perspective.
If \( s \) points are selected out of the \( N \), then the inversion will
cost \( \mathcal{O}(s^3) \), which could be substantially cheaper if \( s \)
is significantly smaller than \( N \). The question is then how to select as
few points as possible while maintaining predictive accuracy.

\todo{cite sparse Gaussian regression papers}

\subsection{Problem: optimal selection}

The natural criterion justified from the \( k \)-NN perspective is to maximize
the \emph{mutual information} between the selected points and the target point
for prediction. Such information-theoretic objectives have seen success in the
spatial statistics community \cite{krause2008sensor}, who use such criteria to
determine the best locations to place sensors in a Gaussian process regression
context. The mutual information, or \emph{information gain} is defined as
\begin{align}
  \label{eq:info}  
  \I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}] &= \entropy[\vec{y}_\text{Pr}] - 
    \entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]
\end{align}
We can use the fact that the entropy of a multivariate Gaussian is
monotonically increasing with the log determinant of its covariance matrix to
efficiently compute these entropies. Because the entropy of \( \vec{y}_\text{Pr}
\) is constant, maximizing the mutual information is equivalent to minimizing
the conditional entropy. From \cref{eq:cond_cov} we see that minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix. Note that for a single predictive point, this
is monotonic with its variance. So another justification is that we are
reducing the \emph{conditional variance} of the desired point as much as
possible. In particular, because our estimator is the conditional expectation
\cref{eq:cond_mean}, it is unbiased because \( \E[\E[\vec{y}_\text{Pr} \mid
\vec{y}_\text{Tr}]] = \E[\vec{y}_\text{Pr}] \). Because it is unbiased, its
expected mean squared error is simply the conditional variance since \(
\E[(\vec{y}_\text{Pr} - \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}])^2 \mid
\vec{y}_\text{Tr}] = \Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] \) where
the expectation is taken under conditioning because of the assumption that
\( \vec{y}_\text{Pr} \) is distributed according to the Gaussian process. So
maximizing the mutual information is equivalent to minimizing the conditional
variance which is in turn equivalent to minimizing the expected mean squared
error of the prediction. Another perspective on the objective can be derived
from comparing the mutual information to the EV-VE identity, which states
\begin{align*}
  \textcolor{orange}{\entropy[\vec{y}_\text{Pr}]} &= 
    \textcolor{lightblue}{\entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]} +
    \textcolor{rust}{\I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}]} \\
  \textcolor{orange}{\Var[\vec{y}_\text{Pr}]} &= 
    \textcolor{lightblue}{\E[\Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]} 
\end{align*}
On the left hand side, entropy is monotone with variance. On the right hand
side, the expectation of the conditional variance can be interpreted to be
the fluctuation of the prediction point after conditioning, and is monotone
with the conditional entropy. Because the expectation of conditional variance
and variance of conditional expectation add to a constant, minimizing the
expectation of the conditional variance is equivalent to maximizing the
variance of conditional expectation, which we see corresponds to the mutual
information term. Supposing \( \vec{y}_\text{Pr} \) was independent of \(
\vec{y}_\text{Tr} \), then the conditional expectation becomes simply the
expectation, whose variance is 0. Thus, the variance of the conditional
expectation can be interpreted to be the ``information'' shared between \(
\vec{y}_\text{Pr} \) and \( \vec{y}_\text{Tr} \), as the larger it is, the
more the prediction for \( \vec{y}_\text{Pr} \) (the conditional expectation)
depends on the observed results of \( \vec{y}_\text{Tr} \).

\subsection{A greedy approach}
\label{subsec:greedy}

We now consider how to efficiently minimize the conditional variance
objective using a greedy approach. At each iteration, we pick the training
point which most reduces the conditional variance of the prediction
point. Let \( I = \{ i_1, i_2, \dots, i_j \} \) be the set of indexes of
training points selected already. Let the prediction point have index
\( n \), the last index. For a candidate index \( k \), we update the
covariance matrix after conditioning on \( y_k \), in addition to the
indices already selected according to \cref{eq:cond_cov}:
\begin{align}
  \nonumber
  \Theta_{:, : \mid I, k} &= \Theta_{:, : \mid I} - 
    \Theta_{:, k \mid I} \Theta_{k, k \mid I}^{-1} \Theta_{k, : \mid I} \\
  \label{eq:cond_select}
                          &= \Theta_{:, : \mid I} - 
    \frac{\Theta_{:, k \mid I} \Theta_{:, k \mid I}^{\top}}{\Theta_{kk \mid I}}
\end{align}

We see that conditioning on a new entry is a rank-one update on the current
covariance matrix \( \Theta_{\mid I} \), given by the vector \( \vec{u} =
\frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk \mid I}}} \). Thus, the amount
that the variance of \( \vec{y}_\text{Pr} \) will decrease after selecting
\( k \) is given by \( u_n^2 \), or
\begin{align}
  \label{eq:obj_gp}
  \frac{\Cov[\vec{y}_\text{Pr}, \vec{y}_{\text{Tr},k}]^2}
       {\Var[\vec{y}_{\text{Tr}, k}, \vec{y}_{\text{Tr}, k}]} &=  
  \frac{\Theta_{nk \mid I}^2}{\Theta_{kk \mid I}}              
\end{align}
For each candidate \( k \), we need to keep track of its conditional variance
and conditional covariance with the prediction point after conditioning on
the points already selected to compute \cref{eq:obj_gp}. We then simply
choose the candidate with the largest decrease in predictive variance. To
keep track of the conditional variance and covariance, we can simply start
with the initial values given by \( \Theta_{kk} \) and \( \Theta_{nk} \) and
update after selecting an index \( j \). We compute \( \vec{u} \) for \( j
\) directly according to \cref{eq:cond_cov} and update \( k \)'s conditional
variance by subtracting \( u_k^2 \) and update its conditional covariance
by subtracting \( u_k u_n \).

In order to efficiently compute \( \vec{u} \), we rely on two main
strategies. The direct method is to keep track of \( \Theta_{I, I}^{-1} \)
and update the inverse every time a new index is added to \( I \). This
can be done efficiently with Schur complementation in \( \mathcal{O}(s^2)
\). Once \( \Theta_{I, I}^{-1} \) has been computed, \( \vec{u} \) is
computed trivially according to \cref{eq:cond_cov}. For each of the \(
s \) rounds of selection, it takes \( s^2 \) to update the inverse, and
costs \( Ns \) to compute \( \vec{u} \), costing \( \mathcal{O}(N s^2 +
s^3) = \mathcal{O}(N s^2) \) overall.

The second approach is to take advantage of the quotient rule of Schur
complementation. Stated statistically, the quotient rule states that
conditioning on \( I \) and then conditioning on \( J \) is the same as
conditioning on \( I \cup J \). Because computing the Cholesky factorization
of \( \Theta \) involves sequence of rank-one updates to \( \Theta \),
Cholesky factorization can be viewed as iterative conditioning, where
the \( i \)th column of the Cholesky factor corresponds precisely to the
corresponding \( \vec{u} \) for \( i \) since a iterative sequence of
conditioning on \( i_1, i_2 \dots \) is equivalent to conditioning on \(
I \) by the quotient rule. The Cholesky factorization can be efficiently
computed without excess dependence on \( N \) with left-looking, so the
conditioning only happens when we need it. For each of the \( s \) rounds
of selection, it costs \( \mathcal{O}(Ns) \) to compute the next column of
the Cholesky factorization, for a total cost of \( \mathcal{O}(Ns^2) \),
matching the time complexity of the explicit inverse approach.

\todo{psuedocode}

\begin{algorithm}
  \caption{Point selection with explicit inverse}
  \label{alg:gp_select}
  \begin{algorithmic}[1]
    \REQUIRE \( \Theta \)
    \ENSURE \( L \)
  \end{algorithmic} 
\end{algorithm}

While both approaches have the same time complexity, the explicit inverse
algorithm uses \( \mathcal{O}(s^2) \) space to store the inverse while the
Cholesky factorization uses \( \mathcal{O}(N s) \) to store the first \( s \)
columns of the Cholesky factorization of \( \Theta \), which is always more
memory than the inverse (\( N > s \)). Both algorithms use an additional \(
\mathcal{O}(N) \) space to store the conditional variances and covariances.

\subsection{Supernodes and blocked selection}

We now consider how to efficiently deal with multiple prediction points. The
first question is how to generalize the previous objective for a single point
\cref{eq:obj_gp} to multiple points. Following the same mutual information
justification as before, a natural criterion is to minimize the log determinant
of the prediction points' covariance matrix after conditioning on the selected
points, or \( \logdet(\Theta_{\text{Pr}, \text{Pr} \mid I}) \). This objective,
known as D-optimal \cite{krause2008sensor}, has many intuitive interpretations
--- for example, as the volume of the region of uncertainty or as the scaling
factor in the density function for the Gaussian process.

We now need to be able to efficiently compute the effect of selecting an
index \( k \) on the log determinant. From \cref{eq:cond_select}, we know
that selecting an index is a rank-one update on the prediction covariance.
Using the matrix determinant lemma,
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet
  \left (
    \Theta_{\text{Pr}, \text{Pr} \mid I \cup \{ k \}}
  \right ) &= \logdet
    \left (
      \Theta_{\text{Pr}, \text{Pr} \mid I} -
      \frac{\Theta_{\text{Pr}, k \mid I}
            \Theta_{\text{Pr}, k \mid I}^{\top}
           }{\Theta_{kk \mid I}}
    \right ) \\
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log 
    \left (
      1 -
      \frac{\Theta_{\text{Pr}, k \mid I}^{\top} 
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \shortintertext{Focusing on the second term, we can turn
    the quadratic form into conditioning:}
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log 
    \left (
      \frac{\Theta_{kk \mid I} -
            \Theta_{k, \text{Pr} \mid I} 
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \label{eq:greedy_mult}
  \shortintertext{By the quotient rule, we combine the conditioning:}
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log 
    \left (
      \frac{\Theta_{kk \mid I, \text{Pr}}}{\Theta_{kk \mid I}}
    \right )
\end{align}
The greedy objective \cref{eq:greedy_mult} tells us that to minimize the
log determinant, we can simply select the index \( k \) with the smallest
ratio between the conditional variance after conditioning on the previously
selected points as well as the prediction points, and the conditional
variance after just conditioning on the selected points. Intuitively
this tells us that we can place sensors \emph{backwards}, where we imagine
placing sensors at the \emph{prediction points} instead of the candidates.
We then measure the conditional variance at a candidate, and pick the 
candidate whose conditional variance decreases the most (relative from
what it started out as). Intuitively, these candidates are likely to give
information about the prediction points, because the prediction points
give information about the candidate.

Re-writing the objective in this way also gives an efficient algorithm to
compute the necessary quantities. We condition on the prediction points
essentially the same as described in \cref{subsec:greedy}, by simply
maintaining two structures instead of one, one for the conditional variance
after conditioning on the previously selected points, and the other for the
conditional variance after also conditioning on the prediction points. By
the quotient rule, the order of conditioning does not matter as long as the
order is consistent. For the second structure, we therefore condition on
the prediction points \emph{first} before any points have been selected. We
again have two strategies, one which explicitly maintains inverses and the
other which relies on Cholesky factorization.

For the inverse algorithm, using \cref{eq:cond_cov} directly, for \(
m \) prediction points it costs \( \mathcal{O}(m^3) \) to compute \(
\Theta_{\text{Pr}, \text{Pr}}^{-1} \) and then \( \mathcal{O}(N m^2) \)
to compute \( \Theta_{kk \mid \text{Pr}} \) for the \( N \) candidates \(
k \). For each of the \( s \) rounds of selecting candidates, it costs \(
s^2 \) and \( m^2 \) to update the inverses \( \Theta_{I, I}^{-1} \) and \(
\Theta_{\text{Pr}, \text{Pr}}^{-1} \) respectively, where \( \Theta_{\text{Pr},
\text{Pr}}^{-1} \) is efficiently updated after the rank-one update in
\cref{eq:obj_gp_mult} with the Sherman–Morrison–Woodbury formula. Given
the inverses, \( \vec{u} = \frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk
\mid I}}} \) and \( \vec{u}_\text{Pr} = \frac{\Theta_{:, k \mid I,
\text{Pr}}}{\sqrt{\Theta_{kk \mid I, \text{Pr}}}} \) are computed as usual
according to \cref{eq:cond_cov} in time \( Ns \) and \( Nm \). Finally,
for each candidate \( j \) the conditional variance \( \Theta_{jj \mid
I} \) is updated by subtracting \( u_j^2 \), the conditional covariance
\( \Theta_{\text{Pr}, k \mid I} \) is updated for each prediction point
index \( c \) each by subtracting \( u_j u_c \), and the conditional
variance \( \Theta_{jj \mid I, \text{Pr}} \) is updated by subtracting
\( {u_\text{Pr}}_j^2 \). The total time complexity after simplification
is \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two Cholesky factorizations are stored. We first
compute the Cholesky factorization after selecting each prediction point,
for a cost of \( (n + m) m \) for each of the \( m \) columns. We then begin
selecting candidates, which requires updating both Cholesky factors in time \(
(n + m)(m + s) \) which is dominated by updating the preconditioned Cholesky
factor. The columns of the Cholesky factors correspond precisely to \( \vec{u}
\) and \( \vec{u}_\text{Pr} \) and both conditional variances \( \Theta_{jj
\mid I} \) and \( \Theta_{jj \mid I, \text{Pr}} \) can be computed as above.
The conditional covariances do not need to be computed. Over \( s \) rounds
the total time complexity is \( \mathcal{O}((N + m) m^2 + s(N + m)(m + s)) \)
which simplifies to \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

Although both approaches have the same time complexity, like the single
point case they differ in memory usage. The explicit inverse requires \(
\mathcal{O}(s^2 + m^2) \) memory to store both inverses, as well as \(
\mathcal{O}(N m) \) memory to store the conditional covariances. The Cholesky
algorithm, on the othe rhand, requires \( \mathcal{O}((n + m)(m + s)) \)
to store the first \( m + s \) columns of the Cholesky factorization of
the joint covariance matrix between training and prediction points, which
simplifies to \( \mathcal{O}(N s + N m + m^2) \). Comparing the memory usage,
they are the same except for \( s^2 \) versus \( N s \), so the Cholesky
algorithm again uses more memory than the explicit inverse algorithm.

\todo{psuedocode}

\begin{algorithm}
  \caption{Multiple prediction point selection with explicit inverse}
  \label{alg:gp_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \Theta \)
    \ENSURE \( L \)
  \end{algorithmic} 
\end{algorithm}

\subsection{Near optimality by submodularity}

\section{Greedy selection for \emph{global} approximation by KL-minimization}

We have a covariance matrix \( \Theta \) and wish to compute the Cholesky
factorization of \( \Theta \) into a lower triangular factor \( L \) such that
\( \Theta = L L^{\top} \). \todo{justify importance/downstream applications
of Cholesky factorization}. This can be done in \( \mathcal{O}(N^3) \) with
standard algorithms, which is often prohibitive. Recall the problem of
inference in Gaussian process regression as described in \cref{subsec:gp_reg}
also took \( \mathcal{O}(N^3) \) to invert the covariance matrix \( \Theta
\). Thus, similar to Guassian process regression, we will use \emph{sparsity}
to mitigate the computational cost. In fact, we will be able to re-use our
previous algorithms \cref{alg:gp_select,alg:gp_mult} on each column of the
Cholesky factorization.

We will first compute the Cholesky factorization of \( \Theta^{-1} \),
also known as the \emph{precision matrix}, and use the resulting sparse
factorization to efficiently compute an approximation for \( \Theta
\). Because the precision matrix encodes the distribution of the full
conditionals, the \( (i, j) \)th entry of the precision matrix is 0 if and
only if the variables \( x_i \) and \( x_j \) are conditionally independent,
conditional on the rest of the variables. Thus, the precision matrix \(
\Theta^{-1} \) can be sparse as a result of conditional independence even
if the original covariance matrix \( \Theta \) is dense. It therefore
makes sense to attempt to approximately ``sparsify'' \( \Theta^{-1} \)
instead of \( \Theta \) with iterated conditioning.

Because of sparsity, we can only get an approximate Cholesky factor \( L \),
\( \hat{L} \) belonging to a pre-specified sparsity pattern \( S \) --- a set
of (row, column) indices that are allowed to be nonzero. In order to measure
the performance of the estimator, we treat the matrices as covariance matrices
of centered Gaussian processes (mean \( \vec{0} \)). In order to compare the
resulting distributions, we use the \emph{KL divergence} according to
\cite{schafer2020sparse}, or the expected difference in log-densities:
\begin{align}
  \label{eq:L_obj} 
  L \coloneqq \argmin_{\hat{L} \in S} \, \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})
  \right ) 
\end{align}

Note that here we are computing the Cholesky factorization
of \( \Theta^{-1} \). Surprisingly enough, it is possible to
exactly compute \( L \). First, we re-write the KL divergence:
\begin{align}
  \label{eq:KL}  
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right ) 
  = \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
\end{align}

\begin{theorem}
  \label{thm:L}
  \cite{schafer2020sparse}. 
  The non-zero entries of the \( i \)th
  column of \( L \) in \cref{eq:L_obj} are:
  \begin{align}
    \label{eq:L_col} 
    L_{s_i, i} = \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
  \end{align}
\end{theorem}

Plugging \( L \) \cref{eq:L_col} back into
the KL divergence \cref{eq:KL}, we obtain:
\begin{align}
 \nonumber
  \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= -\logdet(L L^{\top}) - \logdet(\Theta)
  \shortintertext{Because  \( L \) is lower triangular, its
    determinant is just the product of its diagonal entries:}
  \nonumber
  &= -2 \sum_{i = 1}^N \log(L_{ii}) - \logdet(\Theta) 
  \shortintertext{Plugging \cref{eq:L_col} for its diagonal entry,}
  \nonumber
  &= -\sum_{i = 1}^N \log(\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1) 
    - \logdet(\Theta) \\
  \label{eq:obj_chol}
  &= \sum_{i = 1}^N 
    \log
    \left (
      (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
    \right ) 
    - \logdet(\Theta) 
\end{align}

In order to maximize \cref{eq:obj_chol}, we can maximize over each column
independently, since each term in the sum only depends on a single
column. We want to minimize \( (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1}
\vec{e}_1)^{-1} \), the term corresponding to the diagonal entry in the
inverse of the submatrix of \( \Theta \) corresponding to the entries
we've taken. We can give this value statistical interpretation by taking
advantage of Schur complementation:
\begin{align}
  \label{eq:inverse_cond}
  \Theta_{\text{Pr}, \text{Pr} \mid \text{Tr}} &=
    ((\Theta^{-1})_{\text{Pr}, \text{Pr}})^{-1}
  \shortintertext{where \( \Theta \) is blocked according to}
  \label{eq:blocking}
  \Theta &= 
  \begin{pmatrix} 
    \Theta_{\text{Tr}, \text{Tr}} & \Theta_{\text{Tr}, \text{Pr}} \\
    \Theta_{\text{Pr}, \text{Tr}} & \Theta_{\text{Pr}, \text{Pr}}
  \end{pmatrix}
  \shortintertext{Thus, we see that}
  \nonumber
  (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= ((\Theta_{s_i, s_i}^{-1})_{11})^{-1} \\
         &= \Theta_{ii \mid s_i - \{ i \}}
\end{align}

So our objective on each column is to minimize the conditional variance of
the \( i \)th variable, conditional on the entries we've selected --- \( s_i
\) contains \( i \) to begin with, so \( s_i - \{ i \} \) is the selected
entries. We can therefore use algorithm \cref{alg:gp_select} directly on each
column, where the prediction point is the \( i \) variable and the number
of points selected is the number of nonzero entries per column. The only
difference is that the candidates is limited to indices lower than \( i \),
that is, candidate indices \( k \) such that \( k > i \) to maintain the
lower triangularity of \( L \). Once \( s_i \) has been computed for each \(
i \), \( L \) can be constructed according to \cref{thm:L}. Each column costs
\( \mathcal{O}(s^3) \) to compute \( \Theta_{s_i, s_i}^{-1} \) for a total
cost of \( \mathcal{O}(N s^3) \) for the \( N \) columns of \( L \).

\subsection{Aggregated sparsity pattern}

We can also use the Gaussian process regression viewpoint to efficiently
aggregate multiple columns, that is, to use the same sparsity pattern for
multiple columns. We denote aggregating the column indices \( i_1, \dots,
i_m \) into the same group as \( \tilde{i} = \{i_1, i_2, \dots i_m \} \),
letting \( s_{\tilde{i}} = \bigcup_{i \in \tilde{i}} s_i \) be the aggregated
sparsity pattern, and letting \( \tilde{s} = s_{\tilde{i}} - \tilde{i} \)
be the set of selected entries excluding the diagonal entries. Each \( s_i
= \tilde{s} \cup \, \{ j \in \tilde{i} \mid j \geq i \} \), that is, the
sparsity pattern of the \( i \) column is the selected entries plus all
the diagonal entries lower than it. We will enforce that all the selected
entries, excluding the indices of the diagonals of the columns themselves,
are below the lowest index so that indices are not selected ``partially''
--- that is, an index could be above some indices in the aggregated columns,
and therefore invalid to add to their column, but below others. That is, we
restrict the candidate indices \( k > \max \tilde{i} \) so that the selected
index can be added to each column in \( \tilde{i} \) without violating the
lower triangularity of \( L \). We now show that the KL-minimization objective
on the aggregated indices corresponds precisely to \cref{eq:obj_gp_mult},
the objective multiple point Gaussian regression with the chain rule of log
determinant through Schur complementation.
\begin{align}
  \label{eq:det_chain}
  \logdet(\Theta) &= \logdet(\Theta_{\text{Pr}, \text{Pr} \mid \text{Tr}}) + 
    \logdet(\Theta_{\text{Tr}, \text{Tr}})
  \shortintertext{where \( \Theta \) is blocked according to
    \cref{eq:blocking}. The KL-divergence objective for \( \tilde{i} \) is:}
  \nonumber
  \sum_{i \in \tilde{i}} \log(\Theta_{ii \mid s_i - \{ i \} })
  &= \log(\Theta_{i_m i_m \mid \tilde{s}}) + 
     \log(\Theta_{i_{m - 1} i_{m - 1} \mid \tilde{s} + \{ i_m \}}) + 
     \dots + \\
  \nonumber
  &= \logdet(\Theta_{\{ i_m, i_{m - 1} \}, \{ i_m, i_{m - 1} \}
             \mid \tilde{s}}) + \dots \\
  \label{eq:obj_mult}
  &= \logdet(\Theta_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}

We see that the objective \cref{eq:obj_mult} is equivalent to the
objective \cref{eq:obj_gp_mult}, that is, to minimize the log
determinant of the conditional covariance matrix corresponding to
a set of prediction points, conditional on the selected entries.
We can therefore directly use \cref{alg:gp_mult} on the aggregated
columns, where the prediction points correspond to indices in the
aggregation and where we restrict the candidates \( k \) to those
below each column in the aggregation, \( k > \max \tilde{i} \).

\todo{pseudocode, \( U \) and computing submatrices}

Hence the sparse Cholesky factorization motivated by KL divergence can be
viewed as the sparse Gaussian process regression over each column, where
entries are selected to maximize mutual information with the entry on the
diagonal of the current column.

The explicit inverse algorithm computes \( \Theta_{11}^{-1} \),
which can be directly used to generate the columns of \( L \)
according to \cref{eq:L_col} without extra computational cost.

\subsection{Review of KL approximation}

\section{Numerical experiments} 

\section*{Acknowledgments}
%\todo{add more funding information}
This research was supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA.

\bibliographystyle{siamplain}
\bibliography{references}

\appendix

\todo{add proofs, if any, in appendix}

\end{document}
