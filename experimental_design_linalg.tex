% SIAM Article Template

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Sparse Cholesky factorization by greedy conditional selection},
  pdfauthor={S. Huan, J. Guinness, M. Katzfuss, H. Owhadi, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Dense kernel matrices resulting from pairwise evaluations of a
  kernel function arise naturally in machine learning and statistics.
  Previous work in constructing sparse transport maps or sparse approximate
  inverse Cholesky factors of such matrices by minimizing Kullback-Leibler
  divergence recovers the Vecchia approximation for Gaussian processes.
  However, these methods often rely only on geometry to construct the
  sparsity pattern, ignoring the conditional effect of adding an entry.
  In this work, we construct the sparsity pattern by leveraging a
  greedy selection algorithm that maximizes mutual information with
  target points, conditional on all points selected previously.
  For selecting \( k \) points out of \( N \), the naive time
  complexity is \( \BigO(N k^4) \), but by maintaining a
  partial Cholesky factor we reduce this to \( \BigO(N k^2) \).
  Furthermore, for multiple (\( m \)) targets we achieve a time complexity
  of \( \BigO(N k^2 + N m^2 + m^3) \) which is maintained in the setting of
  aggregated Cholesky factorization where a selected point need not condition
  every target.
  We directly apply the selection algorithm to image
  classification and recovery of sparse Cholesky
  factors, improving upon \( k \)-th nearest neighbors.
  By minimizing Kullback-Leibler divergence, we apply the
  algorithm to Cholesky factorization, Gaussian process
  regression, and preconditioning with the conjugate gradient.
\end{abstract}

% REQUIRED
\begin{keywords}
  \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}

\paragraph{The problem}

Gaussian processes find application 
in spatial statistics and geostatistics \cite{rue2005gaussian}, machine
learning through kernel machines \cite{rasmussen2006gaussian}, and optimal
experimental design \cite{mutny2022experimental}, e.g. in sensor placement
\cite{krause2008nearoptimal}. 
However, Gaussian process statistics from $N$ datapoints with requires computing with the 
covariance matrix $\CM \in \Reals^{N \times N}$ 
to obtain quantities such as \( \CM \vec{v} \),
\( \CM^{-1} \vec{v} \), \( \logdet(\CM) \). 
For dense $\CM$, directly computing these quantities has a computational cost of \( \BigO(N^3) \) and a memory cost of \( \BigO(N^2) \), which is prohibitively expensive for $N \gtrapprox 10^5$.  
Beyond Gaussian processes, computations with large positive-definite matrices are required accross computational mathematics, motivating the search for faster, approximate algorithms.

\paragraph{Existing work}

\todo{I would put knothe rosenblatt, FSAI, and sparse Cholesky factorization into the Vecchia paragraph and use the "existing work" paragraph to disucuss other approaches such as inducing points, wavelet method, h matrices, fast multiplol, random features, Nystrom approximation, sparse Cholesky of covariance, etc.}
Computation with positive-definite matrices is often accelerated with sparse
Cholesky factorization, either through incomplete Cholesky factorization
of the covariance \( \CM \) \cite{schafer2020compression, chenparallel,
chow2015finegrained}, sparse approximate Cholesky factorization of the
precision \( \CM^{-1} \) \cite{schafer2021sparse}, or factorized sparse
approximate inverse (FSAI) preconditioners \cite{kaporin1990alternative,
yeremin2000factorized}. From the perspective of transport maps, the Cholesky
factor \( L \) satisfying \( \CM^{-1} = L^{\top} L \) can be viewed as a linear
transport map mapping an arbitrary multivariate Gaussian distribution \(
\vec{x} \sim \mathcal{N}(\vec{0}, \CM) \) to the standard Gaussian \( \vec{z}
\sim \mathcal{N}(\vec{0}, I_N) \) through \( \vec{z} = L \vec{x} \). Thus,
one generalization of Cholesky factorization to nonlinear and non-Gaussian
distributions is the Knothe-Rosenblatt rearrangement, which preserves lower
triangularity, monotonicity, and sparsity, particularly in the inverse mapping
\cite{spantini2018inference}. These triangular transport maps have been
applied to problems in spatial statistics, particularly for simulation and
sampling problems \cite{marzouk2016introduction, katzfuss2022scalable}.

% not sure this is necessary
% In the special case of the Mat{\'e}rn kernel function with half-integer
% smoothness on one-dimensional points, the covariance matrix \( \CM \) admits
% a sparse, banded factorization, allowing for an exact algorithm linear
% in both time and space with the number of points, and only additionally
% depending on the smoothness parameter \( \nu \) \cite{chen2022kernel}.

\paragraph{Vecchia approximation}

\todo{I would make this one integrated section on Vecchia, FSAI, KL-Cholesky, and transport maps}

Many of these existing approaches can be viewed through the lens of an early
approach for fast Gaussian process regression, the Vecchia approximation
\cite{vecchia1988estimation, katzfuss2021general}. The key observation is a
decomposition of the joint likelihood \( \pi \).
\begin{align}
  \label{eq:joint}
  \p(\vec{x}) &= \p(x_1) \p(x_2 \mid x_1) \dots
    \p(x_N \mid x_1, x_2, \dotsc, x_{N - 1})
  \shortintertext{Assuming that there are many points, the key assumption
    is that many of the points contribute little additional information,
    conditional on a carefully chosen subset of the points. Letting \(
    s_i \) denote the indices of training points to include for the \(
    i \)th variable, the Vecchia approximation proposes to approximate
    \cref{eq:joint} by the sparse approximation}
  \label{eq:vecchia}
  \p(\vec{x}) &\approx \p(x_1) \p(x_2 \mid x_{s_2}) \dots
    \p(x_N \mid x_{s_N})
\end{align}
From the Vecchia approximation, the joint likelihood can be factored into
\( N \) independent regression problems, where each regression problem is
to approximate the conditional distribution of the \( i \)th variable.
Independence allows for embarrassingly parallel algorithms, which is exploited
both in Gaussian process regression \cite{katzfuss2021general} and Cholesky
factorization \cite{schafer2021sparse}. The difference is that while the
Gaussian process perspective emphasizes local regression problems and sparsity
in point selection (or variable interaction), Cholesky factorization emphasizes
minimizing global functionals and sparsity in the resulting matrix factor.
If an appropriate functional is chosen and a sparsity pattern \( \SpSet \)
specified, then it is an optimization problem to determine the entries of
the matrix factor. Multiple functionals in the literature actually converge
to the same closed-form expression for the entries of the resulting Cholesky
factor \( L \) subject to the constraint that \( L \in \SpSet \): Minimizers
of the Kaporin condition number \( (\trace(L \CM L^{\top})/N)^N/\det(L \CM
L^{\top}) \) \cite{kaporin1990alternative}, minimizes of \( \norm{\Id - L
\chol(\CM)}_{\FRO} \) additionally subject to the constraint \( \diag(L
\CM L^{\top}) = 1 \) \cite{yeremin2000factorized}, and minimizers of the
KL-divergence \( \KL{\N(\vec{0}, \CM)} {\N(\vec{0}, (L L^{\top})^{-1})}
\) \cite{schafer2021sparse}. This shared closed-form expression for the
entries of the resulting factor can be shown to be equivalent to the
formula used to compute the Vecchia approximation for Gaussian processes
\cite{vecchia1988estimation}. In addition, the KL-divergence is used to
compute sparse lower-triangular maps in \cite{marzouk2016introduction,
katzfuss2022scalable}.

The primary difference between the numerical linear algebra viewpoint (Kaporin
conditional number or Frobenius norm minimization) and the statistical
viewpoint (KL-minimization) is whether the covariance is factored, \( L
\approx \chol(\CM)^{-1} \), or the precision, \( L \approx \chol(\CM^{-1}) \).
As observed in \cite{schafer2021sparse, spantini2018inference}, factors of
the precision are often much sparser than factors of the covariance, because
the precision encodes conditional independence while the covariance encodes
marginal independence. Covariance matrices arising from kernel functions
are often fully dense, but the approximate factors of their precision can
be sparse if the ordering and sparsity pattern are chosen carefully.

\paragraph{Sparsity selection by geometry}

\todo{Might want to add a quick common explaining the maximin ordering here?}
Assuming that a suitable elimination ordering is chosen, it remains to
be decided the sparsity set for each point. Indices are often chosen
to be the closest points to the current point by Euclidean distance
\cite{vecchia1988estimation, schafer2020compression, schafer2021sparse,
katzfuss2022scalable}. This choice can be justified by noting that
popular kernel functions like the Mat{\'e}rn family of kernel functions
decay exponentially with increasing distance. For more general kernel
functions, geostatisticians have long observed the ``screening effect'',
or the observation that conditional on points close to the point
of interest, far away points are nearly conditionally independent
\cite{stein2002screening, stein20112010}.

% not sure this is necessary
% Possible manifestations of this effect include the observed linearity of the
% marginal log-likelihood \cite{bartels2022adaptive}, approximate submodularity
% of common information-theoretic criteria \cite{das2011submodular,
% jagalur-mohan2021batch}, and exponential decay in error for Cholesky
% factorization \cite{schafer2020compression, schafer2021sparse}.

However, selecting by distance alone ignores the conditional effect of adding
new points to the sparsity set. As an illustrative example, imagine the closest
point to the point of interest has been duplicated multiple times. Once the
duplicated point has been selected, conditional on the selected point, the
duplicates provide no additional information. But they are still closest to
the point of interest, so selecting by distance alone would still add these
redundant points to the sparsity set. Instead, we propose greedily selecting
based on conditional mutual information with the point of interest.

\paragraph{Conditional selection}

\todo{We should start with something like "In this work we propose a conditional .." or so. That is, first make it clear that applying conditional selection is the main contribution of the pres4ent work and thenm relate it to all the other approaches / techniques.}
The machine learning community has long developed algorithms that greedily
select the next point to include by maximizing an information-theoretic
objective \cite{smola2000sparse, herbrich2002fast, seeger2003fast}.
Similar algorithms have been developed in the context of sensor placement
\cite{krause2008nearoptimal, clark2018greedy} and experimental design
\cite{mutny2022experimental} where it is assumed the target phenomena
is modeled by a Gaussian process or is otherwise linearly dependent
on the selected measurements. More recent work exploits the empirical
observation that the marginal log-likelihood is approximately linear,
giving a principled way to determine the number of selected points
without viewing the whole dataset \cite{bartels2022adaptive}.

Our proposed selection method can be viewed as the covariance equivalent of
a popular algorithm in signal processing and compressive sensing, orthogonal
matching pursuit (OMP) \cite{tropp2007signal, tropp2006algorithms}, which
seeks to approximate a target vector as the sparse linear combination from a
given collection of vectors. OMP is a workhorse algorithm used in as diverse
contexts as signal recovery \cite{tropp2007signal, tropp2006algorithms},
polynomial chaos expansion \cite{baptista2019greedy}, and the use of neural
networks to solve partial differential equations \cite{hao2021efficient,
siegel2022optimal}.

Nearly all of these selection methods leverage numerical linear algebra
for efficient computation, particularly the Cholesky factorization
\cite{herbrich2002fast, seeger2003fast, bartels2022adaptive} or the
closely related QR factorization \cite{clark2018greedy, tropp2007signal,
baptista2019greedy}. We wish to emphasize that algorithms for Cholesky
factorization translate naturally to Gaussian processes regression and
vice-versa.

\paragraph{Main results}

\todo{Want to start a one-dentence descrption of our contributions, something like" We propose ...". We shoudl first fully describe what we are doing (greedy selection, integration into the KL framework etc.) We want to start talking about technical things like the computational costs and the improvement only afterrward, once we have estaglished what it is that we are accelerating.}
For a single point of interest, a direct computation of the conditional
mutual information criterion would have a computational time complexity of \(
\BigO(N k^4) \) to greedily select \( k \) points out of \( N \) total, but by
maintaining a partial Cholesky factor, we are able to reduce this complexity
to \( \BigO(N k^2) \). We extend this basic selection algorithm to maximize
mutual information with \emph{multiple} points of interest, to naturally take
advantage of the ``two birds with one stone'' effect. For \( m \) target points
we achieve a time complexity of \( \BigO(N k^2 + N m^2 + m^3) \), which for
\( m \approx k \) is essentially \( m \) times faster than the single-point
algorithm. In the setting of aggregated (or supernodal) Cholesky factorization
where the sparsity pattern for multiple columns is determined at once, a
candidate entry may be between two columns --- above one, but below another.
Adding this entry only conditions a \emph{partial} subset of the target points.
By carefully applying rank-one downdating of Cholesky factors, we are able to
capture this structure at the same time complexity for multiple points.

Greedy selection \emph{locally} infers the posterior distribution for
distinguished point(s) of interest. In order to get a \emph{global}
approximation for the entire Gaussian process, we compute a sparse approximate
Cholesky factor of the precision by minimizing the KL-divergence between the
centered multivariate normal distributions with covariance matrices \( \CM \)
and \( (L L^{\top})^{-1} \) subject to the constraint \( L \in \SpSet \) as
in \cite{schafer2021sparse}. We show that the resulting optimization problem
reduces to independent regression problems for each column. In applying our
greedy selection method, we are able to get more accurate Cholesky factors
for the same number of nonzeros compared to if the sparsity was selected with
geometry. Finally, we show how to adaptively determine the number of nonzeros
per column in order to reduce the global KL-divergence by maintaining a global
priority queue shared between each local greedy selection algorithm.

\paragraph{Outline}

\Stodo{todo once more of the paper is solidified}

The remainder of the paper is organized as follows.

\Stodo{put images in the introduction?}

\begin{figure}[h]
  \centering
  \input{figures/screening/uncond.tex}%
  \input{figures/screening/cond.tex}
  \caption{An illustration of the screening effect with the Mat{\'e}rn
    kernel with a length scale of 1 and smoothness \( \nu = \frac{5}{2}
    \). The first figure shows the unconditional covariance with the point
    at (0, 0). The second figure shows the conditional covariance after
    conditioning on the four points in orange.}
  \label{fig:screen}
\end{figure}

\begin{figure}[h]
  \centering
  \input{figures/selection/knn.tex}%
  \input{figures/selection/cknn.tex}
  \caption{Here, the \textcolor{lightblue}{blue} points are the
    \textcolor{lightblue}{candidates}, or training points, the
    \textcolor{orange}{orange} point is the \textcolor{orange}{unknown} point,
    or the point to make a prediction at, and the \textcolor{seagreen}{green}
    points are the \( k \) \textcolor{seagreen}{selected} points. The
    \textcolor{rust}{red} line is the \textcolor{rust}{conditional mean}
    \( \mu \), conditional on the \( k \) selected points, and the \( \mu
    - 2 \sigma \) to \( \mu + 2 \sigma \) confidence interval is shaded
    for the conditional variance \( \sigma^2 \).}
  \label{fig:select}
\end{figure}

\begin{figure}[h]
  \centering
  \input{figures/cholesky_factor.tex}%
  % TODO: matching GP image with 13 candidates, 3 selected, 1 training point
  \input{figures/selection/cknn.tex}
  \caption{For a column in isolation, the \textcolor{orange}{unknown} point
    is the diagonal entry, below it are \textcolor{lightblue}{candidates},
    and the \textcolor{seagreen}{selected} entries are added to the sparsity
    pattern \( s_i \). Thus, sparsity selection in Cholesky factorization
    is analogous to point selection in Gaussian processes.}
  \label{fig:select_chol}
\end{figure}

\Ftodo{"specific" and "intuitively" are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much as
possible. It's normal to add them out of reflex initially, so it requires
active postprocessing.}

\section{Greedy selection for directed inference}
\label{sec:select}

We motivate the greedy selection algorithm in the concrete setting of Gaussian
process regression. We say the function \( f(\vec{x}) \) is distributed
according to a Gaussian process prior with mean function \( \mu(\vec{x}) \)
and covariance function or kernel function \( \K(\vec{x}, \vec{x}') \), if for
any finite set of points \( X = \{ \vec{x}_i \}^N_{i = 1} \), \( f(X) \sim
\N(\vec{\mu}, \CM) \), where \( \mu_i = \mu(\vec{x}_i) \) and \( \CM_{ij} =
K(\vec{x}_i, \vec{x}_j) \), symbolized as \( f(\vec{x}) \sim \GP(\mu(\vec{x}),
\K(\vec{x}, \vec{x}')) \). By definition, the kernel function \( K \) yields
positive-definite covariance matrices \( \CM \) for any set of points.

Given the training dataset \( \mathcal{D} = \{ (\vec{x}_i, y_i) \}^N_{i =
1} \) where the inputs \( \vec{x}_i \in \Reals^D \) are collected in the
matrix \( X_\Train = [\vec{x}_1, \dotsc, \vec{x}_N]^{\top} \in \Reals^{N
\times D} \) and the measurements at those points are collected in the vector
\( \vec{y}_\Train = [y_1, \dotsc, y_N]^{\top} \in \Reals^N \), we wish to
predict the values at \( M \) new points \( X_\Pred \in \Reals^{M \times D} \)
for which \( \vec{y}_\Pred \in \Reals^M \) is unknown. We assume that there
is a deterministic function \( f(\vec{x}) \) that maps the input points to
observed output measurements and that this function is distributed according
to a Gaussian process, \( f \sim \GP(\mu(\vec{x}), \K(\vec{x}, \vec{x}')) \)
where we assume a zero mean function \( \mu(\vec{x}) = \vec{0} \).

From the distribution on \( f(\vec{x}) \), the joint distribution
of training and testing data \( \vec{y} \) has covariance
\(
  \CM =
  \begin{pmatrix}
    \CM_{\Train, \Train} & \CM_{\Train, \Pred} \\
    \CM_{\Pred, \Train} & \CM_{\Pred, \Pred}
  \end{pmatrix}
\)
where \( \CM_{\I, \J} \) denotes \( \K(X_\I, X_\J) \) for index sets
\( \I, \J \). In order to make predictions at the unknown points \(
X_\Pred \), we condition the desired prediction \( \vec{y}_\Pred
\) on the observed measurements \( \vec{y}_\Train \). For Gaussian
processes, the posterior distribution is of closed form given by
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \vec{\mu}_\Pred +
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    (\vec{y}_\Train - \vec{\mu}_\Train) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \CM_{\Pred, \Pred} -
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    \CM_{\Train, \Pred}
  \shortintertext{For brevity of notation, we
    denote the conditional covariance matrix as}
  \label{eq:cond_cov_notation}
  \CM_{\I, \J \mid \V} &\defeq
    \CM_{\I, \J} - \CM_{\I, \V} \CM_{\V, \V}^{-1} \CM_{\V, \J}
  \shortintertext{When conditioning on multiple sets, the sets are given in
    order of computation. Although the resulting covariance matrix is the same,
    a different order of conditioning means different intermediate results in
    repeated application of \cref{eq:cond_cov}. In general,}
  \CM_{I, J \mid \V_1, \V_2, \dotsc, \V_n} &\defeq
    \Cov[\vec{y}_\I, \vec{y}_\J \mid
         \vec{y}_{\V_1 \cup \V_2 \cup \dotsb \cup \V_n}]
  \shortintertext{denotes the covariance between the variables in index sets
    \( \I \) and \( \J \), conditional on the variables in \( \V_1, \V_2,
    \dots, \V_n \). Note that by the quotient rule of Schur complementation:}
  \label{eq:quotient_rule}
  \CM_{\I,   \J   \mid \V_{1 \dots n}} &=
  \CM_{\I,   \J   \mid \V_{1 \dots n - 1}} -
  \CM_{\I,   \V_n \mid \V_{1 \dots n - 1}}
  \CM_{\V_n, \V_n \mid \V_{1 \dots n - 1}}^{-1}
  \CM_{\V_n, \J   \mid \V_{1 \dots n - 1}}
\end{align}
Calculating the posterior mean and variance requires inverting the training
covariance matrix \( \CM_{\Train, \Train} \), which has a computational time
complexity of \( \BigO(N^3) \) for \( N \) training points. This scaling
is prohibitive for large datasets, so one natural approach is to carefully
select a subset of \( s \) points out of the \( N \), \( s \ll N \), and pay
a much smaller \( \mathcal{O}(s^3) \) cost. Of course, throwing away \( N -
s \) points will necessarily lead to worse predictive accuracy. To maintain
reasonable accuracy at a severely reduced computational cost requires a
criterion to choose the most effective points.

\subsection{Problem: optimal selection}
\label{subsec:select_obj}

\Stodo{elaborate more on mutual
information information-theoretic criteria in the literature}

One natural criterion is to maximize the \emph{mutual information} between the
selected points and the target point for prediction. The mutual information has
been used by \cite{krause2008nearoptimal} to determine the best locations to
place sensors. The mutual information, or \emph{information gain} is defined as
\begin{align}
  \label{eq:info}
  \MI[\vec{y}_\Pred;\vec{y}_\Train] &= \entropy[\vec{y}_\Pred] -
    \entropy[\vec{y}_\Pred \mid \vec{y}_\Train]
\end{align}
Maximizing the mutual information is equivalent to minimizing the conditional
entropy since the entropy of \( \vec{y}_\Pred \) is constant. Because the
differential entropy of a multivariate Gaussian is monotonically increasing
with the log determinant of its covariance matrix, minimizing the conditional
entropy is equivalent to minimizing the log determinant of the posterior
covariance matrix. For a single predictive point, the log determinant
reduces to its variance. Thus, maximizing mutual information minimizes the
\emph{conditional variance} of the target point. In particular, because our
estimator is the conditional expectation \cref{eq:cond_mean}, it is unbiased
because \( \E[\E[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[\vec{y}_\Pred]
\). Because it is unbiased, its expected mean squared error is simply
the conditional variance since \( \E[(\vec{y}_\Pred - \E[\vec{y}_\Pred
\mid \vec{y}_\Train])^2 \mid \vec{y}_\Train] = \Var[\vec{y}_\Pred \mid
\vec{y}_\Train] \) where the outer expectation is taken under conditioning
because of the assumption that \( \vec{y}_\Pred \) is distributed according
to the Gaussian process. So maximizing the mutual information is equivalent
to minimizing the conditional variance which is in turn equivalent to
minimizing the expected mean squared error of the prediction.

Another perspective on the mutual information results from comparing the
definition of mutual information \cref{eq:info} to the EV-VE identity,
\begin{align}
  \textcolor{orange}{\entropy[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\Pred \mid \vec{y}_\Train]} +
    \textcolor{rust}{\MI[\vec{y}_\Pred;\vec{y}_\Train]} \\
  \textcolor{orange}{\Var[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\Pred \mid \vec{y}_\Train]]}
\end{align}
On the left hand side, entropy is monotone with variance. On the right hand
side, the expectation of the conditional variance is monotone with conditional
entropy and can be interpreted to be the fluctuation of the prediction point
after conditioning. Because the sum of the expectation of conditional variance
and variance of conditional expectation is constant, minimizing the expectation
of the conditional variance is equivalent to maximizing the variance of
conditional expectation, which corresponds to the mutual information.
Supposing \( \vec{y}_\Pred \) was independent of \( \vec{y}_\Train \), then
the conditional expectation becomes simply the expectation, whose variance is
0. Thus, the variance of the conditional expectation is the information shared
between \( \vec{y}_\Pred \) and \( \vec{y}_\Train \), as the larger it is,
the more the prediction for \( \vec{y}_\Pred \) (the conditional expectation)
depends on the observed results of \( \vec{y}_\Train \).

\subsection{A greedy approach}
\label{subsec:greedy_select}

In order to maximize conditional mutual information we greedily select the
candidate with highest information, or the point which most reduces the
conditional variance of the prediction point. Let \( \I = \{ i_j \}^s_{j =
1} \subseteq \Train \) be the set of indices of selected training points.
For a candidate index \( k \), we condition the current covariance matrix
on \( y_k \) according to \cref{eq:cond_cov}:
\begin{align}
  \CM_{:, : \mid \I, k} &= \CM_{:, : \mid \I} -
    \CM_{:, k \mid \I} \CM_{k, k \mid \I}^{-1} \CM_{k, : \mid \I} \\
  %                    &= \CM_{:, : \mid \I} -
  % \frac{\CM_{:, k \mid \I} \CM_{:, k \mid \I}^{\top}}{\CM_{k, k \mid \I}} \\
  \label{eq:cond_select}
                       &= \CM_{:, : \mid \I} - \vec{u} \vec{u}^{\top} \\
  \label{eq:cond_cov_vec}
  \vec{u} &= \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k, k \mid \I}}}
\end{align}
From \cref{eq:cond_select}, conditioning on a new point is a
rank-one downdate on the current covariance matrix. Thus, the
amount that the variance of \( y_\Pred \) will decrease after
selecting \( k \) is given by \( u_\Pred^2 \), or
\begin{align}
  \label{eq:obj_gp}
  u_\Pred^2 = \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
  = \frac{\Cov[y_\Pred, \vec{y}_{\Train}[k] \mid \I]^2}
         {\Var[\vec{y}_{\Train}[k] \mid \I]}
  = \Var[y_\Pred \mid \I]
    \Corr[y_\Pred, \vec{y}_\Train[k] \mid \I]^2
\end{align}
We need to keep track of each candidate's variance and covariance with the
prediction point after conditioning on the points already selected to compute
\cref{eq:obj_gp}. We start with the unconditional values given by \( \CM_{k,
k} \) and \( \CM_{\Pred, k} \) and update after selecting an index \( j \). We
compute \( \vec{u} \) for \( j \) directly according to \cref{eq:cond_cov} and
update \( k \)'s conditional variance by subtracting \( u_k^2 \) and update
its conditional covariance by subtracting \( u_k u_\Pred \).

We have two strategies to efficiently compute \( \vec{u} \). The direct method
is to keep track of \( \CM_{\I, \I}^{-1} \), or the precision of the selected
entries, and update the precision every time a new index is added to \( \I \).
This can be done efficiently in computational time complexity \( \BigO(s^2) \),
see \cref{app:prec_insert}. Once \( \CM_{\I, \I}^{-1} \) has been computed, \(
\vec{u} \) is computed directly according to \cref{eq:cond_cov}. For each of
the \( s \) rounds of selection, it takes \( s^2 \) to update the precision,
and costs \( Ns \) to compute \( \vec{u} \), costing \( \BigO(N s^2 + s^3) =
\BigO(N s^2) \) overall.

The second strategy is to take advantage of the quotient rule of Schur
complementation. From a statistical perspective, the quotient rule states
that conditioning on \( \I \) and then conditioning on \( \J \) is the
same as conditioning on \( \I \cup \J \). We then remind ourselves that
Cholesky factorization can be viewed as iterative conditioning.
\begin{align}
  \shortintertext{Re-writing the joint covariance
    matrix by two steps of block Gaussisan elimination,}
  \label{eq:chol_schur}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{so we see that the Cholesky factorization of the joint
    covariance \( \CM \) is}
  \label{eq:chol}
  \chol(\CM) &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \chol(\CM_{1, 1}) & 0 \\
    0 & \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix} \\
  \nonumber
  &=
  \begin{pmatrix}
    \textcolor{darkorange}{\chol(\CM_{1, 1})} & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \chol(\CM_{1, 1})^{-\top}} &
    \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix}
\end{align}
Here the conditional expectation in \cref{eq:cond_mean} corresponds to
\( \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} \)
and the conditional covariance in \cref{eq:cond_cov} corresponds to
\(
\textcolor{lightblue}{
  \CM_{2, 2 \mid 1} = \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
} \).
Thus, we see that Cholesky factorization is iteratively conditioning
the Gaussian process. From the iterative conditioning perspective, the
\( k \)th column of the Cholesky factor corresponds precisely to the
corresponding \( \vec{u} \) for \( k \) in \cref{eq:cond_cov_vec} since
a iterative sequence of conditioning on \( i_1, i_2 \dotsc, i_{k - 1}
\) is equivalent to conditioning on \( \I \) by the quotient rule.

Adding a column to the current Cholesky factor can be efficiently
computed without excess dependence on \( N \) with left-looking
(see \cref{alg:chol_update}), so the conditioning only happens
when we need it. For each of the \( s \) rounds of selection, it
costs \( \BigO(N s) \) to compute the next column of the Cholesky
factorization, for a total time complexity of \( \BigO(N s^2) \),
matching the time complexity of the explicit precision approach.

\Stodo{should the algorithms be moved to the appendix?}

\begin{figure}
  \centering
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Point selection by \\ explicit precision}
    \label{alg:select_prec}
    \input{figures/algorithms/select_prec.tex}
  \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Point selection by \\ Cholesky factorization}
    \label{alg:select_chol}
    \input{figures/algorithms/select_chol.tex}
  \end{algorithm}
  \end{minipage}
  \caption{Algorithms for single-point selection.}
  \label{fig:alg_select}
\end{figure}

While both approaches have the same time complexity, the precision algorithm
uses \( \BigO(s^2) \) space to store the precision \( \Theta_{\I, \I}^{-1}
\) while the Cholesky algorithm uses \( \BigO(N s) \) space to store the
first \( s \) columns of the Cholesky factorization of \( \CM \), which
is always more memory than the precision (\( N > s \)). Both algorithms
use an additional \( \BigO(N) \) space to store the conditional variances
and covariances. Although the precision algorithm uses less memory than
the Cholesky algorithm, the Cholesky algorithm is preferred for better
performance and ease of implementation.

Once the indices have been computed according to \cref{alg:select_prec}
or \cref{alg:select_chol}, inferring the conditional mean and covariance
of the unknown data can be done directly according to \cref{eq:cond_mean}
and \cref{eq:cond_cov} in time \( \BigO(s^3) \) using \cref{alg:infer_select}.

\Stodo{elaborate more on OMP in the literature}

This algorithm is in fact the covariance equivalent of the
signal recovery algorithm orthogonal matching pursuit (OMP)
\cite{tropp2007signal}, a connection elaborated in \cref{app:omp}.

\subsection{Supernodes and blocked selection}
\label{subsec:mult_select}

We now consider efficiently dealing with multiple prediction points. The
first question is how to generalize the objective for a single point
\cref{eq:obj_gp} to multiple points. Following the same mutual information
justification in \cref{subsec:select_obj}, a natural criterion is to
minimize the log determinant of the prediction points' covariance matrix
after conditioning on the selected points, or \( \logdet(\CM_{\Pred, \Pred
\mid I}) \). This objective, known as D-optimal design in the literature
\cite{krause2008nearoptimal}, has many intuitive interpretations: for
example, as the volume of the region of uncertainty or as the scaling
factor in the probability density function for multivariate Gaussians.

We need to efficiently compute the effect of selecting an index \( k \)
on the log determinant. From \cref{eq:cond_select}, selecting an index
is a rank-one update on the covariance matrix of the prediction points.
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{By application of the matrix determinant lemma
    (the details are in \cref{app:logdet_downdate}),}
  \label{eq:greedy_mult}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}
Equation \cref{eq:greedy_mult} shows that to compute the updated log
determinant, it suffices to only compute conditional variances of the
candidate point. Intuitively this corresponds to \emph{backwards}
regression, where we imagine measuring the values of the \emph{prediction
points} instead of at the \emph{candidates}. We then infer the posterior
variance at a candidate, and pick the candidate whose conditional variance
decreases the most (relative to its starting value). These candidates
are likely to give information about the prediction points, because the
prediction points give information about the candidate.

\Stodo{image for ``backwards'' sensor placement}

Re-writing the objective in this way motivates an efficient algorithm to
compute the objective. We condition on a newly added point essentially
the same as in \cref{subsec:greedy_select}, but now maintaining two data
structures instead of one: one for the variance after conditioning on
the previously selected points, and the other for the variance after
also conditioning on the prediction points. By the quotient rule, the
order of conditioning does not matter as long as the order is consistent.
For the second data structure, we therefore condition on the prediction
points \emph{first} before any points have been selected. We again have
two strategies, one which explicitly maintains precisions and the other
which relies on maintaining partial Cholesky factors.

For the precision algorithm, using \cref{eq:cond_cov} directly, for \( m
\) prediction points it costs \( \BigO(m^3) \) to compute \( \CM_{\Pred,
\Pred}^{-1} \) and then \( \BigO(N m^2) \) to compute the initial conditional
variances \( \CM_{k, k \mid \Pred} \) for the \( N \) candidates. For each of
the \( s \) rounds of selecting candidates, it costs \( s^2 \) and \( m^2 \) to
update the precisions \( \CM_{\I, \I}^{-1} \) and \( \CM_{\Pred, \Pred}^{-1}
\) respectively, where the details of efficiently updating \( \CM_{\Pred,
\Pred}^{-1} \) after the rank-one update in \cref{eq:obj_gp_mult} are given
in \cref{app:prec_cond}. Given the precisions, \( \vec{u} = \frac{\CM_{:, k
\mid \I}}{\sqrt{\CM_{k, k \mid \I}}} \) and \( \vec{u}_\Pred = \frac{\CM_{:,
k \mid \I, \Pred}}{\sqrt{\CM_{k, k \mid \I, \Pred}}} \) are computed as usual
according to \cref{eq:cond_cov} in time \( Ns \) and \( Nm \). Finally, for
each candidate \( j \), the conditional variance \( \CM_{j, j \mid \I} \) is
updated by subtracting \( u_j^2 \), the conditional covariance \( \CM_{\Pred,
k \mid \I} \) is updated for each index \( c \) of a prediction point by
subtracting \( u_j u_c \), and the conditional variance \( \CM_{j, j \mid
I, \Pred} \) is updated by subtracting \( {u_\Pred}_j^2 \). The total time
complexity after simplification is \( \BigO(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two partial Cholesky factors are stored. We
first compute the Cholesky factorization after selecting each prediction
point, for a cost of \( (N + m) m \) for each of the \( m \) columns. We then
begin selecting candidates, which requires updating both Cholesky factors in
time \( (N + m)(m + s) \) which is dominated by updating the preconditioned
Cholesky factor. The columns of the Cholesky factors correspond precisely
to \( \vec{u} \) and \( \vec{u}_\Pred \) and both conditional variances \(
\CM_{j, j \mid I} \) and \( \CM_{j, j \mid I, \Pred} \) can be computed as
above. The conditional covariances do not need to be computed. Over \( s \)
rounds the total time complexity is \( \BigO((N + m) m^2 + s(N + m)(m + s))
\) which simplifies to \( \BigO(N s^2 + N m^2 + m^3) \).

Like the single-point case, both approaches have the same time complexity but
differ in space complexity. The precision algorithm requires \( \BigO(s^2 +
m^2) \) memory to store both precisions, as well as \( \BigO(N m) \) memory
to store the conditional covariances. The Cholesky algorithm requires \(
\BigO((N + m)(m + s)) \) memory to store the first \( m + s \) columns of
the Cholesky factor for the joint covariance matrix between training and
prediction points, which simplifies to \( \BigO(N s + N m + m^2) \). The
memory usages are identical except for \( s^2 \) versus \( N s \), so the
Cholesky algorithm again uses more memory than the precision algorithm.

\begin{figure}
  \centering
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Multiple prediction point selection by explicit precision}
    \label{alg:select_mult_prec}
    \input{figures/algorithms/select_mult_prec.tex}
  \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Multiple prediction point selection by Cholesky factorization}
    \label{alg:select_mult_chol}
    \input{figures/algorithms/select_mult_chol.tex}
  \end{algorithm}

  \begin{algorithm}[H]
    \caption{Update Cholesky factor}
    \label{alg:chol_update}
    \input{figures/algorithms/chol_update.tex}
  \end{algorithm}
  \end{minipage}
  \caption{Algorithms for multiple-point selection.}
  \label{fig:alg_mult_select}
\end{figure}

\subsection{Near optimality by empirical submodularity}

If the objective satisfies the property of submodularity, then it is guaranteed
that the greedy algorithm produces an objective within a a \( 1 - \frac{1}{e}
\) of the optimal objective. Unfortunately the information gain objective is
not submodular in general, see \cref{app:submodular}. However, it is possible
to empirically observe for a particular point set and choice of kernel function
whether it is submodular. We note that one-dimensional points with a Mat{\'e}rn
kernel function seems to be submodular, but not for any higher dimension.

\cite{das2011submodular, jagalur-mohan2021batch}

\section{Greedy selection for \emph{global} approximation by KL-minimization}

reverse maximin ordering
\cite{guinness2018permutation, schafer2021sparse, schafer2020compression}

We have a covariance matrix \( \CM \) and wish to compute the Cholesky
factorization of \( \CM \) into a lower triangular factor \( L \) such that
\( \CM = L L^{\top} \). \Stodo{justify importance/downstream applications of
Cholesky factorization}. This can be done in \( \BigO(N^3) \) with standard
algorithms, which is often prohibitive. Recall the problem of inference in
Gaussian process regression as described in \cref{sec:select} also took
\( \BigO(N^3) \) to invert the covariance matrix \( \CM \). Thus, similar
to Guassian process regression, we will use \emph{sparsity} to mitigate
the computational cost. In fact, we will be able to re-use our previous
algorithms \cref{alg:select_chol,alg:select_mult_chol} on each column of
the Cholesky factorization.

We will first compute the Cholesky factorization of \( \CM^{-1} \),
also known as the \emph{precision matrix}, and use the resulting sparse
factorization to efficiently compute an approximation for \( \CM
\). Because the precision matrix encodes the distribution of the full
conditionals, the \( (i, j) \)th entry of the precision matrix is 0 if and
only if the variables \( x_i \) and \( x_j \) are conditionally independent,
conditional on the rest of the variables. Thus, the precision matrix \(
\CM^{-1} \) can be sparse as a result of conditional independence even
if the original covariance matrix \( \CM \) is dense. It therefore
makes sense to attempt to approximately ``sparsify'' \( \CM^{-1} \)
instead of \( \CM \) with iterated conditioning.

Because of sparsity, we can only get an approximate Cholesky factor \( L \),
\( \hat{L} \) belonging to a pre-specified sparsity pattern \( S \) --- a set
of (row, column) indices that are allowed to be nonzero. In order to measure
the performance of the estimator, we treat the matrices as covariance matrices
of centered Gaussian processes (mean \( \vec{0} \)). In order to compare the
resulting distributions, we use the \emph{KL divergence} according to
\cite{schafer2021sparse}, or the expected difference in log-densities:
\begin{align}
  \label{eq:L_obj}
  L \defeq \argmin_{\hat{L} \in S} \,
    \KL*{\N(\vec{0}, \CM)}
        {\N(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})}
\end{align}

Note that here we are computing the Cholesky factorization
of \( \CM^{-1} \). Surprisingly enough, it is possible to
exactly compute \( L \). First, we re-write the KL-divergence:
\begin{align}
  \label{eq:kl}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \CM_2)
  \right )
  = \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}
where \( \CM_1 \) and \( \CM_2 \) are both of size
\( N \times N \). See \cref{app:kl_obj} for details.

\begin{theorem}
  \label{thm:L}
  \cite{schafer2021sparse}.
  The non-zero entries of the \( i \)th
  column of \( L \) in \cref{eq:L_obj} are:
  \begin{align}
    \label{eq:L_col}
    L_{s_i, i} = \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
  \end{align}
\end{theorem}

Plugging the optimal \( L \) \cref{eq:L_col} back
into the KL divergence \cref{eq:kl}, we obtain:
\begin{align}
  \label{eq:obj_chol}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
\end{align}

See \cref{app:kl_L} for details. In particular, it is important
which direction the KL divergence is or else cancellation of
the \( \trace(\CM_2^{-1} \CM_1) \) term may not occur.

In order to maximize \cref{eq:obj_chol}, we can ignore \( \logdet(\CM)
\) since it does not depend on \( L \) and maximize over each column
independently, since each term in the sum only depends on a single
column. We want to minimize \( (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
\vec{e}_1)^{-1} \), the term corresponding to the diagonal entry in the
inverse of the submatrix of \( \CM \) corresponding to the entries we've
taken. We can give this value statistical interpretation by using the
fact that marginalization in covariance is conditioning in precision.
\begin{align}
  \label{eq:inverse_cond}
  \CM_{1, 1 \mid 2} &=
    ((\CM^{-1})_{1, 1})^{-1}
  \shortintertext{where \( \CM \) is blocked according to}
  \label{eq:blocking}
  \CM &=
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix}
  \shortintertext{Thus, we see that}
  \nonumber
  (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= ((\CM_{s_i, s_i}^{-1})_{11})^{-1} \\
  \label{eq:L_cond_var}
         &= \CM_{ii \mid s_i - \{ i \}}
\end{align}

So our objective on each column is to minimize the conditional variance of
the \( i \)th variable, conditional on the entries we've selected --- \( s_i
\) contains \( i \) to begin with, so \( s_i - \{ i \} \) is the selected
entries. We can therefore use algorithm \cref{alg:select_chol} directly on each
column, where the prediction point is the \( i \) variable and the number
of points selected is the number of nonzero entries per column. The only
difference is that the candidates is limited to indices lower than \( i \),
that is, candidate indices \( k \) such that \( k > i \) to maintain the
lower triangularity of \( L \). Once \( s_i \) has been computed for each \(
i \), \( L \) can be constructed according to \cref{thm:L}. Each column costs
\( \BigO(s^3) \) to compute \( \CM_{s_i, s_i}^{-1} \) for a total
cost of \( \BigO(N s^3) \) for the \( N \) columns of \( L \).

\subsection{Aggregated sparsity pattern}

We can also use the Gaussian process regression viewpoint to efficiently
aggregate multiple columns, that is, to use the same sparsity pattern for
multiple columns. We denote aggregating the column indices \( i_1, \dotsc, i_m
\) into the same group as \( \tilde{i} = \{i_1, i_2, \dots i_m \} \), letting
\( s_{\tilde{i}} = \bigcup_{i \in \tilde{i}} s_i \) be the aggregated sparsity
pattern, and letting \( \tilde{s} = s_{\tilde{i}} - \tilde{i} \) be the set
of selected entries excluding the diagonal entries. Each \( s_i = \tilde{s}
\cup \, \{ j \in \tilde{i} \mid j \geq i \} \), that is, the sparsity pattern
of the \( i \) column is the selected entries plus all the diagonal entries
lower than it. We will enforce that all the selected entries, excluding the
indices of the diagonals of the columns themselves, are below the lowest index
so that indices are not selected ``partially'' --- that is, an index could be
above some indices in the aggregated columns, and therefore invalid to add to
their column, but below others. That is, we restrict the candidate indices \(
k > \max \tilde{i} \) so that the selected index can be added to each column
in \( \tilde{i} \) without violating the lower triangularity of \( L \). It
is in fact possible to properly account for these partial updates, but the
reasoning and eventual algorithm becomes more complicated. We defer a detailed
discussion of the partial update case to \cref{app:partial}.

We now show that the KL-minimization objective on the aggregated indices
corresponds precisely to \cref{eq:obj_gp_mult}, the objective multiple
point Gaussian regression with the chain rule of log determinant through
conditioning.
\begin{align}
  \label{eq:det_chain}
  \logdet(\CM) &= \logdet(\CM_{1, 1 \mid 2}) + \logdet(\CM_{2, 2})
  \shortintertext{where \( \CM \) is blocked according to
    \cref{eq:blocking}. The KL divergence objective for \( \tilde{i} \) is:}
  \nonumber
  \sum_{i \in \tilde{i}} \log(\CM_{ii \mid s_i - \{ i \} })
  &= \log(\CM_{i_m i_m \mid \tilde{s}}) +
     \log(\CM_{i_{m - 1} i_{m - 1} \mid \tilde{s} \cup \{ i_m \}})
     + \dotsb \\
  \nonumber
  &= \logdet(\CM_{\{ i_m, i_{m - 1} \}, \{ i_m, i_{m - 1} \}
             \mid \tilde{s}}) +
    \log(\CM_{i_{m - 2} i_{m - 2} \mid \tilde{s} \cup \{ i_m, i_{m - 1} \}})
    + \dotsb \\
  \label{eq:obj_mult}
  &= \logdet(\CM_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}

We see that the objective \cref{eq:obj_mult} is equivalent to the
objective \cref{eq:obj_gp_mult}, that is, to minimize the log
determinant of the conditional covariance matrix corresponding to a
set of prediction points, conditional on the selected entries. We can
therefore directly use \cref{alg:select_mult_chol} on the aggregated
columns, where the prediction points correspond to indices in the
aggregation and where we restrict the candidates \( k \) to those
below each column in the aggregation, \( k > \max \tilde{i} \).

Hence the sparse Cholesky factorization motivated by KL divergence can be
viewed as sparse Gaussian process selection over each column, where entries are
selected to maximize mutual information with the entry on the diagonal of the
current column. In the aggregated case, the multiple columns in the aggregated
group correspond directly to predicting for multiple prediction points, where
entries are again selected to maximize mutual information with each diagonal
entry in the aggregation. This viewpoint leads directly to \cref{alg:chol}.

\begin{algorithm}
  \caption{Cholesky factorization by selection}
  \label{alg:chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s,
      g = \{ \tilde{i}_1, \dotsc, \tilde{i}_{N/m} \} \)
    \ENSURE \( L \) such that
      \( (L L^{\top})^{-1} \approx K(\vec{x}, \vec{x}) \)

    \STATE \( n \gets \lvert \vec{x} \rvert \)
    \FOR{ \( \tilde{i} \in g \)}
      \STATE \( J \gets
        \{ \max(\tilde{i}) + 1, \max(\tilde{i}) + 2, \dotsc, n \}
      \)
      \STATE Compute \( I \) using
        \cref{alg:select_mult_prec} or \cref{alg:select_mult_chol} \\ where
        \( \vec{x}_\Train = \vec{x}[J],
           \vec{x}_\Pred = \vec{x}[\tilde{i}],
           s = s - \lvert \tilde{i} \rvert
        \)
      \STATE \( \tilde{s} \gets J[I] \)
      \FOR{\( i \in \text{reversed}(\text{sorted}(\tilde{i})) \)}
        \STATE \( \tilde{s} \gets \tilde{s} \cup \{ i \} \)
        \STATE \( s_i \gets \text{reversed}(\tilde{s}) \)
      \ENDFOR
    \ENDFOR
    \RETURN \( L \) computed with \cref{alg:L_mult}
  \end{algorithmic}
\end{algorithm}

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ without aggregation}
  \label{alg:L}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s_i \)
    \ENSURE \( L_{s_i, i} \)

    \STATE \( \CM_{s_i, s_i}^{-1} \gets
      K(\vec{x}[s_i], \vec{x}[s_i])^{-1}
    \)
    \STATE \( L_{s_i, i} \gets \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
      {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \)
    \RETURN \( L_{s_i, i} \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ with aggregation}
  \label{alg:L_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), \tilde{s}, \tilde{i} \)
    \ENSURE \( L_{s_i, i} \) for all \( i \in \tilde{i} \)

    \STATE \( s \gets \tilde{i} \cup \tilde{s} \)
    \STATE \( U \gets P^{\updownarrow}
      \chol(P^{\updownarrow} \CM_{s, s} P^{\updownarrow}) P^{\updownarrow}
    \)
    \FOR{\( i \in \tilde{i} \)}
      \STATE \( k \gets \) index of \( i \) in \( \tilde{i} \)
      \STATE \( L_{s_i, i} \gets U^{-\top} \vec{e}_k \)
    \ENDFOR
    \RETURN L
  \end{algorithmic}
\end{algorithm}
\end{minipage}

Once the sparsity pattern has been determined, we need to compute each column
of \( L \) according to \cref{thm:L}. Because the sparsity pattern for each
column in the same group are subsets of each other, we can efficiently compute
all their columns at once. The observation is that the smallest index in the
group (corresponding to the entry highest in the matrix) will have the largest
sparsity pattern, the next index will have one less entry (lacking the entry
above it, which would violate lower triangularity), and so on. We need to
compute \( \CM_{s_i, s_i}^{-1} \vec{e}_1 \) for each \( i \in \tilde{i} \),
or the precision of the marginalized covariance corresponding to the selected
entries. By \cref{eq:inverse_cond}, we can turn marginalization in covariance
into conditioning in precision:
\begin{align}
  \nonumber
  \label{eq:L_precision}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}} \\
             &= \frac{(\CM_{s, s})^{-1}_{k:, k: \mid :k - 1} \vec{e}_1}
             {\sqrt{\vec{e}_1^{\top} (\CM_{s, s})^{-1}_{k:, k: \mid :k - 1}
                    \vec{e}_1}}
\end{align}
where \( s = \tilde{i} \cup \tilde{s} \) and \( k \) is \( i \)'s
index in \( \tilde{i} \). So we want the \( k \)th column of
the precision of the marginalized covariance, conditional on all the
entries before it. From \cref{eq:chol}, this can be directly read
off the Cholesky factorization. Thus, we can simply compute:
\begin{align}
  \label{eq:L_chol}
  L &= \chol \left ( \CM_{s, s}^{-1} \right )
\end{align}
and read off the \( k \)th column to compute \cref{eq:L_precision} for each
\( i \in \tilde{i} \). However, instead of computing a lower triangular
factor for the precision, we can compute an \emph{upper} triangular factor
the covariance whose inverse transpose will be a \emph{lower} triangular
factor for the original matrix. In particular, we see that
\begin{align}
  \label{eq:U_chol}
  U &= P^{\updownarrow} \chol(P^{\updownarrow} \CM_{s, s} P^{\updownarrow})
       P^{\updownarrow}
  \shortintertext{satisfies \( U U^{\top} = \CM_{s, s} \) where
  \( P^{\updownarrow} \) is the order-reversing permutation. Thus,}
  \nonumber
  \CM_{s, s}^{-1} &= U^{-\top} U^{-1}
\end{align}
where \( U^{-\top} \) is an \emph{lower} triangular factor for \( \CM_{s,
s}^{-1} \) equal to \cref{eq:L_col} because the Cholesky factorization is
unique. Computing \( U^{-\top} \) leads directly to \cref{alg:L_mult}.

Recall that the complexity of selecting \( s \) out of \( N \) total training
points for \( m \) prediction points using \cref{alg:select_mult_prec} or
\cref{alg:select_mult_chol} was \( \BigO(N s^2 + N m^2 + m^3) \). In the
context of Cholesky factorization, \( N \) is the size of the matrix, \( m \)
is the number of columns to aggregate, and \( s \) is the number of nonzero
entries in each column of \( L \). We therefore need to do \( \frac{N}{m} \)
selections, one for each aggregated group, where we only need to select \( s
- m \) entries (since the \( m \) prediction points are automatically added).
We then need to actually construct each column of \( L \) after determining
the sparsity pattern, with \cref{alg:L_mult}. This costs \( \BigO(s^3) \)
for each aggregated group to compute the Cholesky factor of the submatrix,
which dominates the time to compute each column of \( L \) for the \( m \)
columns in the group, \( \BigO(m s^2) \) (\( N > s > m \)). Thus, the overall
complexity is \( \BigO(\frac{N}{m} (N (s - m)^2 + N m^2 + m^3 + s^3)) \),
which simplifies to \( \BigO(\frac{N^2 s^2}{m}) \) by making use of the bound
that \( (s - m)^2 = \BigO(s^2 + m^2) \).

Note that the non-aggregated factorization is equivalent to \( m = 1 \),
which yields \( \BigO(N^2 s^2) \) (using the non-aggregated algorithms
\cref{alg:select_chol,alg:L}, but one can also use the aggregated versions
\cref{alg:select_mult_chol,alg:L_mult} with \( m = 1 \) and achieve
equivalent complexity). Thus, we see that the aggregated version is \( m
\) times faster than its non-aggregated counterpart, at the cost that the
resulting sparsity pattern will be lower quality (since the algorithm is
forced to select the same entry for \emph{all} columns in the group).

Unlike the geometric algorithms of \cite{schafer2021sparse,
schafer2020compression} which rely on the pairwise distance between points,
and whose covariance matrix is implicitly determined by a list of points and
kernel function, this algorithm relies only on the entries of the covariance
matrix \( \CM \). Thus, it can factor arbitrary symmetric positive-definite
matrices without access to points or an explicit kernel function.

\section{Numerical experiments}

All experiments were run on the Partnership for an Advanced Computing
Environment (PACE) Phoenix cluster at the Georgia Institute of
Technology, with 8 cores of a Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz
and 22 GB of RAM per core. Python code for all numerical experiments
can be found at \href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\Stodo{cite python libraries that want citations (e.g. scikit-learn)}

\subsection{\textit{k}th-nearest neighbors selection}

We justify that diverse point selection based on conditional information can
lead to better performance than simply selecting the nearest neighbor in a toy
example on the MNIST dataset. We compare \( k \)th-nearest neighbors (KNN)
directly to conditional \( k \)th-nearest neighbors (CKNN) in the following
experiment. We randomly select 1000 images to form the training set and 100
to form the testing set. For each image in the testing set, we select the
\( k \) ``closest'' training points with either KNN or CKNN. For KNN we use
the standard Euclidean distance and for CKNN we use Mat{\'e}rn kernel with
smoothness \( \nu = 1.5 \) and length scale \( l = 2^{10} \). Finally, we
predict the label of the test point by taking the most frequently occurring
label in the \( k \) selected points.

\Stodo{cite mnist dataset}

\begin{figure}[h]
  \centering
  \input{figures/mnist/accuracy_k.tex}
  \caption{Accuracy with increasing \( k \)}
\end{figure}

As \( k \) increases, KNN degrades near-linearly in accuracy. We hypothesize
that nearby images are more likely to have the same label as a given test
image. By forcing the algorithm to select more points, it increases the
likelihood that the algorithm becomes confused by differently labeled
images. However, CKNN is more accurate than KNN for nearly every \( k
\), suggesting that conditional selection is able to take advantage of
selecting more points. We emphasize that the difference in accuracy is
solely a result of conditional selection --- because the Mat{\'e}rn kernel
degrades monotonically with distance, sorting by covariance is identical
to sorting by distance. In addition, we use the mode to aggregate the
labels of the selected points, rather than performing Gaussian process
classification. The difference in accuracy can therefore be attributed
to precisely the difference in which points were selected.

\subsection{Recovery of sparse Cholesky factors}

As noted in \cref{app:omp}, the selection algorithm can be viewed as orthogonal
matching pursuit \cite{tropp2007signal} in feature space. We experiment
with the sparse recovery properties of the selection algorithm by randomly
generating a sparse Cholesky factor \( L \). We prescribe a fixed number
of nonzeros per column \( s \) over \( N \) columns. For each column, we
uniformly randomly pick \( s \) entry that satisfies lower triangularity to
make nonzero. We randomly generate values according to i.i.d. standard normal
\( \mathcal{N}(0, 1) \). Finally, we fill the diagonal with a ``large``
positive value (10) to almost guarantee that the resulting matrix \( \CM =
L L^{\top} \) is positive-definite. The selection algorithms are then given
\( \CM \) and \( s \) and are asked to reconstruct \( L \). The strategies
are as follows: ``cknn'' uses conditional selection on each column to minimize
the conditional variance of the diagonal entry, ``knn'' selects entries with
the largest covariance with the diagonal entry, ``corr'' selects entries with
the highest correlation objective \cref{eq:obj_gp} without accounting for
conditioning, and ``random'' simply randomly samples entries uniformly. The
strategies are given either the covariance \( \CM \) or the precision \(
\CM^{-1} \) depending on which results in higher accuracy, in particular,
the ``cknn'' strategy is given the precision while the rest of the methods
are given the covariance. Accuracy is measured by taking the cardinality of
the intersection of the recovered sparsity set with the ground truth sparsity
set over the cardinality of their union, intersection over union (IOU).

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_n.tex}
  \caption{Accuracy with increasing \( N \)}
\end{figure}

As the number of nonzero entries per column is fixed and the number of rows
and columns is increased, the ```cknn'' retains high accuracy near perfect
recovery, and the rest of the methods quickly degrade and asymptote to their
final accuracies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_s.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

If the number of rows and columns is fixed while the number of nonzero entries
per column is increased, all methods drop in accuracy with increasing density
into a tipping point where the problem starts to become easier. Accuracy then
increases until the Cholesky factor becomes fully dense, in which case perfect
recovery is trivial. The ``cknn'' strategy exhibits the same behavior, but
maintains much higher accuracy than the rest of the strategies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_noise.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

Finally, we experiment with the addition of noise. Noise sampled i.i.d
from \( \mathcal{N}(0, \sigma^2) \) is added to each entry of \( \CM
\) symmetrically (i.e. \( \CM_{ij} \) receives the same noise as \(
\CM_{ji} \)) to preserve the symmetry of \( \CM \). As expected,
accuracy degrades with increasing noise, but the algorithm is fairly robust
to low levels of noise. At higher levels of noise, \( \CM \) can lose
positive-definiteness, which causes the algorithm to break down.

\Stodo{laplacians, problems with
positive-definiteness and dense cholesky factors}

\cite{kyng2016approximate}

\subsection{Cholesky factorization}

\subsection{Gaussian process regression}

\subsection{Preconditioning for conjugate gradient}

\subsection{Comparison to other methods}

against greedy gp information theoretic: most are trying to
get some approximation for the entire process, \emph{directed}
greedy selection (towards a single point or group of points)
(never mind, smola \cite{smola2000sparse} is directed)

engineering for multiple points / nonadjacent case (conditioning structure)

omp: feature space versus covariance

previous Cholesky papers: conditional selection versus geometry

\section{Conclusions}

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{cholesky}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Computation in sparse Gaussian process selection}

\subsection{Updating precision after insertion}
\label{app:prec_insert}

Assuming we have the precision of the selected entries, \( \CM_{\I, \I}^{-1}
\), we wish to account for adding a new index \( k \) to \( \I \), that is, we
wish to compute \( \CM_{\I \cup \{ k \}, \I \cup \{ k \}}^{-1} \), adding a new
row and column to \( \CM_{\I, \I}^{-1} \). In order to update efficiently, we
block the matrix to separate new and old information.
\begin{align}
  \shortintertext{Using the same block \( L D
    L^{\top} \) factorization as \cref{eq:chol_schur},}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{For brevity of notation, we denote the Schur complement
    \textcolor{lightblue}{\( \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1,
    2} \)} as the conditional covariance \textcolor{lightblue}{\( \CM_{2, 2
    \mid 1} \)}. Inverting both sides of the equation,}
  \CM^{-1} &=
  \begin{pmatrix}
    \Id & -\textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & 0 \\
    -\textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \CM_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry to the matrix, \(
    \CM_{1, 1} = \CM_{\I, \I} \), \( \CM_{1, 2} = \CM_{\I, k} \), and \(
    \CM_{2, 2} = \CM_{k, k} \). Also note that \( \textcolor{lightblue}{\CM_{k,
    k \mid \I}^{-1}} \) is the precision of \( k \) conditional on the entries
    in \( \I \), which has already been computed in \cref{alg:select_prec}.
    If we let \( \vec{v} = \textcolor{darkorange}{\CM_{\I, \I}^{-1} \CM_{\I,
    k}} \), then}
  &=
  \begin{pmatrix}
    \CM_{\I, \I}^{-1} + \CM_{k, k \mid \I}^{-1} \vec{v} \vec{v}^T &
    -\CM_{k, k \mid \I}^{-1} \vec{v} \\
    -\CM_{k, k \mid \I}^{-1} \vec{v}^{\top} & \CM_{k, k \mid \I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:select_prec}.
Note that the bulk of the update is a rank-one update to \( \CM_{1,
1}^{-1} \), which can be computed in \( \BigO(\card{\I}) = \BigO(s^2) \).

\subsection{Updating precision after marginalization}
\label{app:prec_delete}

Suppose we have the precision \( \CM^{-1} \) and wish to compute the
precision of the marginalized covariance after ignoring an index \( k \).
That is, we wish to compute the inverse of a matrix after deleting a row and
column, given the inverse of the original matrix. We could use the result
in \cref{app:prec_insert} by ``reading'' the update backwards. That is, we
could identify \( \CM_{2, 2 \mid 1}^{-1} \) from \( (\CM^{-1})_{kk}
\) and \( \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} \) from \( -
\frac{(\CM^{-1})_{-k, k}}{\CM_{2, 2 \mid 1}^{-1}} \) where \( -k \)
denotes all rows excluding the \( k \)th row. We can then revert the rank-one
update by subtracting out the update, computing \( \CM_{-k, -k}^{-1} =
(\CM^{-1})_{-k, -k} - \CM_{kk \mid I}^{-1} \vec{v} \vec{v}^{\top} \).
However, a more intuitive derivation relies on the fact that marginalization
in covariance is conditioning in precision. Using \cref{eq:inverse_cond},
we see that \( \CM_{-k, -k}^{-1} = (\CM^{-1})_{-k, -k \mid k} \), or
the precision conditional on the deleted entry. By \cref{eq:cond_cov}, we
immediately obtain the equivalent update
\begin{align}
  (\CM^{-1})_{-k, -k \mid k} &= \CM^{-1}_{-k, -k} -
    \frac{(\CM^{-1})_{-k, k}
          (\CM^{-1})_{-k, k}^{\top}}{(\CM^{-1})_{kk}}
\end{align}
Since this is a rank-one update to the precision \( \CM^{-1} \), this
can be computed in \\ \( \BigO(\text{\# rows}(\CM^{-1}))^2 \).

\Stodo{this is not used in the paper but is nice to know +
used in the sensor placement}

\subsection{Updating the log determinant after a rank-one downdate}
\label{app:logdet_downdate}

Assuming we already have the log determinant of the covariance matrix of the
prediction points conditional on the selected entries, \( \logdet(\CM_{\Pred,
\Pred \mid \I}) \), we wish to compute the log determinant after we add an
index \( k \) to \( I \), that is, to compute \( \logdet(\CM_{\Pred, \Pred
\mid \I \cup \{ k \}}) \).
\begin{align}
  \shortintertext{From \cref{eq:cond_select}, selecting a
    new point is a rank-one downdate on the covariance matrix.}
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{Using the matrix determinant lemma,}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( 1 -
       \frac{\CM_{\Pred, k \mid \I}^{\top} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{Focusing on the second term, we can turn
    the quadratic form into condtioning.}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left (
       \frac{\CM_{k, k \mid \I} -
             \CM_{k, \Pred \mid \I} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{By the quotient rule, we combine the conditioning.}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

Assuming we have the precision matrix of the prediction points conditional on
the selected entries, \( \CM_{\Pred, \Pred \mid \I}^{-1} \), we want to take
into account selecting an index \( k \), or to compute \( \CM_{\Pred, \Pred
\mid \I \cup \{ k \}}^{-1} \), which is a rank-one update to the covariance
(but not necessarily the precision) from \cref{eq:obj_gp_mult}. We can directly
apply the ShermanMorrisonWoodbury formula which states that:
\begin{align}
  \CM_{1, 1 \mid 2}^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning from \cref{eq:cond_cov},}
  \left (
    \CM_{1, 1} - \CM_{1, 2} \CM_{2, 2}^{-1} \CM_{2, 1}
  \right )^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{For brevity of notation, letting \( \vec{u} = \CM_{1, 2} \)
    and \( \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} = \CM_{1, 1}^{-1} \vec{u} \),}
  (\CM_{1, 1} - \CM_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \CM_{1, 1}^{-1} + \CM_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{So we see that a rank-one update to \( \CM_{1, 1}
    \) then inverting is a rank-one update to \( \CM_{1, 1}^{-1} \). In
    our context, \( \CM_{1, 1} = \CM_{\Pred, \Pred \mid \I}, \vec{u}
    = \CM_{\Pred, k \mid I}, \CM_{2, 2} = \CM_{k, k \mid \I} \) so \(
    \CM_{2, 2 \mid 1}^{-1} = \CM_{k, k \mid \Pred, I}^{-1} \) (this can
    be rigorously shown by expanding the Schur complement and taking
    advantage of the quotient rule as in \cref{eq:greedy_mult}). \(
    \vec{v} \) can be computed according to definition as \( \CM_{\Pred,
    \Pred \mid \I}^{-1} \vec{u} \). Thus, we can write the update as}
  \left ( \CM_{\Pred, \Pred \mid \I} -
    \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
         {\CM_{kk \mid \I}}
  \right )^{-1} &=
    \CM_{1, 1}^{-1} +
    \CM_{k, k \mid \Pred, \I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:select_mult_prec}.
Since the update is a rank-one update, it can be
computed in \( \BigO(\card{\Pred}^2) = \BigO(m^2) \).

\subsection{Updating a Cholesky factor after a rank-one downdate}
\label{app:chol_downdate}

\Stodo{remove because unnecessary, describe insertion instead}

We use the approach from Lemma 1 of \cite{krause2015more}, slightly adapted
to use in-place operations and to make no assumption on the particular row
ordering of the Cholesky factor. Let \( L \) be a Cholesky factorization of \(
\CM \), that is, \( L = \chol(\CM) \). We wish to compute the updated
Cholesky factor \( L' = \chol(\CM') \) where \( \CM' = \CM - \vec{u}
\vec{u}^{\top} \). To do so, assume \( L \) and \( L' \) are blocked according
to the same block structure:
\begin{align}
  L &=
  \begin{pmatrix}
    r_1 & \vec{0} \\
    \vec{r}_2 & L_2
  \end{pmatrix},
  L' =
  \begin{pmatrix}
    r_1' & \vec{0} \\
    \vec{r}_2' & L_2'
  \end{pmatrix}
  \shortintertext{Multiplying, we find}
  L L^{\top} = \CM &=
  \begin{pmatrix}
    r_1^2 & r_1 \vec{r}_2^{\top} \\
    r_1 \vec{r}_2 & L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
  \end{pmatrix}
  \\ L' L'^{\top} = \CM' &=
  \begin{pmatrix}
    r_1'^2 & r_1' \vec{r}_2'^{\top} \\
    r_1' \vec{r}_2' & L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top}
  \end{pmatrix}
  \shortintertext{From here, we solve for
    \( r'_1 \), \( \vec{r}' \), and \( L_2' \)}
  r_1'^2 &= \CM'_{11} = \CM_{11} - u_1^2 \\
         &= r_1^2 - u_1^2 \\
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  r_1' \vec{r}_2' &= \CM'_{2:, 1} = \CM_{2:, 1} - u_1 \vec{u}_2 \\
                  &= r_1 \vec{r}_2 - u_1 \vec{u}_2 \\
  \vec{r}_2' &= \frac{1}{r_1'} (r_1 \vec{r}_2 - u_1 \vec{u}_2) \\
  % L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top} &= \CM'_{22}
  %   = \CM_{22} - \vec{u}_2 \vec{u}_2^{\top} \\
  L_2' L_2'^{\top} &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \vec{r}_2' \vec{r}_2'^{\top}
  \shortintertext{Plugging in the expresion for \( \vec{r}'_2 \),}
                   &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right ) \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right )^{\top} \\
                   &=  L_2 L_2^{\top} +
    \left ( 1 - \frac{r_1^2}{r_1'^2} \right ) \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
    \left ( 1 + \frac{u_1^2}{r_1'^2} \right ) \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{Using \( r_1' = \sqrt{r_1^2 - u_1^2} \),}
                   &=  L_2 L_2^{\top} -
      \frac{u_1^2}{r_1'^2} \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
      \frac{r_1^2}{r_1'^2} \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{After factoring we find}
  L_2' L_2'^{\top} &= L_2 L_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right ) \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right )^{\top}
  \shortintertext{which is a rank-one downdate to the subfactor \( L_2 \).
    Recursively updating \( L_2 \) yields a \( \BigO(N^2) \) algorithm.
    We now re-write the algorithm to be in-place to take advantage of BLAS
    routines. The updates can be summarized as:}
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  \vec{r}' &= \frac{r_1}{r_1'} \vec{r} - \frac{u_1}{r_1'} \vec{u} \\
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'} \vec{r}
  \shortintertext{Note that we drop the subscripting on \( \vec{r} \) and \(
    \vec{u} \). By updating the entire vector on each iteration, we can avoid
    keeping track of the lower triangular structure of \( L \). We will first
    update \( \vec{r}' \) and then use it to update \( \vec{u} \). Solving for
    \( \vec{r} \) in terms of \( \vec{r}' \),}
  \vec{r} &= \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \\
  \shortintertext{Plugging the expression for \( \vec{r} \) into
    the update for \( \vec{u}' \),}
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'}
    \left ( \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \right ) \\
           &= \frac{r_1^2 - u_1^2}{r_1 r_1'} \vec{u}
            - \frac{u_1}{r_1} \vec{r}' \\
           &= \frac{r_1'}{r_1} \vec{u} - \frac{u_1}{r_1} \vec{r}'
  \shortintertext{Thus, the updates proceed sequentally as follows:}
  \gamma &\gets \sqrt{r_1^2 - u_1^2} \\
  \alpha &\gets \frac{r_1}{\gamma} \\
  \beta &\gets \frac{u_1}{\gamma} \\
  \vec{r} &\gets \alpha \vec{r} - \beta \vec{u} \\
  \vec{u} &\gets \frac{1}{\alpha} \vec{u} - \frac{\beta}{\alpha} \vec{r}
\end{align}
These can be efficiently performed in-place by
BLAS as level-one \texttt{daxpy} operations.

\subsection{Updating Cholesky factor after insertion}
\label{app:chol_insert}

Suppose we have a cholesky factor \( L \) of \( \CM \) and we insert
a new point into \( \CM \). We wish to update the Cholesky \( L \) to
account for this insertion. Using the recursive conditioning interpretation
of Cholesky factorization in \cref{eq:chol}, we see that the columns of \( L
\) before the insertion will remain unchanged, the column at the insertion
point is a new column given by the conditional covariance of the new point
with the rest of the points, conditional on the points before it, which can
be computed with standard left-looking, and the columns of \( L \) after the
insertion correspond to the Cholesky factor of the conditional covariance,
conditional on the newly inserted point in addition to the previous points.
From \cref{eq:cond_select} we know that conditioning on an additional point is
a rank-one update of the covariance, so we can use rank-one downdating from
\cref{app:chol_downdate} to update \( L \) for the columns after the insertion
point, where the vector in the rank-one downdate is the newly inserted column.
This update touches every value in the Cholesky factor exactly once, so its
complexity is \( \BigO(N^2) \) as opposed to the \( \BigO(N^3)
\) cost of completely regenerating the Cholesky factor from scratch.

\subsection{Partial updates in the selection algorithm}
\label{app:partial}

In the context of the selection algorithm, we have \( M \) prediction points
and wish to minimize the log determinant of the resulting covariance matrix
of the prediction points, conditional on the points we've selected from the
training data. In the specific context of Cholesky factorization, it is
possible to add a training point and have it apply \emph{partially} on the
prediction points. If nonadjacent columns indices are aggregated, a entry
selected between two indices can be higher than one column, but lower than
another. Adding the entry to the sparsity pattern would therefore only add
to some, but not all, columns in the aggregation. We will model this as
partially conditioning the variables of interest. In particular, if we have
prediction variables \( y_1, y_2, \dotsc, y_M \), a partial condition ignoring
the first \( j \) variables on the selected index \( k \) would result in
\( y_1 , y_2, \dotsc, y_{j}, y_{j + 1 \mid k}, \dotsc, y_{M \mid k} \).

The first question is to compute the resulting covariance matrix. We know \(
\vec{y} \sim \mathcal{N}(\vec{0}, \CM) \) and \( \vec{y}_{\mid k} \) has
conditional distribution according to \cref{eq:cond_cov}, \( \vec{y}_{\mid k}
\sim \mathcal{N}(\mu, \CM - \CM_{:, k} \CM_{k, k}^{-1} \CM_{k,
:}) \). Taking the Cholesky factorization of both covariance matrices, let \(
L = \chol(\CM) \) and \( L_{\mid k} = \chol(\CM_{\mid k}) \). We can
then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z} \) is distributed
according to \( \mathcal{N}(\vec{0}, I) \). Similarly, \( \vec{y}_{\mid k} =
L_{\mid k} \vec{z} + \vec{\mu} \). For unconditioned \( y_i \) and \( y_j \), the
covariance between them is defined to be \( \CM_{ij} \). Similarly, for
conditioned \( y_i \) and \( y_j \), the covariance is \( \CM_{ij \mid k}
\). The only question is what the covariance between unconditioned \( y_i \)
and conditioned \( y_j \) is. By definition,
\begin{align}
  \Cov[y_i, y_j] &= \E[(y_i - \E[y_i])(y_j - \E[y_j])] \\
                 &= \E[(L_i \vec{z}) (L_{i \mid k} \vec{z})] \\
                 &= \E[(L_{1, i} z_1 + \dotsb + L_{N, i} z_N)
                       (L_{1, j \mid k} z_1 + \dotsb + L_{N, j \mid k} z_N)]
  \shortintertext{For \( i \neq j \), \( E[z_i z_j] = \E[z_i] \E[z_j] = 0\)
    since \( z_i \) is independent of \( z_j \) and has mean 0.}
                 &= \E[L_{1, i} L_{1, j \mid k} z_1^2 + \dotsb +
                       L_{N, i} L_{N, j \mid k} z_N^2] \\
                 &= L_{1, i} L_{1, j \mid k}  \E[z_1^2] + \dotsb +
                    L_{N, i} L_{N, j \mid k} \E[z_N^2]
  \shortintertext{For any \( i \), \( \E[z_i^2]
    = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \)}
                 &= L_{1, i} L_{1, j \mid k} + \dotsb +
                    L_{N, i} L_{N, j \mid k} \\
                 &= L_i \cdot L_{j \mid k}
  \shortintertext{Thus, the new covariance matrix can be written as:}
  \label{eq:chol_partial}
  \begin{pmatrix}
    L_{:j} L_{:j}^{\top} & L_{:j} L_{j: \mid k}^{\top} \\
    L_{j: \mid k} L_{:j}^{\top} & L_{j: \mid k} L_{j: \mid k}^{\top}
  \end{pmatrix} &=
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}^{\top}
  \shortintertext{We will denote a partially conditioned matrix as}
  \CM_{:, :, \shortmid k}
\end{align}

\begin{figure}[h]
  \centering
  \input{figures/partial_factor.tex}
  \caption{Illustration of the Cholesky
    factorization of a partially conditioned matrix.}
\end{figure}

We can now connect minimization of the log determinant of the partially updated
covariance matrix to the KL divergence objective of Cholesky factorization.
Computing the log determinant of the partially updated covariance
matrix, we make use of \cref{eq:chol_partial} and make use of the fact
that the determinant of a  triangular matrix is the product of its
diagonal entries:
\begin{align}
  \label{eq:partial_logdet}
  \frac{1}{2} \logdet(\CM_{:, : \shortmid k}) &=
  \underbrace{\log(L_{11}) + \dotsb + \log(L_{jj})}_{\text{the same}} +
  \underbrace{
    \log(L_{j + 1, j + 1 \mid k}) + \dotsb + \log(L_{M, M \mid k})
  }_{\text{conditioned}}
  \shortintertext{Comparing to the KL divergence \cref{eq:obj_chol},
    \( \mathbb{D}_{\text{KL}}
      \left (
        \mathcal{N}(\vec{0}, \CM) \, \Big \| \,
        \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
      \right )
    \) which is equivalent to maximizing}
  &= \sum_{i = 1}^M
      \log \left (
        \CM_{ii \mid s_i - \{ i \}}
      \right )
  \shortintertext{Recalling that \( k \) is added partially to some \( s_i \),
    only those \( i > j \)}
  &= \underbrace{
      \log \left ( \CM_{11 \mid s_1 - \{ 1 \}} \right ) + \dotsb +
      \log \left ( \CM_{jj \mid s_j - \{ j \}} \right )
     }_\text{the same} + \\
  \nonumber
  &  \underbrace{
      \log \left ( \CM_{j + 1, j + 1 \mid s_{j + 1} - \{ j + 1 \}} \right )
      + \dotsb + \log \left ( \CM_{MM \mid s_M - \{ M \}} \right )
     }_\text{conditioned}
  \shortintertext{Since \( L_{ii} \) is the square root
    of the variance of the \( i \)th variable conditional
    on each entry before it in the ordering, we have}
  2 \log(L_{ii}) &= \log(\CM_{ii \mid s_i - \{ i \}})
\end{align}
So minimizing the log determinant of the partially
conditioned covariance matrix \cref{eq:partial_logdet} is
the same as minimizing the KL divergence \cref{eq:obj_chol}.

\subsection{Algorithm for partial updates}
\label{app:partial_alg}

We now need an efficient algorithm to keep track of partial updates.
The key idea is to implicitly maintain the prediction matrix with
selected points inserted to maintain proper ordering, and keep track
of the log determinant throughout selection. We first give how this
different perspective affects the interpretation of the multiple
point selection algorithm. In the example, let \( x \) and \( y \)
be selected points and \( 1 \) and \( 2 \) be prediction points.
\begin{align}
  \CM &=
  \begin{pmatrix}
    \CM_{xx} & \CM_{xy} & \CM_{x1} & \CM_{x2} \\
    \CM_{yx} & \CM_{yy} & \CM_{y1} & \CM_{y2} \\
    \CM_{1x} & \CM_{1y} & \CM_{11} & \CM_{12} \\
    \CM_{2x} & \CM_{2y} & \CM_{21} & \CM_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\CM) &= \log(\CM_{xx}) + \log(\CM_{yy \mid x}) +
    \log(\CM_{11 \mid x, y}) + \log(\CM_{22 \mid x, y, 1})
  \shortintertext{Isolating the objective --- the variances of the
    prediction points}
  \log(\CM_{11 \mid x, y}) + \log(\CM_{22 \mid x, y, 1}) &=
    \logdet(\CM) - \log(\CM_{xx}) - \log(\CM_{yy \mid x})
  \shortintertext{Now consider how inserting \( y \) changed the objective
    from when it was just \( x \).}
  \log(\CM_{11 \mid x}) + \log(\CM_{22 \mid x, 1}) &=
    \logdet(\CM_{-y, -y}) - \log(\CM_{xx}) \\
  \Delta &=
    \logdet(\CM) - \log(\CM_{yy \mid x}) - \logdet(\CM_{-y, -y})
  \shortintertext{But from \cref{eq:greedy_mult} we know}
  \Delta &= \log \left (
      \frac{\CM_{yy \mid x, 1, 2}}{\CM_{yy \mid x}}
    \right )
  \shortintertext{Substituting,}
  \log(\CM_{yy \mid x, 1, 2}) - \log(\CM_{yy \mid x}) &=
    \logdet(\CM) - \log(\CM_{yy \mid x}) - \logdet(\CM_{-y, -y}) \\
  \log(\CM_{yy \mid x, 1, 2}) &=
    \logdet(\CM) - \logdet(\CM_{-y, -y})
  \shortintertext{In general,}
  \label{eq:logdet_diff}
  \log(\CM_{kk \mid I, \Pred}) &=
    \logdet(\CM) - \logdet(\CM_{-k, -k})
\end{align}
Another way to arrive at the same result is to note that if we inserted
\( y \) at the \emph{end} of \( \CM_{-y, -y} \), to compute the log
determinant of the new, bigger matrix \( \CM \) we would add the
variance of \( y \) conditional on every entry in the matrix to the
old determinant by chain rule. Since the determinant is invariant to
symmetric permutation, the matrix inserting \( y \) at the end has the
same determinant as inserting \( y \) where it should be.

So we see that the conditional variance of a candidate point conditional
on everything else in the matrix is the difference in log determinant
between the matrix with the candidate inserted and the original matrix.
The multiple prediction point algorithm can therefore be interpreted as
we insert the candidate \emph{after} all the previously selected points
(so it is conditional on all the previous points) and \emph{before}
the prediction points (which conditions all of them). We then compute
\( \log(\CM_{kk \mid I, \Pred}) \) for some candidate \( k \)
which represents the difference in log determinant and then subtract \(
\log(\CM_{kk \mid I}) \) which is the spurious variance introduced by
inserting \( k \) into the matrix. We do not need to subtract the spurious
variances from the previously selected points because \( k \) does not
affect them, and we select candidates by \emph{relative} score.

We now apply this result to partial selection. In the example,
let \( 1 \) and \( 2 \) be prediction points while \( x \) and \(
y \) are both a selected points below \( 2 \) but above \( 1 \), where
\( x \) has already been selected and \( y \) is a candidate.
\begin{align}
  \CM &=
  \begin{pmatrix}
    \CM_{11} & \CM_{1y} & \CM_{1x} & \CM_{12} \\
    \CM_{y1} & \CM_{yy} & \CM_{yx} & \CM_{y2} \\
    \CM_{x1} & \CM_{xy} & \CM_{xx} & \CM_{x2} \\
    \CM_{21} & \CM_{2y} & \CM_{2x} & \CM_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\CM) &= \log(\CM_{11}) + \log(\CM_{yy  \mid 1}) +
    \log(\CM_{xx \mid 1, y}) + \log(\CM_{22 \mid 1, y, x})
  \shortintertext{We see that \( y \) conditions \( 2 \) but not \( 1 \),
    precisely what we want to encode. However, we have introduced a spurious
    term \( \log(\CM_{yy \mid 1}) \) and changed the variance of \( x \),
    both of which must be subtracted out.}
  \log(\CM_{11}) + \log(\CM_{22 \mid 1, y, x}) &=
  \logdet(\CM) - \log(\CM_{yy \mid 1}) - \log(\CM_{xx \mid 1, y})
  \shortintertext{We can substitute \( \log(\CM_{yy \mid 1, x, 2}) \)
    for \( \logdet(\CM) \) by \cref{eq:logdet_diff}. Athough
    it differs by a constant, this does not change the objective.}
  &= \log(\CM_{yy \mid 1, x, 2}) -
    \log(\CM_{yy \mid 1}) - \log(\CM_{xx \mid 1, y})
\end{align}

As long as we can compute conditional variances of our candidate on each
\emph{prefix} of the current ordering of prediction points interspersed with
selected points, we can use the conditional variances to compute the updated
conditional variances of the selected points by using their conditional
covariances with the candidate. We are then able to compute every term
in the objective. To do so, we maintain a partial Cholesky factor whose
ordering is given by the current ordering. When we select a new point, we
insert it in its appropriate place in the Cholesky factor. To update the
Cholesky factor after an insertion efficiently, we left-look to get the
column of its insertion position, and then update all columns right of the
column by a rank-one downdate as described in \cref{app:chol_insert} which
touches every entry in the Cholesky factor, \( \BigO(N(m + s) \) per
update for a total cost of \( \BigO(N(m + s)(s)) \) over \( s \)
selections. In addition, the algorithm can be considerably simplified by
simply adding the conditional variances of the prediction points, instead
of starting with a proxy for the log determinant of the entire matrix and
subtracting out the spurious interactions from the training points.

By inspecting the Cholesky factor, we get the covariance of a selected point
with a candidate, conditional on all the points prior to the selected point in
the ordering. The conditional variance of the selected point is the diagonal
entry. We can then compute the new conditional variance given the variance of
the candidate, conditional on all points prior to the selected point. Suppose
we are at index \( i \) and the candidate is index \( j \), the updates are as
follows:
\begin{align}
  \CM_{ii \mid :i - 1} &= (L_{ii})^2 \\
  \CM_{ij \mid :i - 1} &= L_{ij} \cdot L_{ii} \\
  \CM_{ii \mid :i - 1, j} &= \CM_{ii \mid :i - 1} -
    \frac{\CM_{ij \mid :i - 1}^2}{\CM_{jj \mid :i - 1}} \\
  \CM_{jj \mid :i - 1, i} &= \CM_{jj \mid :i - 1} -
    \frac{\CM_{ij \mid :i - 1}^2}{\CM_{ii \mid :i - 1}} \\
                             &= \CM_{jj \mid :i}
\end{align}
Of course, the base case \( \CM_{jj} \) is simply \(
K(\vec{x}_j, \vec{x}_j) \), the variance of the \( j \)th point.

For each of the \( N \) candidates, it requires \( m + s \) operations from
the above updates to compute the objective. Over \( s \) selections, the
total time is the same as the cost to update the Cholesky factor, matching
the complexity of the non-partial multiple point algorithm. However, the
asymptotic work in the non-partial algorithm can be implemented as BLAS
level-2 calls, while the partial algorithm relies heavily on vector (level-1)
calls, affecting the constant-factor performance of the algorithm.

\subsection{Global greedy selection}
\label{app:global_greedy}

Although each column is essentially independent from the perspective of
selection, if there is a prescribed budget for the number of nonzeros then
there is the problem of distributing the nonzeros over the columns. A
natural method is to distribute as evenly as possible, this is efficient and
practically useful. However, one principled way of allocating nonzeros is
to maintain a \emph{global} priority queue over all columns, and selecting
from this queue determines not only which entry out of the candidate set is
added as a nonzero, but also which column to select from. This allows the
algorithm to greedily select the next entry which will decrease the global
objective \cref{eq:obj_chol} as much as possible. The main change is that
within a column, any monotonic transformation of the objective will preserve
the relative ranking of candidates, for example adding a constant, multiplying
by a constant, taking the log, etc. However, from the global perspective, if
one column adds a different constant to their objectives than another column,
the relative ranking of candidates between columns is skewed. Thus, each
column must maintain an objective that corresponds directly to minimizing the
global objective \cref{eq:obj_chol}. Here we describe the modifications that
must be made to the selection algorithms to support global comparison.

\subsubsection{Single column selection}

For a single prediction point, the objective is \( \frac{\CM_{k, \Pred
\mid I}^2}{\CM_{kk \mid I}} \) \cref{eq:obj_gp} which is exactly the amount
the variance of the prediction point is decreased if the \( k \)th candidate is
selected, that is, \( \CM_{\Pred, \Pred \mid I} - \CM_{\Pred,
\Pred \mid I, k} = \frac{\CM_{k, \Pred \mid I}^2}{\CM_{kk \mid
I}} \). From the global perspective, all other prediction points are untouched,
so the amount the sum of the log variances of all the prediction points changes
is
\begin{align}
  \min \Delta &= \min \left [
    \log(\CM_{\Pred, \Pred \mid I, k}) -
  \log(\CM_{\Pred, \Pred \mid I}) \right ] \\
              &= \min \frac{\CM_{\Pred, \Pred \mid I, k}}
                           {\CM_{\Pred, \Pred \mid I}} \\
              &= \min \frac{\CM_{\Pred, \Pred \mid I}
              - \frac{\CM_{k, \Pred \mid I}^2}{\CM_{kk \mid I}}}
                           {\CM_{\Pred, \Pred \mid I}} \\
              &= \min \left [ 1 -
                \frac{\CM_{k, \Pred \mid I}^2}
                     {\CM_{kk \mid I} \CM_{\Pred, \Pred \mid I}}
                 \right ] \\
  \label{eq:global_obj}
              &= \max
                \frac{\CM_{k, \Pred \mid I}^2}
                     {\CM_{kk \mid I} \CM_{\Pred, \Pred \mid I}}
\end{align}
where \cref{eq:global_obj} can be interpreted as the
\emph{percentage} of the decrease in variance from selecting
the \( k \)th point to the variance before selecting the point.

\subsubsection{Aggregated selection}

Since the nonadjacent algorithm directly computes the sum of the log of
the conditional variances of the prediction points, few modifications
have to be made. One heuristical improvement is to take into account
for ``bang-for-buck'', that is, to account for the fact that different
candidates cost a different amount of nonzero entries. Selecting a
candidate can add between 1 and the number of columns in its aggregated
group, depending on its relative index. Thus, candidates with larger
groups will appear to decrease the global variance more, even if they
are not as efficient as a candidate with a single group. In practice, it
is better to use the objective \( \frac{\Delta}{n} \) where \( \Delta
\) is the change in variance after selecting the candidate and \( n \)
is the number of nonzero entries selecting the candidate adds.

\subsection{Equivalence of Cholesky and QR factorization}
\label{app:qr}

We show a well-known fact that QR factorization can be viewed
as the feature-space equivalent of Cholesky factorization,
which can be viewed as operating in covariance-space.
\begin{align}
  \shortintertext{Let \( \CM \) be a symmetric
    positive-definite matrix such that}
  \CM &= F^{\top} F
  \shortintertext{for some matrix \( F \) whose
    columns can be viewed as vectors in feature space:}
  \CM_{ij} &= \langle F_i, F_j \rangle
  \shortintertext{where \( F_i \) is the \( i \)th column of
    \( F \). Now suppose \( F \) has the \( QR \) factorization}
  F &= QR
  \shortintertext{where \( Q \) is a \( N \times N \) orthonormal
    matrix and \( R \) is a \( N \times N \) upper triangular matrix.}
  \CM &= F^{\top} F = (Q R)^{\top} (Q R) \\
         &= R^{\top} Q^{\top} Q R \\
         &= R^{\top} R
\end{align}
from the orthogonality of \( Q \). But note that \( R \) is an upper triangular
matrix, so \( L = R^{\top} \) is an lower triangular matrix. So we have \(
\CM = L L^{\top} \) for lower triangular \( L \). By the uniqueness of
Cholesky factorization, \( R^{\top} \) is precisely the Cholesky factor of \(
\CM \). In addition, the columns of \( Q \) are formed from Gram-Schmitt
orthogonalization on the columns of \( F \) (in feature-space), and \( R \)
the coefficients resulting from the Gram-Schmitt procedure. From \( R^{\top} =
\chol(\CM) \) and the statistical interpretation of Cholesky factorization
\cref{eq:chol}, this iterative orthogonalization in feature-space is equivalent
to iterative conditioning in covariance.

\subsection{Equivalence of selection and orthogonal matching pursuit}
\label{app:omp}

We show that the single-point selection algorithm described in
\cref{alg:select_chol} is the covariance space equivalent to the
feature space orthogonal matching pursuit (OMP) algorithm described
in \cite{tropp2007signal}. The equivalence comes from the fact
that Cholesky factorization is Gram-Schmitt in feature space.

\begin{align}
  \shortintertext{Let \( \CM \) be a symmetric
    positive-definite matrix such that}
  \CM &= F^{\top} F
  \shortintertext{for some matrix \( F \)
    whose columns are vectors in feature space,}
  F &=
  \begin{pmatrix}
    \vec{x}_1 & \vec{x}_2 & \hdots & \vec{x}_N
  \end{pmatrix}
  \shortintertext{Immediately we have}
  \CM_{ij} &= \langle \vec{x}_i, \vec{x}_j \rangle
  \shortintertext{where \( \langle \cdot, \cdot \rangle \)
    denotes the ordinary inner product on \( \mathbb{R}^N \).}
  \shortintertext{It suffices to see a single step of Cholesky
    factorization. Selecting \( \vec{x}_1 \),}
  \CM' &= \CM - \frac{\vec{x}_1 \vec{x}_1^{\top}}
    {\CM_{11}} \\
  \label{eq:cov_step}
  \CM_{ij}' &= \CM_{ij} -
    \frac{\CM_{i1} \CM_{j1}}{\CM_{ii}}
  \shortintertext{Switching to the feature space perspective,
    if we select \( \vec{x}_1 \) we force the rest of the
    feature vectors to be orthogonal to \( \vec{x}_1 \),}
  \vec{x}_i' &= \vec{x}_i -
    \frac{\langle \vec{x}_i, \vec{x}_1 \rangle}
         {\langle \vec{x}_1, \vec{x}_1 \rangle} \vec{x}_1 \\
  \label{eq:feature_step}
  \langle \vec{x}_i', \vec{x}_j' \rangle &=
    \langle \vec{x}_i, \vec{x}_j \rangle -
      \frac{\langle \vec{x}_i, \vec{x}_1 \rangle
            \langle \vec{x}_j, \vec{x}_1 \rangle}
          {\langle \vec{x}_1, \vec{x}_1 \rangle}
  \shortintertext{Comparing \cref{eq:cov_step} and \cref{eq:feature_step},
    we see that they are the same as expected. As a corollary, the objective
    of selecting the point \( \vec{x}_k \) that minimizes the residual of some
    target point \( \vec{x}_\Pred \) can be written as}
  \lVert
    \vec{x}_\Pred - \text{proj}_{\vec{x}_k} \vec{x}_\Pred
  \rVert &= \langle \vec{x}_\Pred, \vec{x}_\Pred \rangle -
    \frac{\langle \vec{x}_\Pred, \vec{x}_k \rangle^2}
        {\langle \vec{x}_k, \vec{x}_k \rangle}
  % \shortintertext{By induction, one can show}
  % F_2' = F_2 - P_1 F_2
  % \shortintertext{where \( F_2 \) is a set of feature
  %   vectors being conditioned and \( P_1 \) is the projection
  %   matrix onto the subspace spanned by \( F_1 \).}
\end{align}
which is precisely the squared covariance of the candidate with the
prediction over the variance of the candidate, as in \cref{eq:obj_gp}.
This shows the equivalence as the objective is the same.

\subsection{Checking submodularity}
\label{app:submodular}

Our objective is the mutual information between the prediction and training
points \cref{eq:info}. A natural question is whether this objective is
submodular with respect to the training set. The answer is no in general,
see \cite{krause2008nearoptimal}, section 8.3 for a simple counterexample.
However, we can empirically check submodularity for particular geometries
and choices of kernel function. If \( \Pred \) is the set of prediction
points, \( I \) is a set of indexes, and \( x_1, x_2 \) are additional
indices not in \( I \), then the set function is submodular if and only if
\begin{align*}
  \MI(\Pred, I \cup \{ x_2 \}) - \MI(I) &\overset{?}{\geq}
    \MI(\Pred, I \cup \{ x_1, x_2 \}) - \MI(\Pred, I \cup \{ x_1 \})
  \shortintertext{Expanding from the definition
    of mutual information \cref{eq:info},}
  \entropy[\Pred \mid I] - \entropy[\Pred \mid I \cup \{ x_2 \}]
    &\overset{?}{\geq}
    \entropy[\Pred \mid I \cup \{ x_1 \}] -
    \entropy[\Pred \mid I \cup \{ x_1, x_2 \}]
  \shortintertext{Since this is the objective \cref{eq:obj_gp_mult}
    (with additional log),}
  \frac{\CM_{x_2, x_2 \mid I}}{\CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\geq}
    \frac{\CM_{x_2, x_2 \mid I} -
          \frac{\CM_{x_1, x_2 \mid I}^2}{\CM_{x_1, x_1 \mid I}}}
         {\CM_{x_2, x_2 \mid I, \Pred} -
         \frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
              {\CM_{x_1, x_1 \mid I, \Pred}}}
    \shortintertext{From the fact that \(
    \frac{a}{b} \geq \frac{a - c}{b - d} \) if and only if
     \( \frac{a}{b} \leq \frac{c}{d} \),}
  \frac{\CM_{x_2, x_2 \mid I}}{\CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\frac{\CM_{x_1, x_2 \mid I}^2}{\CM_{x_1, x_1 \mid I}}}
         {\frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
               {\CM_{x_1, x_1 \mid I, \Pred}}}
  \shortintertext{Multiplying by \( \frac{\CM_{x_1, x_1 \mid
    I}}{\CM_{x_1, x_1 \mid I, \Pred}} \) on both sides,}
  \frac{\CM_{x_1, x_1 \mid I} \CM_{x_2, x_2 \mid I}}
       {\CM_{x_1, x_1 \mid I, \Pred}
        \CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\CM_{x_1, x_2 \mid I}^2}
         {\CM_{x_1, x_2 \mid I, \Pred}^2}
  \shortintertext{Multiplying by \( \frac{\CM_{x_1,
    x_2 \mid I, \Pred}^2}{\CM_{x_1, x_1 \mid
    I} \CM_{x_2, x_2 \mid I}} \) on both sides,}
  \frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
       {\CM_{x_1, x_1 \mid I, \Pred}
        \CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\CM_{x_1, x_2 \mid I}^2}
         {\CM_{x_1, x_1 \mid I} \CM_{x_2, x_2 \mid I}}
  \shortintertext{By definition, this is}
  \Corr[x_1, x_2 \mid I, \Pred] &\overset{?}{\leq}
    \Corr[x_1, x_2 \mid I]
\end{align*}
so the mutual information objective is submodular if and only if
conditioning on additional point(s) decreases the correlation between
every pair of points. Intuitively, this corresponds to the screening
effect observed in spatial statistics literature --- conditioning on
a nearby point decreases the correlation for far away points.

\section{Derivations in KL-minimization}

\subsection{Linear-algebraic formulation of objective}
\label{app:kl_obj}

We want to show that the KL divergence between two multivariate
Gaussians centered at \( \vec{0} \) with covariance matrices
\( \CM_1 \) and \( \CM_2 \) can be written as
\begin{align}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \CM_2)
  \right )
  &= \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
  \shortintertext{where \( \CM_1 \) and \( \CM_2 \) are both
    of size \( N \times N \). Recall that the log density \( \log
    \pi(\vec{x}) \) for \( \vec{x} \sim \mathcal{N}(\vec{0}, \CM) \) is}
  \log \pi(\vec{x}) &= -\frac{1}{2} (N \log(2 \pi) + \logdet(\CM) +
    \vec{x}^{\top} \CM^{-1} \vec{x})
  \shortintertext{By the definition of KL divergence,}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \CM_2)
  \right ) &= 2 \E_P[\log P - \log Q]
  \shortintertext{where \( P \) and \( Q \) are the corresponding
    densities for \( \CM_1 \) and \( \CM_2 \) respectively,
    and \( \E_P \) denotes expectation under \( P \).}
  &= 2 \E_P[-\frac{1}{2} (N \log(2 \pi) + \logdet(\CM_1) +
            \vec{x}^{\top} \CM_1^{-1} \vec{x}) \\
  \nonumber
  & \hphantom{= 2 \E_P[}
            +\frac{1}{2} (N \log(2 \pi) + \logdet(\CM_2) +
            \vec{x}^{\top} \CM_2^{-1} \vec{x})] \\
  \label{eq:kl_after_logdet}
  &= \E_P[\vec{x}^{\top} \CM_2^{-1} \vec{x} -
          \vec{x}^{\top} \CM_1^{-1} \vec{x}]
          + \logdet(\CM_2) - \logdet(\CM_1) \\
  \E_P[\vec{x}^{\top} \CM_2^{-1} \vec{x} -
          \vec{x}^{\top} \CM_1^{-1} \vec{x}]
  &=
  \E_P[\trace(\vec{x}^{\top} \CM_2^{-1} \vec{x}) -
       \trace(\vec{x}^{\top} \CM_1^{-1} \vec{x})]
\end{align}
because the trace of a scalar is a scalar, and the linearity of trace.
\begin{align}
  &=
  \E_P[\trace(\CM_2^{-1} \vec{x} \vec{x}^{\top}) -
       \trace(\CM_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{cyclic property of trace} \\
  &=
  \E_P[\trace(\CM_2^{-1} \vec{x} \vec{x}^{\top} -
              \CM_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{linearity of trace} \\
  &= \E_P[\trace \left (
    (\CM_2^{-1} - \CM_1^{-1}) \vec{x} \vec{x}^{\top} \right )]
  && \text{factoring} \\
  &= \trace(\E_P \left [
    (\CM_2^{-1} - \CM_1^{-1}) \vec{x} \vec{x}^{\top} \right])
  && \text{swapping trace and expectation} \\
  &= \trace((\CM_2^{-1} - \CM_1^{-1})
    \E_P \left [ \vec{x} \vec{x}^{\top} \right])
  && \text{linearity of expectation} \\
  &= \trace((\CM_2^{-1} - \CM_1^{-1}) \CM_1)
  && \text{\( \CM_1 = \E_P[\vec{x} \vec{x}^{\top} \)]} \\
  &= \trace(\CM_2^{-1} \CM_1 - I)
  && \text{multiplying} \\
  &= \trace(\CM_2^{-1} \CM_1) - \trace(I)
  && \text{linearity of trace} \\
  &= \trace(\CM_2^{-1} \CM_1) - N
  \label{eq:kl_after_trace}
  && \text{trace of \( N \times N \) identity \( N \)}
\end{align}
Combining \cref{eq:kl_after_trace} with \cref{eq:kl_after_logdet}, we obtain
\begin{align}
  \nonumber
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \CM_2)
  \right )
  &= \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}
as desired.

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL divergence between \( \CM \) and the Cholesky
factor \( L \) computed according to \cref{thm:L}. From \cref{eq:kl},
\begin{align}
  \label{eq:kl_L}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \trace(L L^{\top} \CM) - \logdet(L L^{\top}) - \logdet(\CM) - N
  \shortintertext{Ignoring terms not depending on \( L \),}
  &= \trace(L L^{\top} \CM) - \logdet(L L^{\top})
  \shortintertext{By the cyclic property of trace,}
  &= \trace(L \CM L^{\top}) - \logdet(L L^{\top})
  \shortintertext{Focusing on \( \trace(L \CM
    L^{\top}) \) and expanding on the columns of \( L \),}
  \trace(L \CM L^{\top}) &= \sum_{i = 1}^N
    L_{s_i, i}^{\top} \CM_{s_i, s_i} L_{s_i, i}
  \shortintertext{Plugging in \( L_{s_i, i} \) from \cref{thm:L},}
  &= \sum_{i = 1}^N
    \left (
      \frac{\left ( \CM_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \CM_{s_i, s_i}
    \left (
      \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right ) \\
  &= \sum_{i = 1}^N
    \frac{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
          \CM_{s_i, s_i} \CM_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1} \\
  &= \sum_{i = 1}^N 1 = N
\end{align}
\begin{align}
  \shortintertext{Using \( N \) for \( \trace(L
    L^{\top} \CM) \) in \cref{eq:kl_L},}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= -\logdet(L L^{\top}) - \logdet(\CM)
  \shortintertext{\( L^{\top} \) has the same log determinant
    as \( L \), and because \( L \) is lower triangular, its
    log determinant is just the sum of its diagonal entries:}
  &= -2 \sum_{i = 1}^N \left [ \log(L_{ii}) \right ] - \logdet(\CM)
  \shortintertext{Plugging \cref{eq:L_col} for the diagonal entries,}
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\CM)
  \shortintertext{Bringing the negative inside,}
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
\end{align}
So minimizing the KL divergence (given optimal \( L \)) corresponds to
minimizing the sum of the inverse of the diagonal entries. We can give
an intuitive view of this result by making use of \cref{eq:L_cond_var}
and expanding the log determinant by the chain rule \cref{eq:det_chain}.
\begin{align}
  \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{ii \mid s_i - \{ i \}} \right )
    \right ]
    - \logdet(\CM) \\
  &= \sum_{i = 1}^N
      \log \left ( \CM_{ii \mid s_i - \{ i \}} \right ) -
    \sum_{i = 1}^N
      \log \left ( \CM_{ii \mid i + 1:} \right ) \\
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{ii \mid s_i - \{ i \}} \right ) -
      \log \left ( \CM_{ii \mid i + 1:} \right )
    \right ]
\end{align}
We can view this sum as the accumulated \emph{difference} in prediction
error for a series of prediction problems, where each prediction problem is
to predict the value of the \( i \)th variable given variables \( i + 1, i
+ 2, \dotsc, N \). The left term \( \log \left ( \CM_{ii \mid s_i - \{ i
\}} \right ) \) is restricted to only using those variables in the sparsity
pattern \( s_i \), while the right term \( \log \left ( \CM_{ii \mid i +
1:} \right ) \) is able to use every variable after \( i \). The left term
will necessarily have greater variance than the right, and the goal is to
minimize the accumulated deviation. Thus, the KL divergence gives a natural
way to measure the quality of a sparsity pattern as a good sparsity pattern
should maintain predictive accuracy while subject to the constraint that
some variables have no interaction with others. This interpretation is also
given in \cite{katzfuss2022scalable}.

Another interpretation is from \cite{bartels2022adaptive}, where they view
the full prediction problems as the log likelihood of the variables. Under
this interpretation, conditional independence (through the screening effect)
corresponds to a near-constant value of \( \log(\CM_{ii \mid i + 1:}) \),
which results in a linear plot of log-likelihood with increasing \( N \).

Because the KL divergence is not symmetric, it matters which way the
KL divergence is taken as well as whether both matrices have been
inverted or not. This seems to imply that there are four possible
ways to compare two covariance matrices. However, note that
\begin{align}
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \CM) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right ) &=
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, L L^{\top}) \, \Big \| \,
    \mathcal{N}(\vec{0}, \CM^{-1})
  \right )
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL divergence. There are
therefore only two possible ways to compare the two, which depends on
the order of the arguments. A statistical interpretation comes from the
fact that the KL divergence can be interpreted as the likelihood-ratio
test, so the non-symmetry of the order of the arguments corresponds to
the asymmetry between the null and alternative hypotheses.

\section{Algorithms}

\begin{algorithm}
  \caption{Direct Gaussian process regression by selection}
  \label{alg:infer_select}
  \input{figures/algorithms/infer_select.tex}
\end{algorithm}

\end{document}
