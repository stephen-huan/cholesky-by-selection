% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Sparse Cholesky factorization by greedy conditional selection},
  pdfauthor={S. Huan, J. Guinness, M. Katzfuss, H. Owhadi, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Dense kernel matrices resulting from pairwise evaluations of a
  kernel function arise naturally in machine learning and statistics.
  Previous work in constructing sparse transport maps or sparse approximate
  inverse Cholesky factors of such matrices by minimizing Kullback-Leibler
  divergence recovers the Vecchia approximation for Gaussian processes.
  However, these methods often rely only on geometry to construct the
  sparsity pattern, ignoring the conditional effect of adding an entry.
  In this work, we construct the sparsity pattern by leveraging a
  greedy selection algorithm that maximizes mutual information with
  target points, conditional on all points selected previously.
  For selecting \( k \) points out of \( N \), the naive time
  complexity is \( \BigO(N k^4) \), but by maintaining a
  partial Cholesky factor we reduce this to \( \BigO(N k^2) \).
  Furthermore, for multiple (\( m \)) targets we achieve a time complexity
  of \( \BigO(N k^2 + N m^2 + m^3) \) which is maintained in the setting of
  aggregated Cholesky factorization where a selected point need not condition
  every target.
  We directly apply the selection algorithm to image
  classification and recovery of sparse Cholesky
  factors, improving upon \( k \)-th nearest neighbors.
  By minimizing Kullback-Leibler divergence, we apply the
  algorithm to Cholesky factorization, Gaussian process
  regression, and preconditioning with the conjugate gradient.
\end{abstract}

% REQUIRED
\begin{keywords}
  \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}

\paragraph{The problem}

Gaussian processes enjoy widespread application in spatial
statistics and geostatistics \cite{rue2005gaussian}, machine
learning through kernel methods \cite{rasmussen2006gaussian},
optimal experimental design \cite{mutny2022experimental}, and
sensor placement \cite{krause2008nearoptimal}.
However, Gaussian process statistics from \( N \) data points requires
computing with the covariance matrix \( \CM \in \Reals^{N \times N} \)
to obtain quantities such as \( \CM \vec{v} \), \( \CM^{-1} \vec{v} \),
\( \logdet(\CM) \).
For dense \( \CM \), directly computing these quantities has a
computational cost of \( \BigO(N^3) \) and a memory cost of \(
\BigO(N^2) \), which is prohibitively expensive for large \( N \).
Beyond Gaussian processes, computations with large positive-definite
matrices are required across computational mathematics,
motivating the search for faster, approximate algorithms.

\paragraph{Existing work}

\todo{I would put knothe rosenblatt, FSAI, and sparse Cholesky
factorization into the Vecchia paragraph and use the ``existing work''
paragraph to disucuss other approaches such as inducing points,
wavelet method, h matrices, fast multiplol, random features, Nystrom
approximation, sparse Cholesky of covariance, etc.}
TODO

\paragraph{Vecchia approximation}

\todo{I would make this one integrated section on
Vecchia, FSAI, KL-Cholesky, and transport maps}
As \cite{katzfuss2021general} points out, many sparse
factorizations of covariance matrices can be viewed as special
cases of the Vecchia approximation \cite{vecchia1988estimation}.
The Vecchia approximation decomposes the joint distribution
into the product of univariate densities, each density
conditional on only a subset of those prior in the ordering.
If an ordering and sparsity pattern are determined, then
an approximate Cholesky factor obeying the sparsity
can be computed by optimizing a chosen objective.
Within the context of factorized sparse approximate inverse (FSAI)
preconditioners, the Kaporin condition number \cite{kaporin1990alternative}
and the Frobenius norm with an additional Jacobi scaling constraint
\cite{yeremin2000factorized} have been proposed as objectives, and
from a statistical perspective the Kullback-Leibler (KL) divergence
\cite{schafer2021sparse} has been proposed.
These three objectives actually converge to the same closed-form expression
for the entries of the resulting Cholesky factor \cite{schafer2021sparse},
and this expression is equivalent to the formula used to compute the Vecchia
approximation for Gaussian processes \cite{vecchia1988estimation}.
In addition, using the KL divergence to compute Cholesky factors
can be viewed as a special case of computing sparse transport
maps in \cite{marzouk2016introduction, katzfuss2022scalable}.
Transport maps with Knothe-Rosenblatt structure generalize Cholesky factors to
nonlinear and non-Gaussian distributions while preserving lower triangularity
and sparsity \cite{spantini2018inference}, and have been applied to simulation
and sampling problems \cite{marzouk2016introduction, katzfuss2022scalable}.
Finally, exploiting the independence of the Vecchia approximation allows
for embarrassingly parallel algorithms for Gaussian process regression
\cite{katzfuss2021general}, Cholesky factorization \cite{schafer2021sparse},
and transport map construction \cite{marzouk2016introduction}.

\paragraph{Ordering and sparsity selection by geometry}

\todo{Might want to add a quick comment explaining the maximin ordering here?}
The selected ordering and sparsity pattern have a
significant effect on the quality of the resulting factor.
Vecchia originally proposed ordering points lexicographically
\cite{vecchia1988estimation}, but more recent work exploits space-covering
orderings like the maximum-minimum ordering \cite{guinness2018permutation},
which has become popular \cite{schafer2020compression, schafer2021sparse,
katzfuss2021general, kang2021correlationbased, katzfuss2022scalable}.
Once the ordering is fixed, the sparsity set for a particular
point can include any point prior to it in the ordering.
Following Vecchia's recommendation, the points are often chosen to be
the closest points by Euclidean distance \cite{vecchia1988estimation,
schafer2020compression, schafer2021sparse, katzfuss2022scalable}.
This choice is often justified by noting that popular kernel functions
like the Mat{\'e}rn family decay exponentially with increasing distance.
For more general kernel functions, geostatisticians have long observed the
``screening effect'', or the observation that conditional on points close to
the point of interest, far away points are nearly conditionally independent
\cite{stein2002screening, stein20112010} (see \cref{fig:screening}).
However, selecting by distance ignores the conditional
effect of adding new points to the sparsity set.
As an illustrative example, imagine the closest point
to the point of interest is duplicated multiple times.
Conditional on the duplicated point, the
duplicates provide no additional information.
But they are still closest to the point of interest, so selecting by
distance alone would still add these redundant points to the sparsity set.

\paragraph{Conditional selection}

\todo{We should start with something like ``In this work we propose
a conditional...'' or so. That is, first make it clear that applying
conditional selection is the main contribution of the present work
and then relate it to all the other approaches / techniques.}
Instead of adding points to the sparsity pattern by distance, we propose
greedily selecting points that maximize mutual information with the
point of interest, conditional on all points selected previously.
The machine learning community has long developed similar algorithms that
greedily optimize information-theoretic objectives in the context of Gaussian
process inference \cite{smola2000sparse, herbrich2002fast, seeger2003fast}.
Similar algorithms have also been developed in the context of
sensor placement \cite{krause2008nearoptimal, clark2018greedy}
and experimental design \cite{mutny2022experimental} where it is
assumed the target phenomena is modeled by a Gaussian process or
is otherwise linearly dependent on the selected measurements.
However, these works often focus on obtaining a global approximation of
the entire process, e.g. through sparse approximation of the likelihood
or covariance matrix (see \cite{liu2020when, chalupka2012framework,
quinonero-candela2005unifying} for a comprehensive overview).
To the best of our knowledge \cite{wada2013gaussian} is the first to
propose \emph{directed} inference for a point of interest, in which
they use the kernel function itself as the objective like the later
work \cite{kang2021correlationbased}.
Seemingly independently of \cite{wada2013gaussian}, \cite{gramacy2014local}
and the follow up work \cite{gramacy2015speeding} use the active learning Cohn
(ALC) objective first described in \cite{cohn1996neural} for directed Gaussian
process regression, yielding an algorithm equivalent to our proposed algorithm
in the case of a single point of interest.

Our proposed selection method can also be viewed as the covariance
equivalent of orthogonal matching pursuit (OMP) \cite{tropp2007signal,
tropp2006algorithms}, a workhorse algorithm popular in signal processing and
compressive sensing which seeks to approximate in feature space a target vector
as the sparse linear combination from a given collection of vectors.

\paragraph{Main results}

\todo{Want to start a one-sentence descrption of our contributions, something
like ``We propose...''. We should first fully describe what we are doing
(greedy selection, integration into the KL framework etc.) We want to start
talking about technical things like the computational costs and the improvement
only afterward, once we have established what it is that we are accelerating.}
Our main contribution is a selection algorithm that greedily maximizes
conditional mutual information with a point of interest, which we use to select
sparsity entries for sparse approximate Cholesky factors of precision matrices.
We compute Cholesky factors through the KL-minimization framework
of \cite{schafer2021sparse}, but use conditional selection
to form the sparsity pattern instead of nearest neighbors.
The final algorithm extends the kernel-based selection algorithms
\cite{wada2013gaussian, kang2021correlationbased} to account for conditioning
and extends directed Gaussian process regression \cite{gramacy2014local}
to multiple points as well as global approximation.
For a single point of interest, direct computation of the mutual information
criterion would have time complexity \( \BigO(N k^4) \) to select \( k \)
points out of \( N \), but by maintaining a partial Cholesky factor we reduce
the complexity to \( \BigO(N k^2) \).
We extend the algorithm to maximize mutual information
with \emph{multiple} points of interest, naturally taking
advantage of the ``two birds with one stone'' effect.
For \( m \) target points we achieve a time complexity of \(
\BigO(N k^2 + N m^2 + m^3) \), which for \( m \approx k \) is
essentially \( m \) times faster than the single-point algorithm.
In the setting of aggregated (or supernodal) Cholesky factorization where
the sparsity patterns of multiple columns are determined together, a
candidate entry may only condition a \emph{partial} subset of the targets.
By carefully applying rank-one downdating of Cholesky factors, we
capture this structure at the same time complexity for multiple points.
When applying our greedy selection method for sparsity selection
of Cholesky factors, we achieve more accurate factors for the same
number of nonzeros compared to selection by nearest neighbors.
Finally, we show how to adaptively determine the number of nonzeros per
column in order to reduce the global KL divergence by maintaining a global
priority queue shared between each local greedy selection algorithm.

\paragraph{Outline}

\Stodo{todo once more of the paper is solidified}
The remainder of the paper is organized as follows.

\begin{figure}[t]
  \centering
  \input{figures/screening/uncond.tex}%
  \input{figures/screening/cond.tex}
  \caption{An illustration of the screening effect with the Mat{\'e}rn kernel
    with length scale \( \ell = 1 \) and smoothness \( \nu = \frac{5}{2} \).
    The first figure shows the unconditional covariance with the point at (0,
    0). The second figure shows the conditional covariance after conditioning
    on the four points in orange.}
  \label{fig:screening}
\end{figure}

\Ftodo{``specific'' and ``intuitively'' are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much
as possible. It's normal to add them out of reflex initially, so it
requires active postprocessing.}

\section{Sparse Cholesky factorization by KL-minimization}
\label{sec:chol}

Let \( \Theta \in \Reals^{N \times N} \) be a positive-definite matrix.
We view \( \Theta \) as the covariance matrix of a Gaussian process;
we say a function \( f(\vec{x}) \) is distributed according to a
Gaussian process prior with mean function \( \mean(\vec{x}) \) and
covariance function or kernel function \( \K(\vec{x}, \vec{x}')
\), if for any finite set of points \( X = \{ \vec{x}_i \}^N_{i =
1} \), \( f(X) \sim \N(\vec{\mean}, \CM) \), where \( \mean_i =
\mu(\vec{x}_i) \) and \( \CM_{ij} = \K(\vec{x}_i, \vec{x}_j) \).
\begin{align}
  f(\vec{x}) &\sim \GP(\mean(\vec{x}), \K(\vec{x}, \vec{x}'))
\end{align}
For any positive-definite matrix \( \CM \) there exists a set of points \( X
\) and kernel function \( \K \) such that \( \CM = \K(X, X) \), and for any \(
X \) the matrix \( \K(X, X) \) is positive-definite, so we freely move between
positive-definite matrices and point sets with kernel functions.

In many applications of Gaussian processes we wish
to infer unknown data given a training dataset.
Given the dataset \( \mathcal{D} = \{ (\vec{x}_i, y_i) \}^N_{i = 1} \)
where the inputs \( \vec{x}_i \in \Reals^D \) are collected in the matrix
\( X_\Train = [\vec{x}_1, \dotsc, \vec{x}_N]^{\top} \in \Reals^{N \times
D} \) and the measurements at those points are collected in the vector \(
\vec{y}_\Train = [y_1, \dotsc, y_N]^{\top} \in \Reals^N \), we wish to
predict the values at \( M \) new points \( X_\Pred \in \Reals^{m \times
D} \) for which \( \vec{y}_\Pred \in \Reals^m \) is unknown.
We assume the function \( f(\vec{x}) \) that maps input points to their
outputs is distributed according to a Gaussian process with zero mean
function, \( f(\vec{x}) \sim \GP(\vec{0}, \K(\vec{x}, \vec{x}')) \).
From the distribution of \( f(\vec{x}) \), the joint distribution
of training and testing data \( \vec{y} \) has covariance
\(
  \CM =
  \begin{pmatrix}
    \CM_{\Train, \Train} & \CM_{\Train, \Pred} \\
    \CM_{\Pred, \Train} & \CM_{\Pred, \Pred}
  \end{pmatrix}
\)
where \( \CM_{\I, \J} \defeq \K(X_\I, X_\J) \) for index sets \( \I, \J \).
In order to make predictions at \( X_\Pred \), we condition the desired
prediction \( \vec{y}_\Pred \) on the known data \( \vec{y}_\Train \).
For Gaussian processes, the closed-form posterior distribution is
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \vec{\mean}_\Pred +
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    (\vec{y}_\Train - \vec{\mean}_\Train) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \CM_{\Pred, \Pred} -
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    \CM_{\Train, \Pred}
  \shortintertext{For brevity of notation, we
    denote the conditional covariance matrix as}
  \label{eq:cond_cov_notation}
  \CM_{\I, \J \mid \V} &\defeq
    \CM_{\I, \J} - \CM_{\I, \V} \CM_{\V, \V}^{-1} \CM_{\V, \J}
  \shortintertext{where \( \CM_{\V, \V}^{-1} \defeq (\CM_{\V, \V})^{-1} \).
    To denote the covariance between the variables in index sets \( \I \) and
    \( \J \), conditional on the variables in \( \V_1, \V_2, \dots, \V_n \) we
    write}
  \CM_{I, J \mid \V_1, \V_2, \dotsc, \V_n} &\defeq
    \Cov[\vec{y}_\I, \vec{y}_\J \mid
         \vec{y}_{\V_1 \cup \V_2 \cup \dotsb \cup \V_n}]
  \shortintertext{The sets \( \V_1, \dotsc, \V_n \) are written in order
    of computation. Although the final covariance matrix is the same, a
    different order of conditioning means different intermediate results
    in repeated application of \cref{eq:cond_cov}. By the quotient rule
    of Schur complementation:}
  \label{eq:quotient_rule}
  \CM_{\I,   \J   \mid \V_{1 \dots n}} &=
  \CM_{\I,   \J   \mid \V_{1 \dots n - 1}} -
  \CM_{\I,   \V_n \mid \V_{1 \dots n - 1}}
  \CM_{\V_n, \V_n \mid \V_{1 \dots n - 1}}^{-1}
  \CM_{\V_n, \J   \mid \V_{1 \dots n - 1}}
\end{align}
Calculating the posterior mean \cref{eq:cond_mean} and covariance
\cref{eq:cond_cov} requires inverting the training covariance matrix.
Often the inverse Cholesky factor \( L = \chol(\CM_{\Train,
\Train})^{-1} \) is used instead of the direct inverse \( \CM_{\Train,
\Train}^{-1} \) for improved numerical stability and performance.
However, the time complexity of computing the Cholesky factorization
is \( \BigO(N^3) \), which is prohibitive for large \( N \).
For computational efficiency, we will enforce that \(
L \) is \emph{sparse}, yielding an approximate factor.
A significant factor affecting the approximation quality is whether \( L
\) is computed as an approximate inverse factor of the covariance (\( L
\chol(\Theta) \approx \Id \)) or as an approximate factor of the precision
(\( L L^{\top} \approx \Theta^{-1} \)).

\subsection{Vecchia approximation}
\label{subsec:vecchia}

Both sparse inverse factors of the covariance and sparse
factors of the precision can be computed from the Vecchia
approximation for Gaussian processes \cite{vecchia1988estimation}.
We start by decomposing the joint likelihood \( \pi \).
\begin{align}
  \label{eq:joint}
  \p(\vec{x}) &= \p(x_1) \p(x_2 \mid x_1) \p(x_3 \mid x_1, x_2) \dots
    \p(x_N \mid x_1, x_2, \dotsc, x_{N - 1})
  \shortintertext{The key assumption is that many of the points are
    redundant after conditioning on a carefully chosen subset of the
    points. Letting \( i_1, \dotsc, i_N \) denote an ordering of the
    points and \( s_k \) the indices of points that condition the \(
    k \)th point in the ordering, the Vecchia approximation proposes
    to approximate \cref{eq:joint} by the sparse approximation}
  \label{eq:vecchia}
  \p(\vec{x}) &\approx \p(x_{i_1}) \p(x_{i_2} \mid x_{s_2})
    \p(x_{i_3} \mid x_{s_3}) \dots \p(x_{i_N} \mid x_{s_N})
\end{align}
Following \cref{eq:vecchia} if an elimination ordering \( \prec \) is fixed
(a permutation of \( \{ 1, \dotsc, N \} \)) and a lower triangular sparsity
pattern \( S \defeq \{ (i, j) : i \in s_j, i \succeq j \} \) is specified
then all is needed is a functional criterion \( \Loss: \Reals^{N \times N}
\to \Reals \) to specify the optimization problem
\begin{align}
  \label{eq:generic_obj}
  L &\defeq \argmin_{\hat{L} \in \SpSet} \Loss(\hat{L})
\end{align}
where \( \SpSet \defeq \{ M \in \Reals^{N \times N} :
M_{ij} \neq 0 \Rightarrow (i, j) \in S \} \) is the space
of matrices satisfying the sparsity pattern \( S \).
Proposed functionals which compute inverse Cholesky factors of the
covariance, \( L \chol(\CM) \approx \Id \), include the Kaporin
condition number \( (\trace(L \CM L^{\top})/N)^N/\det(L \CM L^{\top})
\) \cite{kaporin1990alternative} and the Frobenius norm \( \norm{\Id
- L \chol(\CM)}_{\FRO} \) additionally subject to the constraint \(
\diag(L \CM L^{\top}) = 1 \) \cite{yeremin2000factorized}, while the KL
divergence \( \KL{\N(\vec{0}, \CM)} {\N(\vec{0}, (L L^{\top})^{-1})}
\) \cite{schafer2021sparse} computes factors of the precision, \( L
L^{\top} \approx \CM^{-1} \).

As observed in \cite{schafer2021sparse}, factors of the precision are often
much sparser than factors of the covariance, because the precision encodes
conditional independence while the covariance encodes marginal independence.
The same phenomenon is observed by \cite{spantini2018inference}
working with the more general transport maps.
Covariance matrices arising from kernel functions are often fully
dense, but the approximate factors of their precision can be
sparse if the ordering and sparsity pattern are chosen carefully.

\subsection{Ordering and sparsity pattern}
\label{subsec:ordering}

Although in this work we focus on constructing sparsity
patterns, a good ordering is critical since the sparsity for
a point can only include points after it in the ordering.
Vecchia originally proposed ordering points lexicographically, which is
most natural in a one-dimensional setting \cite{vecchia1988estimation}.
More recent work finds that in higher dimensions, exploiting
space-covering orderings leads to significantly better
approximation quality \cite{guinness2018permutation}.
We specifically use the maximum-minimum (maximin) ordering
\cite{guinness2018permutation}, which has become popular for the
Vecchia approximation \cite{katzfuss2021general} and Cholesky
factorization \cite{schafer2020compression, schafer2021sparse,
kang2021correlationbased, katzfuss2022scalable}.
The reverse-maximin ordering \( i_1, \dotsc, i_N \) on a set of \( N \) points
\( \{ \vec{x}_i \}^N_{i = 1} \) is defined by first selecting the last index \(
i_N \) arbitrarily and then choosing for \( k = N - 1, N - 2, \dotsc, 1 \) the
index
\begin{align}
  i_k = \argmax_{i \in -\Order_{k + 1}} \; \min_{j \in \Order_{k + 1}}
    \norm{\vec{x}_i - \vec{x}_j}
\end{align}
where \( -\Order \defeq \{ 1, \dotsc, N \} \setminus \Order \)
and \( \Order_n \defeq \{ i_n, i_{n + 1}, \dotsc, i_N \} \),
i.e. select the point farthest from previously selected points.
The ordering is reversed for factorizing the precision.

Vecchia also originally proposed to select the sparsity set by Euclidean
distance \cite{vecchia1988estimation}, which, unlike the lexicographic
ordering, still remains widely used \cite{schafer2020compression,
schafer2021sparse, katzfuss2022scalable}.
Instead, we show how to select the sparsity pattern to directly
optimize the objective \( \Loss \) \cref{eq:generic_obj} in an
end-to-end manner by decomposing the KL divergence along each column.

\subsection{Review of KL-minimization}
\label{subsec:kl}

The Kullback-Leibler (KL) divergence between two probability distributions \( P
\) and \( Q \) is defined as \( \KL*{P}{Q} \defeq \E_P[\log(\frac{P}{Q})] \).
As the expected difference between true and approximate
log-densities, the KL divergence is a natural way to
judge the quality of an approximating distribution.
In line with the Gaussian process regression problem in \cref{sec:chol}, we
identify the positive-definite matrix \( \CM \in \Reals^{N \times N} \) as
the covariance of a centered Gaussian process \( \N(\vec{0}, \CM) \) which we
seek to approximate by a sparse approximate Cholesky factor \( L \in \SpSet
\) of its precision, \( \N(\vec{0}, (L L^{\top})^{-1}) \).
In order to compare the resulting distributions, we specialize
the generic optimization problem \cref{eq:generic_obj}
to the KL divergence as \cite{schafer2021sparse} does.
\begin{align}
  \label{eq:L_obj}
  L \defeq \argmin_{\hat{L} \in S} \,
    \KL*{\N(\vec{0}, \CM)}
        {\N(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})}
\end{align}
For multivariate Gaussians, the KL divergence
has a closed-form expression given by
\begin{align}
  \label{eq:kl}
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}
where \( \CM_1, \CM_2 \in \Reals^{N \times
N} \) (see \cref{app:kl_obj} for details).
Using this expression for the KL divergence and optimizing for \( L \)
yields the following closed-form expression for the \( i \)th column of
\( L \) with sparsity pattern \( s_i \), reproduced from Theorem 2.1 of
\cite{schafer2021sparse}:
\begin{align}
  \label{eq:L_col}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
\end{align}
where the notation for \( \CM_{s_i, s_i}^{-1} \) is from
\cref{eq:cond_cov_notation} and \( \vec{e}_1 \in \Reals^{\card{s_i}
\times 1} \) denotes the vector with first entry one and the rest zero.
We enforce the convention that \( i \) is the first entry
of \( s_i \), also implying that \( L \) is of full rank.
Plugging the optimal \( L \) \cref{eq:L_col} back into the KL divergence
\cref{eq:kl}, we obtain the objective as a function of the sparsity pattern.
\begin{align}
  \label{eq:obj_chol}
  2 \KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} &=
    \sum_{i = 1}^N
      \left [
        \log \left (
          (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
        \right )
      \right ]
      - \logdet(\CM)
\end{align}
See \cref{app:kl_L} for details. In particular, it is important
which direction the KL divergence is taken or cancellation
of the \( \trace(\CM_2^{-1} \CM_1) \) term may not occur.

In order to pick a sparsity pattern that minimizes the KL divergence
\cref{eq:obj_chol}, we can ignore the constant \( \logdet(\CM) \) and minimize
over each column independently, as each term \( (\vec{e}_1^{\top} \CM_{s_i,
s_i}^{-1} \vec{e}_1)^{-1} \) in the sum depends only on \( s_i \).
To do so, we recall that conditioning in
covariance is marginalization in precision.
For
\( \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\),
\begin{align}
  \label{eq:inverse_cond}
  \CM_{1, 1 \mid 2} &= \left ( \CM^{-1} \right )_{1, 1}^{-1} \\
  \shortintertext{Viewing \( (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
    \vec{e}_1)^{-1} = \left (\CM_{s_i, s_i}^{-1} \right )_{1, 1}^{-1}
    \) as a marginalization of \( \CM_{s_i, s_i}^{-1} \) to apply
    \cref{eq:inverse_cond},}
  \label{eq:L_cond_var}
  (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= \CM_{i, i \mid s_i \setminus \{ i \}}
\end{align}
So minimizing the KL divergence of a sparse Cholesky factor is equivalent
to independently minimizing over each column the variance of the \( i
\)th variable, conditional on the entries \( s_i \) (excluding itself).
Decomposition of the KL divergence into independent
regression problems was also observed in
\cite{katzfuss2022scalable} for lower triangular transport maps.

\subsubsection{Aggregated sparsity pattern}
\label{subsubsec:aggregated}

We can derive a similar decomposition of the KL divergence
if the same sparsity pattern is reused for multiple columns,
known as aggregated or supernodal Cholesky factorization.
Aggregation can lead to substantial asymptotic
time and space savings \cite{schafer2021sparse}.
Throughout the following discussion we will focus on a single group
\( \tilde{i} = \{i_1, \dots, i_m \} \) composed from aggregating
the column indices \( i_1 \succ i_2 \succ \dotsb \succ i_m \).
Let \( \tilde{i} \) have aggregated sparsity pattern \(
s_{\tilde{i}} \) satisfying \( s_{\tilde{i}} \supseteq \tilde{i}
\) to guarantee that the Cholesky factor has full rank.
Let \( \tilde{s} \defeq s_{\tilde{i}} \setminus \tilde{i} \) be the aggregated
sparsity pattern excluding the diagonal entries of the columns in the group.
The sparsity pattern of the \( k \)th column in the group is then
the aggregated sparsity pattern excluding the entries that violate lower
triangularity, \( s_k \defeq \{ j \in s_{\tilde{i}} : j \succeq k \} \).
Assuming every entry of \( \tilde{s} \) is after every index in \( \tilde{i}
\), then \( s_k = \tilde{s} \cup \{ j \in \tilde{i} : j \succeq k \} \).
This condition is guaranteed if the aggregated
columns are adjacent in the ordering, for example.
We defer handling the non-adjacent case to \cref{subsubsec:partial}.
We now simplify the KL minimization objective on aggregated
indices with the conditional chain rule of log determinants.
Using the same blocking as \cref{eq:inverse_cond},
\begin{align}
  \label{eq:det_chain}
  \logdet(\CM) &= \logdet(\CM_{1, 1}) + \logdet(\CM_{2, 2 \mid 1})
\end{align}
The KL divergence objective \cref{eq:obj_chol} restricted
to the contribution from the group \( \tilde{i} \) is
\begin{align}
  \sum_{i \in \tilde{i}} \log(\CM_{i \mid s_i \setminus \{ i \} }) &=
    \log(\CM_{i_1 \mid \tilde{s}}) +
    \log(\CM_{i_2 \mid \tilde{s} \cup \{ i_1 \}}) + \dotsb +
    \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{where we write \( \CM_j \defeq \CM_{j, j} \).
    Combining the first two terms by the chain rule \cref{eq:det_chain},}
  &= \logdet(\CM_{\{ i_1, i_2 \} \mid \tilde{s}}) +
     \log(\CM_{i_3 \mid \tilde{s} \cup \{ i_1, i_2 \}}) + \dotsb +
     \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{Proceeding by induction, we are
    able to reduce the entire sum to the single term}
  \label{eq:obj_mult}
  &= \logdet(\CM_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}
So the suitable generalization of the conditional variance
in \cref{eq:obj_chol} to aggregated columns is the log
determinant of the covariance matrix of the columns,
conditional on (well-behaved) selected entries.
We briefly discuss what happens in the general case.

\subsubsection{Non-adjacent or partial aggregation}
\label{subsubsec:partial}

\begin{figure}[t]
  \centering
  \input{figures/partial_factor.tex}
  \caption{Illustration of the Cholesky factorization of a partially
    conditioned covariance matrix. Here \textcolor{darksilver}{grey}
    denotes fully unconditional, \textcolor{darklightblue}{blue} denotes
    fully conditional, and the \textcolor{silver!50!lightblue}{mixed
    color} denotes interaction between the two. Surprisingly, such a
    matrix factors into a ``pure'' Cholesky factor.}
  \label{fig:partial_factor}
\end{figure}

Let the random variables corresponding to the indices \( i_1, \dotsc, i_m
\) be collected in a vector \( \vec{y} = [y_1, \dotsc, y_m]^{\top} \) with
joint density multivariate Gaussian with covariance matrix \( \CM \).
We select an index \( k \) that conditions the variables, but ignores the first
\( p \) variables (recall that the indices are sorted in decreasing order,
so if \( k \) conditions a variable, it conditions everything afterwards).
After this partial conditioning the variables become \( \vec{y}_{\parallel
k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k}, \dotsc, y_{m \mid k} \)
and the covariance matrix \( \Cov[\vec{y}_{\parallel k}] \) becomes
\begin{align}
  \label{eq:chol_partial}
  \CM_{\tilde{i}, \tilde{i} \parallel k} &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}
where \( L = \chol(\CM) \) and \( L' = \chol(\CM_{\mid
k}) \) (see \cref{app:partial} for details).
As depicted in \cref{fig:partial_factor}, the Cholesky factor of \(
\CM_{\parallel k} \) is composed of ``gluing'' the prefix of the unconditional
factor \( L \) with the suffix of the conditional factor \( L' \).
Armed with this representation, we show the equivalence between minimizing \(
\logdet(\CM_{\parallel k}) \) and minimizing KL divergence \cref{eq:obj_chol}.
We compute \( \logdet(\CM_{\parallel k}) \) by its Cholesky
factor \cref{eq:chol_partial} and recall the determinant of
a triangular matrix is the product of its diagonal entries.
\begin{align}
  \label{eq:partial_logdet}
  \frac{1}{2} \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
  \underbrace{\log(L_{1, 1}) + \dotsb + \log(L_{p, p})}_{\text{the same}} +
  \underbrace{
    \log({L'}_{p + 1, p + 1}) + \dotsb + \log({L'}_{m, m})
  }_{\text{conditioned}}
  \shortintertext{Comparing to the KL divergence \cref{eq:obj_chol}
    and recalling that \( k \) is added to \( s_i \) if \( i > p \),}
  \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
  \label{eq:partial_kl}
  &= \underbrace{
      \log \left ( \CM_{1, 1 \mid s_1 \setminus \{ 1 \}} \right ) + \dotsb +
      \log \left ( \CM_{p, p \mid s_p \setminus \{ p \}} \right )
     }_\text{the same} + \\
  \nonumber
  % disgusting hack, do properly later --- looks good enough to me, though
  & \hphantom{=} \: \,
  \underbrace{
    \log \left (
      \CM_{p + 1, p + 1 \mid s_{p + 1} \setminus \{ p + 1 \}}
    \right ) + \dotsb +
    \log \left ( \CM_{m, m \mid s_m \setminus \{ m \}} \right )
  }_\text{conditioned}
  \shortintertext{Since \( L_{i, i} \) (and \( {L'}_{i, i} \)) is the
    square root of the conditional variance of the \( i \)th variable from
    the staistical perspective in \cref{eq:chol}, we have \( 2 \log(L_{i,
    i}) = \log(\CM_{i, i \mid s_i \setminus \{ i \}}) \) and so}
  \nonumber
  \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
    \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
\end{align}
Like the aggregated case in \cref{subsubsec:aggregated},
minimizing the log determinant of the \emph{partially} conditioned
covariance matrix is the same as minimizing the KL divergence.

In every case we have considered, picking the right sparsity pattern
to minimize KL divergence reduces to selecting the points \( s_i \)
out of the possible candidates \( i + 1, \dotsc, N \) that most reduce
predictive error at point(s) of interest.
In the next section, we develop such a selection
algorithm for directed inference in Gaussian processes.
We apply this algorithm for sparsity selection
of Cholesky factors in \cref{sec:chol_select}.

\section{Greedy selection for directed inference}
\label{sec:select}

\begin{figure}[t]
  \centering
  \input{figures/selection/knn.tex}%
  \input{figures/selection/cknn.tex}
  \caption{Here, the \textcolor{lightblue}{blue} points are the
    \textcolor{lightblue}{candidates}, the \textcolor{orange}{orange}
    point is the \textcolor{orange}{unknown} point to predict at, and the
    \textcolor{seagreen}{green} points are the \textcolor{seagreen}{selected}
    points. The \textcolor{rust}{red} line is the \textcolor{rust}{conditional
    mean} \( \mu \), conditional on the selected points, and the \( \pm 2
    \sigma \) confidence interval is shaded for the conditional variance
    \( \var \). On the left is selection by Euclidean distance and on the
    right is selection by conditional variance.}
  \label{fig:selection}
\end{figure}

In Gaussian process regression, we are given \( N \) points of training
data \( X_\Train \in \Reals^{N \times D} \) with associated measurements
\( \vec{y}_\Pred \in \Reals^N \) and wish to predict at \( m \) points
\( X_\Pred \in \Reals^{m \times D} \) whose values \( \vec{y}_\Pred \in
\Reals^m \) are unknown.
Unfortunately, computing the conditional distribution of \( \vec{y}_\Pred
\) by the equations \cref{eq:cond_mean} and \cref{eq:cond_cov}
has computational time complexity \( \mathcal{O}(N^3) \).
Instead, we carefully select a subset of \( s \) points out of the
\( N \), \( s \ll N \), and pay a much smaller \( \mathcal{O}(s^3)
\) cost by using only this subset to make predictions.
Of course, throwing away \( N - s \) points worsens the predictive accuracy.
Maintaining reasonable accuracy at a reduced computational
cost requires a criterion to choose the most effective points.

From minimizing the KL divergence, the criteria should
be to minimize the variance of the target point
conditional on the selected points \cref{eq:L_cond_var}.
The variance objective was first described by \cite{cohn1996neural} for optimal
experimental design and later applied to directed Gaussian process inference
by \cite{gramacy2014local} who refer to it as the active learning Cohn (ALC)
objective in honor of \cite{cohn1996neural}.
In addition, the variance objective is equivalent to maximizing
the \emph{mutual information} or \emph{information gain} with the
target point as well as minimizing the expected mean squared error
(see \cref{app:mutual_info}).
The mutual information (in a slightly different context) is
also used by \cite{krause2008nearoptimal} for sensor placement.

In contrast to using Euclidean distance \cite{vecchia1988estimation}
or the unconditional kernel function \cite{wada2013gaussian,
kang2021correlationbased}, conditional variance incentives picking points near
the target point, but also away from previously selected points.
This spreading effect is illustrated in \cref{fig:selection},
where each method is given a budget of two points.
Euclidean distance prefers the two points right of the target point.
However, a more balanced view of the situation is obtained when picking the
further away but ultimately more informative point on the left, reducing
variance at the target point and thereby reducing predictive error.

\subsection{A greedy approach}
\label{subsec:greedy_select}

Maximizing the conditional variance over all possible \( \binom{N}{s} \)
subsets is intractable, so we greedily select the next point which most
reduces the conditional variance, conditional on previously selected points.
Let \( \I = \{ i_j \}^s_{j = 1} \subseteq \Train \) be the
set of indices of previously selected training points.
For a candidate index \( k \), we condition the current
covariance matrix on \( y_k \) according to \cref{eq:cond_cov}:
\begin{align}
  \CM_{:, : \mid \I, k} &= \CM_{:, : \mid \I} -
    \CM_{:, k \mid \I} \CM_{k, k \mid \I}^{-1} \CM_{k, : \mid \I} \\
  %                    &= \CM_{:, : \mid \I} -
  % \frac{\CM_{:, k \mid \I} \CM_{:, k \mid \I}^{\top}}{\CM_{k, k \mid \I}} \\
  \label{eq:cond_select}
                       &= \CM_{:, : \mid \I} - \vec{u} \vec{u}^{\top} \\
  \label{eq:cond_cov_vec}
  \vec{u} &= \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k, k \mid \I}}}
\end{align}
From \cref{eq:cond_select}, adding a new index \( k \)
is a rank-one downdate on the current covariance matrix.
The decrease in the variance of \( y_\Pred \) after
selecting \( k \) is given by \( u_\Pred^2 \), or
\begin{align}
  \label{eq:obj_gp}
  u_\Pred^2 = \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
  = \frac{\Cov[y_\Pred, \vec{y}_{\Train}[k] \mid \I]^2}
         {\Var[\vec{y}_{\Train}[k] \mid \I]}
  = \Var[y_\Pred \mid \I]
    \Corr[y_\Pred, \vec{y}_\Train[k] \mid \I]^2
\end{align}
After adding the index \( k \), we need to keep track of each
candidate \( j \)'s updated variance and covariance with the
prediction point to compute the objective \cref{eq:obj_gp}.
We start with the unconditional values given by \( \CM_{j, j} \) and
\( \CM_{\Pred, j} \) and update after selecting each index \( k \).
We directly compute \( \vec{u} \) for \( k \) according to \cref{eq:cond_cov}
and update \( j \)'s conditional variance by subtracting \( u_j^2 \)
and update its conditional covariance by subtracting \( u_j u_\Pred \).

We have two efficient strategies to compute \( \vec{u} \).
The direct method is to keep track of \( \CM_{\I, \I}^{-1}
\), or the precision of the selected entries, and update
the precision every time a new index is added to \( \I \).
This can be done efficiently in computational time
complexity \( \BigO(s^2) \), see \cref{app:prec_insert}.
Once \( \CM_{\I, \I}^{-1} \) has been computed, \( \vec{u}
\) is computed directly according to \cref{eq:cond_cov}.
For each of the \( s \) rounds of selection, it takes \( \BigO(s^2)
\) to update the precision and \( \BigO(Ns) \) to compute \( \vec{u}
\), costing \( \BigO(N s^2 + s^3) \equiv \BigO(N s^2) \) overall.

The second strategy is to take advantage of the quotient
rule of Schur complementation \cref{eq:quotient_rule}.
From a statistical perspective, the quotient rule states
that conditioning on \( \I \) and then conditioning on \(
\J \) is the same as conditioning on \( \I \cup \J \).
We then remind ourselves that Cholesky factorization
can be viewed as iterative conditioning.
Re-writing the joint covariance matrix by
two steps of block Gaussisan elimination,
\begin{align}
  \label{eq:chol_schur}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{so we see that the Cholesky factorization of the joint
    covariance \( \CM \) is}
  \label{eq:chol}
  \chol(\CM) &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \chol(\CM_{1, 1}) & 0 \\
    0 & \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix} \\
  \nonumber
  &=
  \begin{pmatrix}
    \textcolor{darkorange}{\chol(\CM_{1, 1})} & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \chol(\CM_{1, 1})^{-\top}} &
    \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix}
\end{align}
Here the conditional expectation in \cref{eq:cond_mean} corresponds to
\( \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} \)
and the conditional covariance in \cref{eq:cond_cov} corresponds to
\(
\textcolor{lightblue}{
  \CM_{2, 2 \mid 1} = \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
} \).
Thus, we see that Cholesky factorization is
iteratively conditioning the Gaussian process.
From the iterative conditioning perspective, the \( k \)th column of the
Cholesky factor corresponds precisely to the corresponding \( \vec{u} \) for
\( k \) in \cref{eq:cond_cov_vec} since a iterative sequence of conditioning
on \( i_1, i_2 \dotsc, i_{k - 1} \) is equivalent to conditioning on \( \I
\) by the quotient rule \cref{eq:quotient_rule}.

Adding a column to the current Cholesky factor can be efficiently
computed without excess dependence on \( N \) with left-looking (see
\cref{alg:chol_update}), so the conditioning only happens when we need it.
For each of the \( s \) rounds of selection, it costs \( \BigO(N s) \)
to compute the next column of the Cholesky factorization, for a total
time complexity of \( \BigO(N s^2) \), matching the time complexity of
the explicit precision approach.

However, the precision algorithm uses \( \BigO(s^2) \) space to store the
precision \( \Theta_{\I, \I}^{-1} \) while the Cholesky algorithm uses \(
\BigO(N s) \) space to store the first \( s \) columns of the Cholesky factor
of \( \CM \), always more memory than the precision (\( N > s \)).
Both algorithms use an additional \( \BigO(N) \) space
to store the conditional variances and covariances.
Although the precision algorithm uses slightly less memory
than the Cholesky algorithm, the Cholesky algorithm is
preferred for better performance and ease of implementation.

\Stodo{this is a bit useless}
Once the indices have been computed by \cref{alg:select_prec} or
\cref{alg:select_chol}, inferring the conditional mean and covariance
of unknown data can be done directly according to \cref{eq:cond_mean}
and \cref{eq:cond_cov} in time \( \BigO(s^3) \) using \cref{alg:infer_select}.

\subsection{Supernodes and blocked selection}
\label{subsec:mult_select}

We now consider efficiently dealing with multiple prediction points.
The first problem is generalizing the objective for
a single point \cref{eq:obj_gp} to multiple points.
Following the KL minimization reasoning in \cref{subsubsec:aggregated},
the criterion should be to minimize the log determinant of the prediction
points' covariance matrix after conditioning on the selected points, \(
\logdet(\CM_{\Pred, \Pred \mid I}) \).
This objective, known as D-optimal design in the literature
\cite{krause2008nearoptimal}, has many intuitive interpretations, for
example, as a volume of the region of uncertainty or as the scaling factor
in the probability density function of multivariate Gaussians.
In addition, it is equivalent to maximizing mutual information
since the differential entropy of a Gaussian is monotonically
increasing with its log determinant (see \cref{app:mutual_info}).

We want to quickly compute how selecting an
index \( k \) affects the log determinant.
From \cref{eq:cond_select}, selecting an index
is a rank-one downdate on the covariance matrix:
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{By application of the matrix determinant lemma
    (the details are in \cref{app:logdet_downdate}),}
  \label{eq:greedy_mult}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}
Equation \cref{eq:greedy_mult} shows that to compute
the updated log determinant, it suffices to only
compute conditional variances of the candidate point.
Intuitively this corresponds to \emph{backwards}
regression, where we imagine measuring the values of the
\emph{prediction points} instead of at the \emph{candidates}.
We then infer the posterior variance at a candidate, and pick the candidate
whose conditional variance decreases the most (relative to its starting value).
These candidates are likely to give information about the prediction
points, because the prediction points give information about the candidate.

\Stodo{image for ``backwards'' sensor placement}

Re-writing the objective in this way motivates
an efficient algorithm to compute the objective.
We condition on a newly added point essentially the same as in
\cref{subsec:greedy_select}, but now maintaining two data structures
instead of one: one for the variance after conditioning on the
previously selected points, and the other for the variance after
also conditioning on the prediction points.
By the quotient rule \cref{eq:quotient_rule}, the order of
conditioning does not matter as long as the order is consistent.
For the second data structure, we therefore condition on the
prediction points \emph{first} before any points have been selected.
We again have two strategies, one which explicitly maintains precisions
and the other which relies on maintaining partial Cholesky factors.

For the precision algorithm, using \cref{eq:cond_cov} directly, for \( m
\) prediction points it costs \( \BigO(m^3) \) to compute \( \CM_{\Pred,
\Pred}^{-1} \) and then \( \BigO(N m^2) \) to compute the initial conditional
variances \( \CM_{k, k \mid \Pred} \) for the \( N \) candidates.
For each of the \( s \) rounds of selecting candidates, it costs \(
\BigO(s^2) \) and \( \BigO(m^2) \) to update the precisions \( \CM_{\I,
\I}^{-1} \) and \( \CM_{\Pred, \Pred}^{-1} \) respectively, where the details
of efficiently updating \( \CM_{\Pred, \Pred}^{-1} \) after the rank-one
update in \cref{eq:obj_gp_mult} are given in \cref{app:prec_cond}.
Given the precisions, \( \vec{u} = \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k,
k \mid \I}}} \) and \( \vec{u}_\Pred = \frac{\CM_{:, k \mid \I,
\Pred}}{\sqrt{\CM_{k, k \mid \I, \Pred}}} \) are computed as usual according
to \cref{eq:cond_cov} in time \( \BigO(Ns) \) and \( \BigO(Nm) \).
Finally, for each candidate \( j \), the conditional variance \( \CM_{j, j
\mid \I} \) is updated by subtracting \( u_j^2 \), the conditional covariance
\( \CM_{\Pred, k \mid \I} \) is updated for each index \( c \) of a prediction
point by subtracting \( u_j u_c \), and the conditional variance \( \CM_{j, j
\mid I, \Pred} \) is updated by subtracting \( {u_\Pred}_j^2 \).
The total time complexity after simplification
is \( \BigO(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two partial Cholesky factors are stored.
We first compute the Cholesky factorization after selecting each prediction
point, for a cost of \( \BigO((N + m) m) \) for each of the \( m \) columns.
We then begin selecting candidates, which requires updating
both Cholesky factors in time \( \BigO((N + m)(m + s)) \)
dominated by updating the preconditioned Cholesky factor.
The columns of the Cholesky factors correspond precisely to \( \vec{u}
\) and \( \vec{u}_\Pred \) and both conditional variances \( \CM_{j, j
\mid I} \) and \( \CM_{j, j \mid I, \Pred} \) can be computed as above.
The conditional covariances do not need to be computed.
Over \( s \) rounds the total time complexity is \( \BigO((N + m) m^2 +
s(N + m)(m + s)) \) which simplifies to \( \BigO(N s^2 + N m^2 + m^3) \).

Like the single-point case, both approaches have the
same time complexity but differ in space complexity.
The precision algorithm requires \( \BigO(s^2 + m^2) \)
memory to store both precisions, as well as \( \BigO(N
m) \) memory to store the conditional covariances.
The Cholesky algorithm requires \( \BigO((N + m)(m + s)) \) memory
to store the first \( m + s \) columns of the Cholesky factor for
the joint covariance matrix between training and prediction points,
which simplifies to \( \BigO(N s + N m + m^2) \).
The memory usages are identical except for \( \BigO(s^2)
\) versus \( \BigO(N s) \), so the Cholesky algorithm
again uses more memory than the precision algorithm.

\subsection{Partial selection}
\label{subsec:partial_select}

We can now generalize the multiple-point algorithm to track the log
determinant of the target points' covariance matrix if a selected
index \( k \) only conditions \emph{suffixes} of the target points,
skipping the first \( p \) targets, say.

The form of the Cholesky factor of the target covariance
matrix \cref{eq:chol_partial} suggests to keep track of
the \emph{insertion position} of the new index \( k \).
To be precise, we maintain a partial Cholesky factor \( L \) of the joint
covariance matrix between the target points and all training points, whose
columns consist of the target points and previously selected points sorted
in reverse order with respect to \( \prec \).
Adding a new entry adds a column to \( L \).
Inserting the index \( k \) into its proper
spot in the Cholesky factor \( L \),
\begin{align}
  \label{eq:chol_partial_update}
  L &\gets
  \begin{pmatrix}
    L_{:, :j} & \vec{u} & {L'}_{:, j + 1:}
  \end{pmatrix} \\
  \vec{u} &= \frac{\CM_{:, k \mid :j}}{\sqrt{\CM_{k, k \mid :j}}}
  \shortintertext{where unlike \cref{eq:chol_partial}, we are using a
    lower-triangular factor \( L^{\top} L = \CM \) for simplicity: if}
  \label{eq:U_chol}
  U &= P^{\Reverse} \chol(P^{\Reverse} \CM P^{\Reverse}) P^{\Reverse}
\end{align}
where \( P^{\Reverse} \) is the order-reversing permutation, then \( U \)
is an upper-triangular factor satisfying \( U U^{\top} = \CM \) so \( L =
U^{\top} \) is a lower-triangular factor satisfying \( L^{\top} L = \CM \).

To account for this insertion, the first \( j \) columns \( L_{:, :j} \)
are unchanged, the newly inserted column \( \vec{u} \) as a variance-scaled
conditional covariance of the form \cref{eq:cond_cov_vec} can be computed
as usual with left-looking (see \cref{alg:chol_update}) and the remaining
columns \( {L'}_{:, j + 1:} \) are the Cholesky factor of the covariance
matrix conditional on the point \( k \).

\Stodo{reference psuedocode}
From \cref{eq:cond_select} conditioning on an additional point is a rank-one
downdate of the covariance matrix by the vector \( \vec{u} \), allowing the
updated factor \( {L'}_{:, j + 1:} \) to be efficiently computed from the
original factor \( L_{:, j + 1:} \).
Specifically we use Lemma 1 of \cite{krause2015more}, adapted to make
no assumption on the row ordering of the Cholesky factor, allowing the
downdate to be implemented with in-place BLAS \texttt{daxpy} operations.
For a Cholesky factor with \( R \) rows and \( C \) columns, the
time complexity of the downdate is \( \BigO(R C) \) compared to
\( \BigO(R C^2) \) if the factor was recomputed from scratch.

In the context of the overall update for \( N \) training points,
\( m \) target points, and a budget of \( s \) selected points, the
Cholesky factor \( L \) is of size \( (N + m) \times (s + m) \).
When adding a new index \( k \) that ignores the first \( j \) targets, the
first \( j \) columns of \( L \) are ignored, the new column \( \vec{u} \)
must be computed with left-looking in time \( \BigO((N + m) j) \), and the
columns past \( j \) are computed with downdating in time \( \BigO((N + m)
(s + m - j)) \).
The cost for a single insertion is \( \BigO((N + m)(s + m)) \) for a total time
complexity of \( \BigO(s(N + m)(s + m)) \) over \( s \) rounds of selection.
Given the factor \( L \), we now discuss actually computing the log
determinant objective \cref{eq:partial_logdet} after adding a candidate.

We have a candidate index \( j \) and wish to compute
its effect on the log determinant of the target points
(or KL divergence) \cref{eq:partial_kl} if added.
To do so, we will directly compute each individual target point's
variance, conditional on \( j \) as well as previously selected
points, and simply add these variances to get the overall objective.

Recall from \cref{eq:chol} that the \( j \)th row of the \( i \)th column
of a Cholesky factor \( L \) is \( L_{j, i} = \frac{\CM_{j, i \mid :i -
1}}{\sqrt{\CM_{i, i \mid :i - 1}}} \), i.e. the covariance between the \( j
\)th point and the \( i \)th point, conditional on all points prior to \(
i \) in the \emph{columns} of \( L \) (not with respect to \( \prec \)).
A special case is that the ``diagonal'' entry \( L_{i, i} = \sqrt{\CM_{i, i
\mid :i - 1}} \) is the square root of the conditional variance of the \( i
\)th point (the rows of \( L \) are not necessarily ordered with respect to \(
\prec \), so \( L \) is not necessarily literally lower triangular in memory).

If we have the variance of the \( j \)th point conditional on all points
prior to the \( i \)th column, \( \CM_{j, j \mid :i - 1} \), then by reading
off the conditional covariance \( L_{j, i} \) we can use conditioning
\cref{eq:quotient_rule} to compute the desired effect on the \( i \)th point
\( \CM_{i, i \mid :i - 1, j} \), and by using the conditional variance \(
L_{i, i} \) we can condition the \( j \)th point \( \CM_{j, j \mid :i - 1,
i} = \CM_{j, j \mid :i} \) to bring it to the next column.
These series of updates imply an inductive algorithm starting with the
first column, where \( \CM_{j, j \mid :i - 1} \) is just \( \CM_{j,
j} = K(\vec{x}_j, \vec{x}_j) \), the variance of the \( j \)th point.
For the \( i \)th column, the updates are as follows:
\begin{align}
  \CM_{i, i \mid :i - 1} &= L_{i, i}^2 \\
  \CM_{j, i \mid :i - 1} &= L_{j, i} \cdot L_{i, i} \\
  % \CM_{i, i \mid :i - 1, j} &= \CM_{i, i \mid :i - 1} -
  %   \frac{\CM_{j, j \mid :i - 1}^2}{\CM_{j, j \mid :i - 1}} \\
  \label{eq:obj_partial}
  \CM_{i, i \mid :i - 1, j} &= \CM_{i, i \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{j, j \mid :i - 1} \\
  % \CM_{j, j \mid :i - 1, i} &= \CM_{j, j \mid :i - 1} -
  %   \frac{\CM_{j, i \mid :i - 1}^2}{\CM_{i, i \mid :i - 1}} \\
  \label{eq:partial_induct}
  \CM_{j, j \mid :i - 1, i} &= \CM_{j, j \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{i, i \mid :i - 1} \\
                            &= \CM_{j, j \mid :i}
\end{align}
where \cref{eq:partial_induct} satisfies the conditions of the inductive
hypothesis for the next column and \cref{eq:obj_partial} is the desired
conditional variance when the \( i \)th point is a target (recall the
columns of \( L \) include previously selected training points, whose
variances are ignored in the objective but columns still processed to
compute \cref{eq:partial_induct}).

For each of the \( N \) candidates, it requires \( \BigO(1) \) work per column
for \( s + m \) columns. Over \( s \) selections, the total time complexity
is \( \BigO(s N (s + m)) \) which is dominated by the time to downdate the
Cholesky factor, meaning the partial selection algorithm matches the asymptotic
time complexity of the multiple-point algorithm.
However, the downdate of the Cholesky factor is implemented with
repeated BLAS level-one \texttt{daxpy} operations while the bulk of
left-looking takes place in a BLAS level-two \texttt{dgemv} operation.
Higher level operations often have better constant-factor
performance for the same asymptotic time complexity.
In addition, the objective for the partial selection algorithm is
more complicated to compute than its multiple-point counterpart.

\subsection{Near optimality by empirical submodularity}

If the objective satisfies the property of submodularity, then it is guaranteed
that the greedy algorithm produces an objective within a a \( 1 - \frac{1}{e}
\) of the optimal objective. Unfortunately the information gain objective is
not submodular in general, see \cref{app:submodular}. However, it is possible
to empirically observe for a particular point set and choice of kernel function
whether it is submodular. We note that one-dimensional points with a Mat{\'e}rn
kernel function seems to be submodular, but not for any higher dimension.

\cite{das2011submodular, jagalur-mohan2021batch}

\section{Greedy selection for \emph{global} approximation by KL-minimization}
\label{sec:chol_select}

\begin{figure}[t]
  \centering
  \input{figures/cholesky_factor.tex}%
  % TODO: matching GP image with 13 candidates, 3 selected, 1 training point
  \input{figures/selection/cknn.tex}
  \caption{For a column in isolation, the \textcolor{orange}{unknown} point
    is the diagonal entry, below it are \textcolor{lightblue}{candidates},
    and the \textcolor{seagreen}{selected} entries are added to the sparsity
    pattern \( s_i \). Thus, sparsity selection in Cholesky factorization
    is analogous to point selection in Gaussian processes.}
  \label{fig:select_chol}
\end{figure}

We can therefore use algorithm \cref{alg:select_chol} directly on each
column, where the prediction point is the \( i \) variable and the number
of points selected is the number of nonzero entries per column. The only
difference is that the candidates is limited to indices lower than \( i
\), that is, candidate indices \( k \) such that \( k > i \) to maintain
the lower triangularity of \( L \). Once \( s_i \) has been computed for
each \( i \), \( L \) can be constructed according to \cref{eq:L_col}. Each
column costs \( \BigO(s^3) \) to compute \( \CM_{s_i, s_i}^{-1} \) for a
total cost of \( \BigO(N s^3) \) for the \( N \) columns of \( L \).

\subsection{Aggregated sparsity pattern}

We can also use the Gaussian process regression viewpoint to efficiently
aggregate multiple columns, that is, to use the same sparsity pattern for
multiple columns. We denote aggregating the column indices \( i_1, \dotsc, i_m
\) into the same group as \( \tilde{i} = \{i_1, i_2, \dots i_m \} \), letting
\( s_{\tilde{i}} = \bigcup_{i \in \tilde{i}} s_i \) be the aggregated sparsity
pattern, and letting \( \tilde{s} = s_{\tilde{i}} - \tilde{i} \) be the set
of selected entries excluding the diagonal entries. Each \( s_i = \tilde{s}
\cup \, \{ j \in \tilde{i} \mid j \geq i \} \), that is, the sparsity pattern
of the \( i \) column is the selected entries plus all the diagonal entries
lower than it. We will enforce that all the selected entries, excluding the
indices of the diagonals of the columns themselves, are below the lowest index
so that indices are not selected ``partially'' --- that is, an index could be
above some indices in the aggregated columns, and therefore invalid to add to
their column, but below others. That is, we restrict the candidate indices \(
k > \max \tilde{i} \) so that the selected index can be added to each column
in \( \tilde{i} \) without violating the lower triangularity of \( L \). It
is in fact possible to properly account for these partial updates, but the
reasoning and eventual algorithm becomes more complicated. We defer a detailed
discussion of the partial update case to \cref{app:partial}.

We now show that the KL-minimization objective on the aggregated indices
corresponds precisely to \cref{eq:obj_gp_mult}, the objective multiple
point Gaussian regression with the chain rule of log determinant through
conditioning.
\begin{align}
  \label{eq:det_chain}
  \logdet(\CM) &= \logdet(\CM_{1, 1 \mid 2}) + \logdet(\CM_{2, 2})
  \shortintertext{where \( \CM \) is blocked according to
    TODO. The KL divergence objective for \( \tilde{i} \) is:}
  \nonumber
  \sum_{i \in \tilde{i}} \log(\CM_{ii \mid s_i - \{ i \} })
  &= \log(\CM_{i_m i_m \mid \tilde{s}}) +
     \log(\CM_{i_{m - 1} i_{m - 1} \mid \tilde{s} \cup \{ i_m \}})
     + \dotsb \\
  \nonumber
  &= \logdet(\CM_{\{ i_m, i_{m - 1} \}, \{ i_m, i_{m - 1} \}
             \mid \tilde{s}}) +
    \log(\CM_{i_{m - 2} i_{m - 2} \mid \tilde{s} \cup \{ i_m, i_{m - 1} \}})
    + \dotsb \\
  \label{eq:obj_mult}
  &= \logdet(\CM_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}

We see that the objective \cref{eq:obj_mult} is equivalent to the
objective \cref{eq:obj_gp_mult}, that is, to minimize the log
determinant of the conditional covariance matrix corresponding to a
set of prediction points, conditional on the selected entries. We can
therefore directly use \cref{alg:select_mult_chol} on the aggregated
columns, where the prediction points correspond to indices in the
aggregation and where we restrict the candidates \( k \) to those
below each column in the aggregation, \( k > \max \tilde{i} \).

Hence the sparse Cholesky factorization motivated by KL divergence can be
viewed as sparse Gaussian process selection over each column, where entries are
selected to maximize mutual information with the entry on the diagonal of the
current column. In the aggregated case, the multiple columns in the aggregated
group correspond directly to predicting for multiple prediction points, where
entries are again selected to maximize mutual information with each diagonal
entry in the aggregation. This viewpoint leads directly to \cref{alg:chol}.

\begin{algorithm}
  \caption{Cholesky factorization by selection}
  \label{alg:chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s,
      g = \{ \tilde{i}_1, \dotsc, \tilde{i}_{N/m} \} \)
    \ENSURE \( L \) such that
      \( (L L^{\top})^{-1} \approx K(\vec{x}, \vec{x}) \)

    \STATE \( n \gets \lvert \vec{x} \rvert \)
    \FOR{ \( \tilde{i} \in g \)}
      \STATE \( J \gets
        \{ \max(\tilde{i}) + 1, \max(\tilde{i}) + 2, \dotsc, n \}
      \)
      \STATE Compute \( I \) using
        \cref{alg:select_mult_prec} or \cref{alg:select_mult_chol} \\ where
        \( \vec{x}_\Train = \vec{x}[J],
           \vec{x}_\Pred = \vec{x}[\tilde{i}],
           s = s - \lvert \tilde{i} \rvert
        \)
      \STATE \( \tilde{s} \gets J[I] \)
      \FOR{\( i \in \text{reversed}(\text{sorted}(\tilde{i})) \)}
        \STATE \( \tilde{s} \gets \tilde{s} \cup \{ i \} \)
        \STATE \( s_i \gets \text{reversed}(\tilde{s}) \)
      \ENDFOR
    \ENDFOR
    \RETURN \( L \) computed with \cref{alg:L_mult}
  \end{algorithmic}
\end{algorithm}

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ without aggregation}
  \label{alg:L}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s_i \)
    \ENSURE \( L_{s_i, i} \)

    \STATE \( \CM_{s_i, s_i}^{-1} \gets
      K(\vec{x}[s_i], \vec{x}[s_i])^{-1}
    \)
    \STATE \( L_{s_i, i} \gets \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
      {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \)
    \RETURN \( L_{s_i, i} \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ with aggregation}
  \label{alg:L_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), \tilde{s}, \tilde{i} \)
    \ENSURE \( L_{s_i, i} \) for all \( i \in \tilde{i} \)

    \STATE \( s \gets \tilde{i} \cup \tilde{s} \)
    \STATE \( U \gets P^{\updownarrow}
      \chol(P^{\updownarrow} \CM_{s, s} P^{\updownarrow}) P^{\updownarrow}
    \)
    \FOR{\( i \in \tilde{i} \)}
      \STATE \( k \gets \) index of \( i \) in \( \tilde{i} \)
      \STATE \( L_{s_i, i} \gets U^{-\top} \vec{e}_k \)
    \ENDFOR
    \RETURN L
  \end{algorithmic}
\end{algorithm}
\end{minipage}

Once the sparsity pattern has been determined, we need to compute each column
of \( L \) according to \cref{thm:L}. Because the sparsity pattern for each
column in the same group are subsets of each other, we can efficiently compute
all their columns at once. The observation is that the smallest index in the
group (corresponding to the entry highest in the matrix) will have the largest
sparsity pattern, the next index will have one less entry (lacking the entry
above it, which would violate lower triangularity), and so on. We need to
compute \( \CM_{s_i, s_i}^{-1} \vec{e}_1 \) for each \( i \in \tilde{i} \),
or the precision of the marginalized covariance corresponding to the selected
entries. By \cref{eq:inverse_cond}, we can turn marginalization in covariance
into conditioning in precision:
\begin{align}
  \nonumber
  \label{eq:L_precision}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}} \\
             &= \frac{(\CM_{s, s})^{-1}_{k:, k: \mid :k - 1} \vec{e}_1}
             {\sqrt{\vec{e}_1^{\top} (\CM_{s, s})^{-1}_{k:, k: \mid :k - 1}
                    \vec{e}_1}}
\end{align}
where \( s = \tilde{i} \cup \tilde{s} \) and \( k \) is \( i \)'s
index in \( \tilde{i} \). So we want the \( k \)th column of
the precision of the marginalized covariance, conditional on all the
entries before it. From \cref{eq:chol}, this can be directly read
off the Cholesky factorization. Thus, we can simply compute:
\begin{align}
  \label{eq:L_chol}
  L &= \chol \left ( \CM_{s, s}^{-1} \right )
\end{align}
and read off the \( k \)th column to compute \cref{eq:L_precision} for each
\( i \in \tilde{i} \). However, instead of computing a lower triangular
factor for the precision, we can compute an \emph{upper} triangular factor
the covariance whose inverse transpose will be a \emph{lower} triangular
factor for the original matrix. In particular, we see that
\begin{align}
  % \label{eq:U_chol}
  U &= P^{\updownarrow} \chol(P^{\updownarrow} \CM_{s, s} P^{\updownarrow})
       P^{\updownarrow}
  \shortintertext{satisfies \( U U^{\top} = \CM_{s, s} \) where
  \( P^{\updownarrow} \) is the order-reversing permutation. Thus,}
  \nonumber
  \CM_{s, s}^{-1} &= U^{-\top} U^{-1}
\end{align}
where \( U^{-\top} \) is an \emph{lower} triangular factor for \( \CM_{s,
s}^{-1} \) equal to \cref{eq:L_col} because the Cholesky factorization is
unique. Computing \( U^{-\top} \) leads directly to \cref{alg:L_mult}.

Recall that the complexity of selecting \( s \) out of \( N \) total training
points for \( m \) prediction points using \cref{alg:select_mult_prec} or
\cref{alg:select_mult_chol} was \( \BigO(N s^2 + N m^2 + m^3) \). In the
context of Cholesky factorization, \( N \) is the size of the matrix, \( m \)
is the number of columns to aggregate, and \( s \) is the number of nonzero
entries in each column of \( L \). We therefore need to do \( \frac{N}{m} \)
selections, one for each aggregated group, where we only need to select \( s
- m \) entries (since the \( m \) prediction points are automatically added).
We then need to actually construct each column of \( L \) after determining
the sparsity pattern, with \cref{alg:L_mult}. This costs \( \BigO(s^3) \)
for each aggregated group to compute the Cholesky factor of the submatrix,
which dominates the time to compute each column of \( L \) for the \( m \)
columns in the group, \( \BigO(m s^2) \) (\( N > s > m \)). Thus, the overall
complexity is \( \BigO(\frac{N}{m} (N (s - m)^2 + N m^2 + m^3 + s^3)) \),
which simplifies to \( \BigO(\frac{N^2 s^2}{m}) \) by making use of the bound
that \( (s - m)^2 = \BigO(s^2 + m^2) \).

Note that the non-aggregated factorization is equivalent to \( m = 1 \),
which yields \( \BigO(N^2 s^2) \) (using the non-aggregated algorithms
\cref{alg:select_chol,alg:L}, but one can also use the aggregated versions
\cref{alg:select_mult_chol,alg:L_mult} with \( m = 1 \) and achieve
equivalent complexity). Thus, we see that the aggregated version is \( m
\) times faster than its non-aggregated counterpart, at the cost that the
resulting sparsity pattern will be lower quality (since the algorithm is
forced to select the same entry for \emph{all} columns in the group).

Unlike the geometric algorithms of \cite{schafer2021sparse,
schafer2020compression} which rely on the pairwise distance between points,
and whose covariance matrix is implicitly determined by a list of points and
kernel function, this algorithm relies only on the entries of the covariance
matrix \( \CM \). Thus, it can factor arbitrary symmetric positive-definite
matrices without access to points or an explicit kernel function.

\section{Numerical experiments}

All experiments were run on the Partnership for an Advanced Computing
Environment (PACE) Phoenix cluster at the Georgia Institute of
Technology, with 8 cores of a Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz
and 22 GB of RAM per core. Python code for all numerical experiments
can be found at \href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\Stodo{cite python libraries that want citations (e.g. scikit-learn)}

\subsection{\textit{k}th-nearest neighbors selection}

We justify that diverse point selection based on conditional information can
lead to better performance than simply selecting the nearest neighbor in a toy
example on the MNIST dataset. We compare \( k \)th-nearest neighbors (KNN)
directly to conditional \( k \)th-nearest neighbors (CKNN) in the following
experiment. We randomly select 1000 images to form the training set and 100
to form the testing set. For each image in the testing set, we select the
\( k \) ``closest'' training points with either KNN or CKNN. For KNN we use
the standard Euclidean distance and for CKNN we use Mat{\'e}rn kernel with
smoothness \( \nu = 1.5 \) and length scale \( l = 2^{10} \). Finally, we
predict the label of the test point by taking the most frequently occurring
label in the \( k \) selected points.

\Stodo{cite mnist dataset}

\begin{figure}[h]
  \centering
  \input{figures/mnist/accuracy_k.tex}
  \caption{Accuracy with increasing \( k \)}
\end{figure}

As \( k \) increases, KNN degrades near-linearly in accuracy. We hypothesize
that nearby images are more likely to have the same label as a given test
image. By forcing the algorithm to select more points, it increases the
likelihood that the algorithm becomes confused by differently labeled
images. However, CKNN is more accurate than KNN for nearly every \( k
\), suggesting that conditional selection is able to take advantage of
selecting more points. We emphasize that the difference in accuracy is
solely a result of conditional selection --- because the Mat{\'e}rn kernel
degrades monotonically with distance, sorting by covariance is identical
to sorting by distance. In addition, we use the mode to aggregate the
labels of the selected points, rather than performing Gaussian process
classification. The difference in accuracy can therefore be attributed
to precisely the difference in which points were selected.

\subsection{Recovery of sparse Cholesky factors}

As noted in \cref{app:omp}, the selection algorithm can be viewed as orthogonal
matching pursuit \cite{tropp2007signal} in feature space. We experiment
with the sparse recovery properties of the selection algorithm by randomly
generating a sparse Cholesky factor \( L \). We prescribe a fixed number
of nonzeros per column \( s \) over \( N \) columns. For each column, we
uniformly randomly pick \( s \) entry that satisfies lower triangularity to
make nonzero. We randomly generate values according to i.i.d. standard normal
\( \mathcal{N}(0, 1) \). Finally, we fill the diagonal with a ``large``
positive value (10) to almost guarantee that the resulting matrix \( \CM =
L L^{\top} \) is positive-definite. The selection algorithms are then given
\( \CM \) and \( s \) and are asked to reconstruct \( L \). The strategies
are as follows: ``cknn'' uses conditional selection on each column to minimize
the conditional variance of the diagonal entry, ``knn'' selects entries with
the largest covariance with the diagonal entry, ``corr'' selects entries with
the highest correlation objective \cref{eq:obj_gp} without accounting for
conditioning, and ``random'' simply randomly samples entries uniformly. The
strategies are given either the covariance \( \CM \) or the precision \(
\CM^{-1} \) depending on which results in higher accuracy, in particular,
the ``cknn'' strategy is given the precision while the rest of the methods
are given the covariance. Accuracy is measured by taking the cardinality of
the intersection of the recovered sparsity set with the ground truth sparsity
set over the cardinality of their union, intersection over union (IOU).

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_n.tex}
  \caption{Accuracy with increasing \( N \)}
\end{figure}

As the number of nonzero entries per column is fixed and the number of rows
and columns is increased, the ```cknn'' retains high accuracy near perfect
recovery, and the rest of the methods quickly degrade and asymptote to their
final accuracies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_s.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

If the number of rows and columns is fixed while the number of nonzero entries
per column is increased, all methods drop in accuracy with increasing density
into a tipping point where the problem starts to become easier. Accuracy then
increases until the Cholesky factor becomes fully dense, in which case perfect
recovery is trivial. The ``cknn'' strategy exhibits the same behavior, but
maintains much higher accuracy than the rest of the strategies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_noise.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

Finally, we experiment with the addition of noise. Noise sampled i.i.d
from \( \mathcal{N}(0, \sigma^2) \) is added to each entry of \( \CM
\) symmetrically (i.e. \( \CM_{ij} \) receives the same noise as \(
\CM_{ji} \)) to preserve the symmetry of \( \CM \). As expected,
accuracy degrades with increasing noise, but the algorithm is fairly robust
to low levels of noise. At higher levels of noise, \( \CM \) can lose
positive-definiteness, which causes the algorithm to break down.

\Stodo{laplacians, problems with
positive-definiteness and dense cholesky factors}

\cite{kyng2016approximate}

\subsection{Cholesky factorization}

\subsection{Gaussian process regression}

\subsection{Preconditioning for conjugate gradient}

\subsection{Comparison to other methods}

against greedy gp information theoretic: most are trying to
get some approximation for the entire process, \emph{directed}
greedy selection (towards a single point or group of points)
(never mind, smola \cite{smola2000sparse} is directed)

engineering for multiple points / nonadjacent case (conditioning structure)

omp: feature space versus covariance

previous Cholesky papers: conditional selection versus geometry

\section{Conclusions}

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{cholesky}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Derivations in KL-minimization}

\subsection{KL divergence of two multivariate Gaussians}
\label{app:kl_obj}

\Stodo{this is useless, remove}
We want to show that the KL divergence between \(
\N(\vec{0}, \CM_1) \) and \( \N(\vec{0}, \CM_2) \) for \(
\CM_1, \CM_2 \in \Reals^{N \times N} \) can be written
\begin{align}
  \nonumber
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
  \shortintertext{Recall that the log density \( \log
    \p(\vec{x}) \) for \( \vec{x} \sim \N(\vec{0}, \CM) \) is}
  \log \p(\vec{x}) &= -\frac{1}{2} (N \log(2 \pi) + \logdet(\CM) +
    \vec{x}^{\top} \CM^{-1} \vec{x})
  \shortintertext{If \( P \) and \( Q \) are the respective densities for
    \( \CM_1 \) and \( \CM_2 \), then by definition}
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &= 2 \E_P[\log P - \log Q]
  \shortintertext{where \( \E_P \) denotes expectation under \( P \).}
  &= 2 \E_P[-\frac{1}{2} (N \log(2 \pi) + \logdet(\CM_1) +
            \vec{x}^{\top} \CM_1^{-1} \vec{x}) \\
  \nonumber
  & \hphantom{= 2 \E_P[}
            +\frac{1}{2} (N \log(2 \pi) + \logdet(\CM_2) +
            \vec{x}^{\top} \CM_2^{-1} \vec{x})] \\
  \label{eq:kl_after_logdet}
  &= \E_P[\vec{x}^{\top} \CM_2^{-1} \vec{x} -
          \vec{x}^{\top} \CM_1^{-1} \vec{x}]
          + \logdet(\CM_2) - \logdet(\CM_1) \\
  \E_P[\vec{x}^{\top} \CM_2^{-1} \vec{x} -
          \vec{x}^{\top} \CM_1^{-1} \vec{x}]
  &=
  \E_P[\trace(\vec{x}^{\top} \CM_2^{-1} \vec{x}) -
       \trace(\vec{x}^{\top} \CM_1^{-1} \vec{x})]
\end{align}
because the trace of a scalar is a scalar, and the linearity of trace.
\begin{align}
  &=
  \E_P[\trace(\CM_2^{-1} \vec{x} \vec{x}^{\top}) -
       \trace(\CM_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{cyclic property of trace} \\
  &=
  \E_P[\trace(\CM_2^{-1} \vec{x} \vec{x}^{\top} -
              \CM_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{linearity of trace} \\
  &= \E_P[\trace \left (
    (\CM_2^{-1} - \CM_1^{-1}) \vec{x} \vec{x}^{\top} \right )]
  && \text{factoring} \\
  &= \trace(\E_P \left [
    (\CM_2^{-1} - \CM_1^{-1}) \vec{x} \vec{x}^{\top} \right])
  && \text{swapping trace and expectation} \\
  &= \trace((\CM_2^{-1} - \CM_1^{-1})
    \E_P \left [ \vec{x} \vec{x}^{\top} \right])
  && \text{linearity of expectation} \\
  &= \trace((\CM_2^{-1} - \CM_1^{-1}) \CM_1)
  && \text{\( \CM_1 = \E_P[\vec{x} \vec{x}^{\top} \)]} \\
  &= \trace(\CM_2^{-1} \CM_1 - \Id_N)
  && \text{multiplying} \\
  &= \trace(\CM_2^{-1} \CM_1) - \trace(\Id_N)
  && \text{linearity of trace} \\
  &= \trace(\CM_2^{-1} \CM_1) - N
  \label{eq:kl_after_trace}
  && \text{trace of \( N \times N \) identity \( N \)}
\end{align}
Combining \cref{eq:kl_after_trace} with
\cref{eq:kl_after_logdet}, we obtain the desired result
\begin{align}
  \nonumber
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL divergence between \( \CM \) and the Cholesky
factor \( L \) computed according to \cref{eq:L_col}. From \cref{eq:kl},
\begin{align}
  \label{eq:kl_L}
  2 \KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} &=
    \trace(L L^{\top} \CM) - \logdet(L L^{\top}) - \logdet(\CM) - N
  \shortintertext{Ignoring terms not depending on \( L \),}
  &= \trace(L L^{\top} \CM) - \logdet(L L^{\top})
  \shortintertext{By the cyclic property of trace,}
  &= \trace(L \CM L^{\top}) - \logdet(L L^{\top})
  \shortintertext{Focusing on \( \trace(L \CM
    L^{\top}) \) and expanding on the columns of \( L \),}
  \trace(L \CM L^{\top}) &= \sum_{i = 1}^N
    L_{s_i, i}^{\top} \CM_{s_i, s_i} L_{s_i, i}
  \shortintertext{Plugging in \( L_{s_i, i} \) from \cref{eq:L_col},}
  &= \sum_{i = 1}^N
    \left (
      \frac{\left ( \CM_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \CM_{s_i, s_i}
    \left (
      \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right ) \\
  &= \sum_{i = 1}^N
    \frac{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
          \CM_{s_i, s_i} \CM_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1} \\
  &= \sum_{i = 1}^N 1 = N
\end{align}
\begin{align}
  \shortintertext{Using \( N \) for \( \trace(L
    L^{\top} \CM) \) in \cref{eq:kl_L},}
  2 \KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} &=
    -\logdet(L L^{\top}) - \logdet(\CM)
  \shortintertext{\( L^{\top} \) has the same log determinant
    as \( L \), and because \( L \) is lower triangular, its
    log determinant is the sum of its diagonal entries:}
  &= -2 \sum_{i = 1}^N \left [ \log(L_{i, i}) \right ] - \logdet(\CM)
  \shortintertext{Plugging \cref{eq:L_col} for the diagonal entries,}
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\CM)
  \shortintertext{Bringing the negative inside,}
  &= \sum_{i = 1}^N
    \left [
      \log
      \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
\end{align}
So minimizing the KL divergence (given optimal \( L \)) corresponds
to minimizing the sum of the inverse of the diagonal entries.
We can give an intuitive view of this result by making
use of \cref{eq:L_cond_var} and expanding the log
determinant by the chain rule \cref{eq:det_chain}.
\begin{align}
  \sum_{i = 1}^N
    \left [
      \log
      \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i - \{ i \}} \right )
    \right ]
    - \logdet(\CM) \\
  &= \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid s_i - \{ i \}} \right ) -
    \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid i + 1:} \right ) \\
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i - \{ i \}} \right ) -
      \log \left ( \CM_{i, i \mid i + 1:} \right )
    \right ]
\end{align}
This sum is the accumulated \emph{difference} in error for a series of
independent prediction problems: to predict the \( i \)th variable given
the variables after it in the ordering, \( i + 1, i + 2, \dotsc, N \).
The left term \( \log \left ( \CM_{i, i \mid s_i - \{ i \}} \right ) \) is
restricted to the variables in the sparsity pattern \( s_i \), while the right
term \( \log \left ( \CM_{i, i \mid i + 1:} \right ) \) is unconstrained.
As a result, the left term will necessarily be greater than the
right, and the goal is to minimize the accumulated deviation.
Thus, the KL divergence measures the maintenance of predictive accuracy after
adding conditional independence (sparsity) to the original distribution.
This independent regression problem interpretation of the KL
divergence is also given in \cite{katzfuss2022scalable}, equation 5.

The asymmetry of KL divergence implies the order of the matrices matters as
well as whether both matrices have been inverted or not. This seems to imply
that there are four possible ways to compare two covariance matrices. However,
note that
\begin{align}
  \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \KL*{\N(\vec{0}, \CM_2^{-1})}{\N(\vec{0}, \CM_1^{-1})}
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL divergence.
There are therefore only two possible ways to
compare the two matrices, depending on their order.
From a statistical perspective, the KL divergence can interpreted
as the likelihood-ratio test, so the asymmetry in order corresponds
to the asymmetry between the null and alternative hypotheses.

\subsection{Partial updates in the selection algorithm}
\label{app:partial}

We have the collection of Gaussian random variables \( \vec{y} = [y_1, \dotsc,
y_m]^{\top} \) corresponding to the ordered column indices \( i_1 \succ
\dotsb \succ i_m \) in the context of aggregated Cholesky factorization.
We then select an index \( k \) that conditions the variables, but ignores
the first \( p \) variables (the indices are sorted in decreasing order,
so if \( k \) conditions a variable, it conditions everything afterwards).
After this partial conditioning we denote the updated variables as
\( \vec{y}_{\parallel k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k},
\dotsc, y_{m \mid k} \) and wish to compute the covariance matrix \(
\Cov[\vec{y}_{\parallel k}] \).
We know the original variables \( \vec{y} \) have joint density multivariate
Gaussian according to some covariance matrix \( \CM \), \( \vec{y} \sim
\N(\vec{0}, \CM) \), and the fully conditional variables \( \vec{y}_{\mid k}
\) have closed-form posterior distribution according to \cref{eq:cond_cov},
\( \vec{y}_{\mid k} \sim \N(\vec{\mean}, \CM - \CM_{:, k} \CM_{k, k}^{-1}
\CM_{k, :}) \) for some posterior mean \( \vec{\mean} \).
For unconditioned \( y_i \) and \( y_j \),
their covariance is simply \( \CM_{i, j} \).
Similarly, for conditioned \( y_{i \mid k} \) and \( y_{j
\mid k} \), their covariance is \( \CM_{i, j \mid k} \).
The only unknown is the covariance between unconditioned
\( y_i \) and conditioned \( y_{j \mid k} \).
Taking the Cholesky factorization of both covariance matrices,
let \( L = \chol(\CM) \) and \( L' = \chol(\CM_{:, : \mid k}) \).
We can then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z}
\) is distributed according to standard normal, \( \N(\vec{0}, \Id) \).
Similarly, \( \vec{y}_{\mid k} = L' \vec{z} + \vec{\mean} \).
By definition,
\begin{align}
  \Cov[y_i, y_{j \mid k}] &=
    \E[(y_i - \E[y_i])(y_{j \mid k} - \E[y_{j \mid k}])] \\
  &= \E[(L_i \vec{z}) ({L'}_j \vec{z} + \mu_j - \mu_j)] \\
  &= \E[(L_{i, 1} z_1 + \dotsb + L_{i, m} z_m)
        ({L'}_{j, 1} z_1 + \dotsb + {L'}_{j, m} z_m)]
  \shortintertext{For \( i \neq j \), \( \E[z_i z_j] = \E[z_i] \E[z_j]
    = 0 \) since \( z_i \) is independent of \( z_j \) and has mean 0.}
  &= \E[L_{i, 1} {L'}_{j, 1} z_1^2 + \dotsb +
        L_{i, m} {L'}_{j, m} z_m^2] \\
  &= L_{i, 1} {L'}_{j, 1} \E[z_1^2] + \dotsb +
     L_{i, m} {L'}_{j, m} \E[z_m^2]
  \shortintertext{For any \( i \), \( \E[z_i^2]
    = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \)}
  &= L_{i, 1} {L'}_{j, 1} + \dotsb + L_{i, m} {L'}_{j, m} \\
  &= L_i \cdot {L'}_j
  \shortintertext{Thus, the new covariance matrix can be written as}
  \Cov[\vec{y}_{\parallel k}] &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}

\subsection{Aggregated computation of sparsity entries}
\label{app:L_mult}

We wish to compute the entries of the sparse Cholesky factor
\( L_{s_i, i} \) according to \cref{eq:L_col} in the aggregated
sparsity setting of \cref{subsubsec:aggregated}.
Recall that we have aggregated the column indices \( i_1 \succ
\dots \succ i_m \) into \( \tilde{i} = \{ i_1, \dotsc, i_m \} \)
and \( s_\tilde{i} \) denotes the aggregated sparsity pattern.
The sparsity pattern for the \( i \)th column of the group is all
those entries that satisfy lower-triangularity, \( s_i \defeq \{
j \in s_{\tilde{i}} : j \succeq i \} \subseteq s_{\tilde{i}} \).
Because each column's sparsity pattern is a subset of the
overall sparsity pattern, it is possible to compute an outer
approximation and specialize to each column efficiently.
Assuming the sparsity entries \( s_{\tilde{i}} \) are sorted according to
\( \prec \), let \( Q \defeq \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) be
the precision of the aggregated sparsity pattern and let \( k \) be the \(
i \)th column's index in \( s_{\tilde{i}} \).
Because \( s_{\tilde{i}} \) is sorted according to \(
\prec \), the sparsity pattern for the \( i \)th column
\( s_i \) is exactly the entries \( k \) and after.
\begin{align}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}} \\
  \shortintertext{Since \( Q^{-1} = \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \)
    and \( s_i \) is the \( k \)th index of \( s_{\tilde{i}} \) and after,}
  &= \frac{\left (Q^{-1} \right )_{k:, k:}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \left (Q^{-1} \right )_{k:, k:} \vec{e}_1}} \\
  \shortintertext{By \cref{eq:inverse_cond}, we can turn
    marginalization in precision into conditioning in covariance.}
  \label{eq:L_precision}
             &= \frac{Q_{k:, k: \mid :k - 1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} Q_{k:, k: \mid :k - 1} \vec{e}_1}}
\end{align}
So we want the \( k \)th column of \( Q
\), conditional on all columns before it.
From \cref{eq:chol}, this can be directly read off the \(
k \)th column of the Cholesky factor \( L = \chol(Q) \) to
compute \cref{eq:L_col} for each \( i \in \tilde{i} \).
However, computing \( Q = \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) by
inverting \( \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) and then additionally
computing its Cholesky factor \( L = \chol(Q) \) is a bit wasteful.

Instead of computing a lower-triangular factor for the precision, we can
compute an \emph{upper}-triangular factor for the covariance whose inverse
transpose will be a \emph{lower}-triangular factor for the precision.
Using the same flipping trick as \cref{eq:U_chol}, let \( U = P^{\Reverse}
\chol(P^{\Reverse} \CM_{s_{\tilde{i}}, s_{\tilde{i}}} P^{\Reverse})
P^{\Reverse} \) where \( P^{\Reverse} \) is the order-reversing permutation;
\( U \) is upper-triangular and satisfies \( U U^{\top} = \CM_{s_{\tilde{i}},
s_{\tilde{i}}} \) so \( U^{-\top} U^{-1} = \CM_{s_{\tilde{i}},
s_{\tilde{i}}}^{-1} = Q \) and we see that \( L = U^{-\top} \) is a
lower-triangular Cholesky factor satisfying \( L L^{\top} = Q \).
Thus we can compute \( U \) as a Cholesky factor of the covariance and
dynamically form its inverse transpose by solving the triangular system
\( L_{:, k} = U^{-T} \vec{e}_k \) for the desired column \( k \) of \(
L \) (like \cref{eq:obj_chol}, \( \vec{e}_k \) is the vector with \( k
\)th entry one and rest zero).

\section{Computation in sparse Gaussian process selection}

\subsection{Mutual information objective}
\label{app:mutual_info}

The mutual information or information gain between two collections of
random variables \( \vec{y}_\Pred \) and \( \vec{y}_\Train \) is defined as
\begin{align}
  \label{eq:info}
  \MI[\vec{y}_\Pred;\vec{y}_\Train] &= \entropy[\vec{y}_\Pred] -
    \entropy[\vec{y}_\Pred \mid \vec{y}_\Train]
\end{align}
Maximizing the mutual information is equivalent to minimizing the
conditional entropy since the entropy of \( \vec{y}_\Pred \) is constant.
Because the differential entropy of a multivariate Gaussian is monotonically
increasing with the log determinant of its covariance matrix, minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix.
For a single predictive point, its log determinant is simply its variance.
Thus, maximizing mutual information minimizes the
\emph{conditional variance} of the target point.
In particular, because our estimator is the conditional
expectation \cref{eq:cond_mean}, it is unbiased because \(
\E[\E[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[\vec{y}_\Pred] \).
Because it is unbiased, its expected mean squared error is simply the
conditional variance since \( \E[(\vec{y}_\Pred - \E[\vec{y}_\Pred
\mid \vec{y}_\Train])^2 \mid \vec{y}_\Train] = \Var[\vec{y}_\Pred
\mid \vec{y}_\Train] \) where the outer expectation is taken under
conditioning because of the assumption that \( \vec{y}_\Pred \) is
distributed according to the Gaussian process.
So maximizing the mutual information is equivalent to minimizing
the conditional variance which is in turn equivalent to
minimizing the expected mean squared error of the prediction.
Another perspective arises from comparing the definition of mutual
information \cref{eq:info} to the EV-VE identity \cref{eq:eve},
\begin{align}
  \label{eq:info_eve}
  \textcolor{orange}{\entropy[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\Pred \mid \vec{y}_\Train]} +
    \textcolor{rust}{\MI[\vec{y}_\Pred;\vec{y}_\Train]} \\
  \label{eq:eve}
  \textcolor{orange}{\Var[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\Pred \mid \vec{y}_\Train]]}
\end{align}
On the left hand side, entropy is monotone with variance.
On the right hand side, the expectation of conditional variance
is monotone with conditional entropy and can be interpreted to
be the fluctuation of the prediction point after conditioning.
Because the sum of the two terms on the right hand side is constant,
minimizing the expectation of conditional variance is equivalent to
maximizing the variance of conditional expectation, which corresponds
to the mutual information.

Supposing \( \vec{y}_\Pred \) was independent of \( \vec{y}_\Train
\), then the predictor \( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)
reduces to the constant \( \E[\vec{y}_\Pred] \) whose variance is 0.
Meanwhile, the error \( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]
\) becomes \( \E[\Var[\vec{y}_\Pred]] = \Var[\vec{y}_\Pred] \).
On the other extreme, supposing \( \vec{y}_\Pred \) was a deterministic
function of \( \vec{y}_\Train \), then \( \Var[\E[\vec{y}_\Pred \mid
\vec{y}_\Train]] = \Var[\vec{y}_\Pred] \) while the error term becomes
\( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[0] = 0 \).
Thus, the variance of conditional expectation is the information shared
between \( \vec{y}_\Pred \) and \( \vec{y}_\Train \), as it increases
when the predictor for \( \vec{y}_\Pred \) (the conditional expectation
\( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)) depends on the observed
results of \( \vec{y}_\Train \).

\subsection{Updating precision after insertion}
\label{app:prec_insert}

Assuming we have the precision matrix \( \CM_{1, 1}^{-1} \), we wish
to compute the precision of the covariance \( \CM_{1, 1} \) with a
new row and column added to it, that is, compute \( \CM^{-1} \) for
\(
  \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\)
where \( \CM_{2, 1} = \CM_{1, 2}^{\top} \) is a column vector and \(
\CM_{2, 2} \) is a scalar, assuming \( \CM \) is positive-definite.
\begin{align}
  \shortintertext{Using the same block \( L D
    L^{\top} \) factorization as \cref{eq:chol_schur},}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{For brevity of notation, we denote the Schur complement
    \textcolor{lightblue}{\( \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1,
    2} \)} as the conditional covariance \textcolor{lightblue}{\( \CM_{2, 2
    \mid 1} \)}. Inverting both sides of the equation,}
  \CM^{-1} &=
  \begin{pmatrix}
    \Id & -\textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & 0 \\
    -\textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \CM_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry to the matrix, \(
    \CM_{1, 1} = \CM_{\I, \I} \), \( \CM_{1, 2} = \CM_{\I, k} \), and \(
    \CM_{2, 2} = \CM_{k, k} \). Also note that \( \textcolor{lightblue}{\CM_{k,
    k \mid \I}^{-1}} \) is the precision of \( k \) conditional on the entries
    in \( \I \), which has already been computed in \cref{alg:select_prec}.
    If we let \( \vec{v} = \textcolor{darkorange}{\CM_{\I, \I}^{-1} \CM_{\I,
    k}} \), then}
  &=
  \begin{pmatrix}
    \CM_{\I, \I}^{-1} + \CM_{k, k \mid \I}^{-1} \vec{v} \vec{v}^T &
    -\CM_{k, k \mid \I}^{-1} \vec{v} \\
    -\CM_{k, k \mid \I}^{-1} \vec{v}^{\top} & \CM_{k, k \mid \I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:select_prec}.
Note that the bulk of the update is a rank-one update to \( \CM_{1,
1}^{-1} \), which can be computed in \( \BigO(\card{\I}) = \BigO(s^2) \).

\subsection{Updating precision after marginalization}
\label{app:prec_delete}

Suppose we have the precision \( \CM^{-1} \) and wish to compute the precision
of the marginalized covariance \( \CM \) after ignoring an index \( k \).
That is, we wish to compute the inverse of a matrix after deleting
a row and column, given the inverse of the original matrix.
We could use the result in \cref{app:prec_insert}
by ``reading'' the update backwards.
That is, we could identify \( \CM_{2, 2 \mid 1}^{-1} \) from \(
(\CM^{-1})_{k, k} \) and \( \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} \)
from \( - \frac{(\CM^{-1})_{-k, k}}{\CM_{2, 2 \mid 1}^{-1}} \)
where \( -k \) denotes all rows excluding the \( k \)th row.
We can then revert the rank-one update by subtracting out
the update, computing \( \CM_{-k, -k}^{-1} = (\CM^{-1})_{-k,
-k} - \CM_{k, k \mid I}^{-1} \vec{v} \vec{v}^{\top} \).
However, a more intuitive derivation relies on the fact that
marginalization in covariance is conditioning in precision.
Using \cref{eq:inverse_cond}, we see that \(
\CM_{-k, -k}^{-1} = (\CM^{-1})_{-k, -k \mid k} \),
or the precision conditional on the deleted entry.
By \cref{eq:cond_cov}, we immediately obtain the equivalent update
\begin{align}
  (\CM^{-1})_{-k, -k \mid k} &= \CM^{-1}_{-k, -k} -
    \frac{(\CM^{-1})_{-k, k}
          (\CM^{-1})_{-k, k}^{\top}}{(\CM^{-1})_{k, k}}
\end{align}
Since this is a rank-one update to the precision \( \CM^{-1} \),
this can be computed in \( \BigO(\text{\# rows}(\CM^{-1}))^2 \).

\Stodo{this is not used in the paper but is
nice to know + used in the sensor placement}

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

Assuming we have the precision matrix of the prediction points conditional on
the selected entries, \( \CM_{\Pred, \Pred \mid \I}^{-1} \), we want to take
into account selecting an index \( k \), or to compute \( \CM_{\Pred, \Pred
\mid \I \cup \{ k \}}^{-1} \), which is a rank-one update to the covariance
(but not necessarily the precision) from \cref{eq:obj_gp_mult}.
We can directly apply the ShermanMorrisonWoodbury
formula which states that:
\begin{align}
  \CM_{1, 1 \mid 2}^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning from \cref{eq:cond_cov},}
  \left (
    \CM_{1, 1} - \CM_{1, 2} \CM_{2, 2}^{-1} \CM_{2, 1}
  \right )^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{For brevity of notation, letting \( \vec{u} = \CM_{1, 2} \)
    and \( \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} = \CM_{1, 1}^{-1} \vec{u} \),}
  (\CM_{1, 1} - \CM_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \CM_{1, 1}^{-1} + \CM_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{So we see that a rank-one update to \( \CM_{1, 1}
    \) then inverting is a rank-one update to \( \CM_{1, 1}^{-1} \). In
    our context, \( \CM_{1, 1} = \CM_{\Pred, \Pred \mid \I}, \vec{u}
    = \CM_{\Pred, k \mid I}, \CM_{2, 2} = \CM_{k, k \mid \I} \) so \(
    \CM_{2, 2 \mid 1}^{-1} = \CM_{k, k \mid \Pred, I}^{-1} \) (this can
    be rigorously shown by expanding the Schur complement and taking
    advantage of the quotient rule as in \cref{eq:greedy_mult}). \(
    \vec{v} \) can be computed according to definition as \( \CM_{\Pred,
    \Pred \mid \I}^{-1} \vec{u} \). Thus, we can write the update as}
  \left ( \CM_{\Pred, \Pred \mid \I} -
    \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
         {\CM_{kk \mid \I}}
  \right )^{-1} &=
    \CM_{1, 1}^{-1} +
    \CM_{k, k \mid \Pred, \I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:select_mult_prec}.
Since the update is a rank-one update, it can be
computed in \( \BigO(\card{\Pred}^2) = \BigO(m^2) \).

\subsection{Updating the log determinant after a rank-one downdate}
\label{app:logdet_downdate}

Assuming we already have the log determinant of the covariance matrix of the
prediction points conditional on the selected entries, \( \logdet(\CM_{\Pred,
\Pred \mid \I}) \), we wish to compute the log determinant after we add an
index \( k \) to \( \I \), that is, to compute \( \logdet(\CM_{\Pred, \Pred
\mid \I \cup \{ k \}}) \).
\begin{align}
  \shortintertext{From \cref{eq:cond_select}, selecting a
    new point is a rank-one downdate on the covariance matrix.}
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{Using the matrix determinant lemma,}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( 1 -
       \frac{\CM_{\Pred, k \mid \I}^{\top} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{Focusing on the second term, we can turn
    the quadratic form into condtioning.}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left (
       \frac{\CM_{k, k \mid \I} -
             \CM_{k, \Pred \mid \I} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{By the quotient rule
    \cref{eq:quotient_rule}, we combine the conditioning.}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}

\subsection{Updating a Cholesky factor after a rank-one downdate}
\label{app:chol_downdate}

\Stodo{remove because unnecessary, describe insertion instead}

We use the approach from Lemma 1 of \cite{krause2015more}, slightly adapted
to use in-place operations and to make no assumption on the particular row
ordering of the Cholesky factor. Let \( L \) be a Cholesky factorization of \(
\CM \), that is, \( L = \chol(\CM) \). We wish to compute the updated
Cholesky factor \( L' = \chol(\CM') \) where \( \CM' = \CM - \vec{u}
\vec{u}^{\top} \). To do so, assume \( L \) and \( L' \) are blocked according
to the same block structure:
\begin{align}
  L &=
  \begin{pmatrix}
    r_1 & \vec{0} \\
    \vec{r}_2 & L_2
  \end{pmatrix},
  L' =
  \begin{pmatrix}
    r_1' & \vec{0} \\
    \vec{r}_2' & L_2'
  \end{pmatrix}
  \shortintertext{Multiplying, we find}
  L L^{\top} = \CM &=
  \begin{pmatrix}
    r_1^2 & r_1 \vec{r}_2^{\top} \\
    r_1 \vec{r}_2 & L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
  \end{pmatrix}
  \\ L' L'^{\top} = \CM' &=
  \begin{pmatrix}
    r_1'^2 & r_1' \vec{r}_2'^{\top} \\
    r_1' \vec{r}_2' & L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top}
  \end{pmatrix}
  \shortintertext{From here, we solve for
    \( r'_1 \), \( \vec{r}' \), and \( L_2' \)}
  r_1'^2 &= \CM'_{11} = \CM_{11} - u_1^2 \\
         &= r_1^2 - u_1^2 \\
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  r_1' \vec{r}_2' &= \CM'_{2:, 1} = \CM_{2:, 1} - u_1 \vec{u}_2 \\
                  &= r_1 \vec{r}_2 - u_1 \vec{u}_2 \\
  \vec{r}_2' &= \frac{1}{r_1'} (r_1 \vec{r}_2 - u_1 \vec{u}_2) \\
  % L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top} &= \CM'_{22}
  %   = \CM_{22} - \vec{u}_2 \vec{u}_2^{\top} \\
  L_2' L_2'^{\top} &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \vec{r}_2' \vec{r}_2'^{\top}
  \shortintertext{Plugging in the expresion for \( \vec{r}'_2 \),}
                   &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right ) \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right )^{\top} \\
                   &=  L_2 L_2^{\top} +
    \left ( 1 - \frac{r_1^2}{r_1'^2} \right ) \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
    \left ( 1 + \frac{u_1^2}{r_1'^2} \right ) \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{Using \( r_1' = \sqrt{r_1^2 - u_1^2} \),}
                   &=  L_2 L_2^{\top} -
      \frac{u_1^2}{r_1'^2} \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
      \frac{r_1^2}{r_1'^2} \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{After factoring we find}
  L_2' L_2'^{\top} &= L_2 L_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right ) \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right )^{\top}
  \shortintertext{which is a rank-one downdate to the subfactor \( L_2 \).
    Recursively updating \( L_2 \) yields a \( \BigO(N^2) \) algorithm.
    We now re-write the algorithm to be in-place to take advantage of BLAS
    routines. The updates can be summarized as:}
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  \vec{r}' &= \frac{r_1}{r_1'} \vec{r} - \frac{u_1}{r_1'} \vec{u} \\
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'} \vec{r}
  \shortintertext{Note that we drop the subscripting on \( \vec{r} \) and \(
    \vec{u} \). By updating the entire vector on each iteration, we can avoid
    keeping track of the lower triangular structure of \( L \). We will first
    update \( \vec{r}' \) and then use it to update \( \vec{u} \). Solving for
    \( \vec{r} \) in terms of \( \vec{r}' \),}
  \vec{r} &= \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \\
  \shortintertext{Plugging the expression for \( \vec{r} \) into
    the update for \( \vec{u}' \),}
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'}
    \left ( \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \right ) \\
           &= \frac{r_1^2 - u_1^2}{r_1 r_1'} \vec{u}
            - \frac{u_1}{r_1} \vec{r}' \\
           &= \frac{r_1'}{r_1} \vec{u} - \frac{u_1}{r_1} \vec{r}'
  \shortintertext{Thus, the updates proceed sequentally as follows:}
  \gamma &\gets \sqrt{r_1^2 - u_1^2} \\
  \alpha &\gets \frac{r_1}{\gamma} \\
  \beta &\gets \frac{u_1}{\gamma} \\
  \vec{r} &\gets \alpha \vec{r} - \beta \vec{u} \\
  \vec{u} &\gets \frac{1}{\alpha} \vec{u} - \frac{\beta}{\alpha} \vec{r}
\end{align}
These can be efficiently performed in-place by
BLAS as level-one \texttt{daxpy} operations.

\subsection{Global greedy selection}
\label{app:global_greedy}

Although each column is essentially independent from the perspective of
selection, if there is a prescribed budget for the number of nonzeros then
there is the problem of distributing the nonzeros over the columns. A
natural method is to distribute as evenly as possible, this is efficient and
practically useful. However, one principled way of allocating nonzeros is
to maintain a \emph{global} priority queue over all columns, and selecting
from this queue determines not only which entry out of the candidate set is
added as a nonzero, but also which column to select from. This allows the
algorithm to greedily select the next entry which will decrease the global
objective \cref{eq:obj_chol} as much as possible. The main change is that
within a column, any monotonic transformation of the objective will preserve
the relative ranking of candidates, for example adding a constant, multiplying
by a constant, taking the log, etc. However, from the global perspective, if
one column adds a different constant to their objectives than another column,
the relative ranking of candidates between columns is skewed. Thus, each
column must maintain an objective that corresponds directly to minimizing the
global objective \cref{eq:obj_chol}. Here we describe the modifications that
must be made to the selection algorithms to support global comparison.

\subsubsection{Single column selection}

For a single prediction point, the objective is \( \frac{\CM_{k, \Pred
\mid I}^2}{\CM_{kk \mid I}} \) \cref{eq:obj_gp} which is exactly the amount
the variance of the prediction point is decreased if the \( k \)th candidate is
selected, that is, \( \CM_{\Pred, \Pred \mid I} - \CM_{\Pred,
\Pred \mid I, k} = \frac{\CM_{k, \Pred \mid I}^2}{\CM_{kk \mid
I}} \). From the global perspective, all other prediction points are untouched,
so the amount the sum of the log variances of all the prediction points changes
is
\begin{align}
  \min \Delta &= \min \left [
    \log(\CM_{\Pred, \Pred \mid I, k}) -
  \log(\CM_{\Pred, \Pred \mid I}) \right ] \\
              &= \min \frac{\CM_{\Pred, \Pred \mid I, k}}
                           {\CM_{\Pred, \Pred \mid I}} \\
              &= \min \frac{\CM_{\Pred, \Pred \mid I}
              - \frac{\CM_{k, \Pred \mid I}^2}{\CM_{kk \mid I}}}
                           {\CM_{\Pred, \Pred \mid I}} \\
              &= \min \left [ 1 -
                \frac{\CM_{k, \Pred \mid I}^2}
                     {\CM_{kk \mid I} \CM_{\Pred, \Pred \mid I}}
                 \right ] \\
  \label{eq:global_obj}
              &= \max
                \frac{\CM_{k, \Pred \mid I}^2}
                     {\CM_{kk \mid I} \CM_{\Pred, \Pred \mid I}}
\end{align}
where \cref{eq:global_obj} can be interpreted as the
\emph{percentage} of the decrease in variance from selecting
the \( k \)th point to the variance before selecting the point.

\subsubsection{Aggregated selection}

Since the nonadjacent algorithm directly computes the sum of the log of
the conditional variances of the prediction points, few modifications
have to be made. One heuristical improvement is to take into account
for ``bang-for-buck'', that is, to account for the fact that different
candidates cost a different amount of nonzero entries. Selecting a
candidate can add between 1 and the number of columns in its aggregated
group, depending on its relative index. Thus, candidates with larger
groups will appear to decrease the global variance more, even if they
are not as efficient as a candidate with a single group. In practice, it
is better to use the objective \( \frac{\Delta}{n} \) where \( \Delta
\) is the change in variance after selecting the candidate and \( n \)
is the number of nonzero entries selecting the candidate adds.

\subsection{Equivalence of Cholesky and QR factorization}
\label{app:qr}

We show a well-known fact that QR factorization can be viewed
as the feature-space equivalent of Cholesky factorization,
which can be viewed as operating in covariance-space.
\begin{align}
  \shortintertext{Let \( \CM \) be a symmetric
    positive-definite matrix such that}
  \CM &= F^{\top} F
  \shortintertext{for some matrix \( F \) whose
    columns can be viewed as vectors in feature space:}
  \CM_{ij} &= \langle F_i, F_j \rangle
  \shortintertext{where \( F_i \) is the \( i \)th column of
    \( F \). Now suppose \( F \) has the \( QR \) factorization}
  F &= QR
  \shortintertext{where \( Q \) is a \( N \times N \) orthonormal
    matrix and \( R \) is a \( N \times N \) upper triangular matrix.}
  \CM &= F^{\top} F = (Q R)^{\top} (Q R) \\
         &= R^{\top} Q^{\top} Q R \\
         &= R^{\top} R
\end{align}
from the orthogonality of \( Q \). But note that \( R \) is an upper triangular
matrix, so \( L = R^{\top} \) is an lower triangular matrix. So we have \(
\CM = L L^{\top} \) for lower triangular \( L \). By the uniqueness of
Cholesky factorization, \( R^{\top} \) is precisely the Cholesky factor of \(
\CM \). In addition, the columns of \( Q \) are formed from Gram-Schmitt
orthogonalization on the columns of \( F \) (in feature-space), and \( R \)
the coefficients resulting from the Gram-Schmitt procedure. From \( R^{\top} =
\chol(\CM) \) and the statistical interpretation of Cholesky factorization
\cref{eq:chol}, this iterative orthogonalization in feature-space is equivalent
to iterative conditioning in covariance.

\subsection{Equivalence of selection and orthogonal matching pursuit}
\label{app:omp}

We show that the single-point selection algorithm described in
\cref{alg:select_chol} is the covariance space equivalent to the
feature space orthogonal matching pursuit (OMP) algorithm described
in \cite{tropp2007signal}. The equivalence comes from the fact
that Cholesky factorization is Gram-Schmitt in feature space.

\begin{align}
  \shortintertext{Let \( \CM \) be a symmetric
    positive-definite matrix such that}
  \CM &= F^{\top} F
  \shortintertext{for some matrix \( F \)
    whose columns are vectors in feature space,}
  F &=
  \begin{pmatrix}
    \vec{x}_1 & \vec{x}_2 & \hdots & \vec{x}_N
  \end{pmatrix}
  \shortintertext{Immediately we have}
  \CM_{ij} &= \langle \vec{x}_i, \vec{x}_j \rangle
  \shortintertext{where \( \langle \cdot, \cdot \rangle \)
    denotes the ordinary inner product on \( \mathbb{R}^N \).}
  \shortintertext{It suffices to see a single step of Cholesky
    factorization. Selecting \( \vec{x}_1 \),}
  \CM' &= \CM - \frac{\vec{x}_1 \vec{x}_1^{\top}}
    {\CM_{11}} \\
  \label{eq:cov_step}
  \CM_{ij}' &= \CM_{ij} -
    \frac{\CM_{i1} \CM_{j1}}{\CM_{ii}}
  \shortintertext{Switching to the feature space perspective,
    if we select \( \vec{x}_1 \) we force the rest of the
    feature vectors to be orthogonal to \( \vec{x}_1 \),}
  \vec{x}_i' &= \vec{x}_i -
    \frac{\langle \vec{x}_i, \vec{x}_1 \rangle}
         {\langle \vec{x}_1, \vec{x}_1 \rangle} \vec{x}_1 \\
  \label{eq:feature_step}
  \langle \vec{x}_i', \vec{x}_j' \rangle &=
    \langle \vec{x}_i, \vec{x}_j \rangle -
      \frac{\langle \vec{x}_i, \vec{x}_1 \rangle
            \langle \vec{x}_j, \vec{x}_1 \rangle}
          {\langle \vec{x}_1, \vec{x}_1 \rangle}
  \shortintertext{Comparing \cref{eq:cov_step} and \cref{eq:feature_step},
    we see that they are the same as expected. As a corollary, the objective
    of selecting the point \( \vec{x}_k \) that minimizes the residual of some
    target point \( \vec{x}_\Pred \) can be written as}
  \lVert
    \vec{x}_\Pred - \text{proj}_{\vec{x}_k} \vec{x}_\Pred
  \rVert &= \langle \vec{x}_\Pred, \vec{x}_\Pred \rangle -
    \frac{\langle \vec{x}_\Pred, \vec{x}_k \rangle^2}
        {\langle \vec{x}_k, \vec{x}_k \rangle}
  % \shortintertext{By induction, one can show}
  % F_2' = F_2 - P_1 F_2
  % \shortintertext{where \( F_2 \) is a set of feature
  %   vectors being conditioned and \( P_1 \) is the projection
  %   matrix onto the subspace spanned by \( F_1 \).}
\end{align}
which is precisely the squared covariance of the candidate with the
prediction over the variance of the candidate, as in \cref{eq:obj_gp}.
This shows the equivalence as the objective is the same.

\subsection{Checking submodularity}
\label{app:submodular}

Our objective is the mutual information between the prediction and training
points \cref{eq:info}. A natural question is whether this objective is
submodular with respect to the training set. The answer is no in general,
see \cite{krause2008nearoptimal}, section 8.3 for a simple counterexample.
However, we can empirically check submodularity for particular geometries
and choices of kernel function. If \( \Pred \) is the set of prediction
points, \( I \) is a set of indexes, and \( x_1, x_2 \) are additional
indices not in \( I \), then the set function is submodular if and only if
\begin{align*}
  \MI(\Pred, I \cup \{ x_2 \}) - \MI(I) &\overset{?}{\geq}
    \MI(\Pred, I \cup \{ x_1, x_2 \}) - \MI(\Pred, I \cup \{ x_1 \})
  \shortintertext{Expanding from the definition
    of mutual information \cref{eq:info},}
  \entropy[\Pred \mid I] - \entropy[\Pred \mid I \cup \{ x_2 \}]
    &\overset{?}{\geq}
    \entropy[\Pred \mid I \cup \{ x_1 \}] -
    \entropy[\Pred \mid I \cup \{ x_1, x_2 \}]
  \shortintertext{Since this is the objective \cref{eq:obj_gp_mult}
    (with additional log),}
  \frac{\CM_{x_2, x_2 \mid I}}{\CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\geq}
    \frac{\CM_{x_2, x_2 \mid I} -
          \frac{\CM_{x_1, x_2 \mid I}^2}{\CM_{x_1, x_1 \mid I}}}
         {\CM_{x_2, x_2 \mid I, \Pred} -
         \frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
              {\CM_{x_1, x_1 \mid I, \Pred}}}
    \shortintertext{From the fact that \(
    \frac{a}{b} \geq \frac{a - c}{b - d} \) if and only if
     \( \frac{a}{b} \leq \frac{c}{d} \),}
  \frac{\CM_{x_2, x_2 \mid I}}{\CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\frac{\CM_{x_1, x_2 \mid I}^2}{\CM_{x_1, x_1 \mid I}}}
         {\frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
               {\CM_{x_1, x_1 \mid I, \Pred}}}
  \shortintertext{Multiplying by \( \frac{\CM_{x_1, x_1 \mid
    I}}{\CM_{x_1, x_1 \mid I, \Pred}} \) on both sides,}
  \frac{\CM_{x_1, x_1 \mid I} \CM_{x_2, x_2 \mid I}}
       {\CM_{x_1, x_1 \mid I, \Pred}
        \CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\CM_{x_1, x_2 \mid I}^2}
         {\CM_{x_1, x_2 \mid I, \Pred}^2}
  \shortintertext{Multiplying by \( \frac{\CM_{x_1,
    x_2 \mid I, \Pred}^2}{\CM_{x_1, x_1 \mid
    I} \CM_{x_2, x_2 \mid I}} \) on both sides,}
  \frac{\CM_{x_1, x_2 \mid I, \Pred}^2}
       {\CM_{x_1, x_1 \mid I, \Pred}
        \CM_{x_2, x_2 \mid I, \Pred}}
    &\overset{?}{\leq}
    \frac{\CM_{x_1, x_2 \mid I}^2}
         {\CM_{x_1, x_1 \mid I} \CM_{x_2, x_2 \mid I}}
  \shortintertext{By definition, this is}
  \Corr[x_1, x_2 \mid I, \Pred] &\overset{?}{\leq}
    \Corr[x_1, x_2 \mid I]
\end{align*}
so the mutual information objective is submodular if and only if
conditioning on additional point(s) decreases the correlation between
every pair of points. Intuitively, this corresponds to the screening
effect observed in spatial statistics literature --- conditioning on
a nearby point decreases the correlation for far away points.

\newpage

\section{Algorithms}

\begin{figure}[th!]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Point selection by \\ explicit precision}
    \label{alg:select_prec}
    \input{figures/algorithms/select_prec.tex}
  \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Point selection by \\ Cholesky factorization}
    \label{alg:select_chol}
    \input{figures/algorithms/select_chol.tex}
  \end{algorithm}
  \end{minipage}
  \caption{Algorithms for single-point selection.}
  \label{fig:alg_select}
\end{figure}

\begin{algorithm}[H]
  \caption{Direct sparse Gaussian process regression by selection}
  \label{alg:infer_select}
  \input{figures/algorithms/infer_select.tex}
\end{algorithm}

\begin{figure}[th!]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Multiple-point selection by explicit precision}
    \label{alg:select_mult_prec}
    \input{figures/algorithms/select_mult_prec.tex}
  \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Multiple-point selection by Cholesky factorization}
    \label{alg:select_mult_chol}
    \input{figures/algorithms/select_mult_chol.tex}
  \end{algorithm}

  \begin{algorithm}[H]
    \caption{Update Cholesky factor}
    \label{alg:chol_update}
    \input{figures/algorithms/chol_update.tex}
  \end{algorithm}
  \end{minipage}
  \caption{Algorithms for multiple-point selection.}
  \label{fig:alg_mult_select}
\end{figure}

\begin{figure}[th!]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Partial point selection}
    \label{alg:select_partial}
    \input{figures/algorithms/select_partial.tex}
  \end{algorithm}
  \begin{algorithm}[H]
    \caption{Find \( j \)'s index in \( \Order \)}
    \label{alg:insert_index}
    \input{figures/algorithms/insert_index.tex}
  \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
  \begin{algorithm}[H]
    \caption{Compute \( j \)'s objective}
    \label{alg:partial_score}
    \input{figures/algorithms/partial_score.tex}
  \end{algorithm}
  \begin{algorithm}[H]
    \caption{Update \( L \) after insertion}
    \label{alg:chol_insert}
    \input{figures/algorithms/chol_insert.tex}
  \end{algorithm}
  \end{minipage}
  \caption{Algorithms for partial selection.}
  \label{fig:alg_partial_select}
\end{figure}

\end{document}
