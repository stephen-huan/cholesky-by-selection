% SIAM Article Template

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={m-calculus},
  pdfauthor={S Huan, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  experimental design for linear algebra
\end{abstract}

% REQUIRED
\begin{keywords}
   \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}
\cite{schafer2020compression} test citation

\section{Greedy selection for directed inference}

\subsection{Conditional \textit{k}-th nearest neighbors}

Consider the simple regression algorithm \( k \)th-nearest neighbors (\( k
\)-NN). Given a training set \( X_\text{Tr} = \{ \vec{x}_1, \dotsc, \vec{x}_n
\} \) and corresponding labels \( \vec{y}_\text{Tr} = \{ y_1, \dotsc, y_n \}
\), the goal is to estimate the unknown label \( y_\text{Pr} \) of some unseen
prediction point \( \vec{x}_\text{Pr} \) Stated informally, the \( k \)-NN
approach is to select the \( k \) points in \( X_\text{Tr} \) \emph{most
informative} about \( \vec{x}_\text{Pr} \) and combine their results.

\begin{algorithm}
  \caption{Idealized \( k \)-NN regression}
  Given \( (X_\text{Tr}, \vec{y}_\text{Tr}) =
        \{ (\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n) \} \)
  and \( \vec{x}_\text{Pr} \)
  \begin{enumerate}
    \item Select the \( k \) points in \( X_\text{Tr} \)
      most informative about \( \vec{x}_\text{Pr} \)
    \item Combine the labels of the selected points to generate a prediction
  \end{enumerate}
\end{algorithm}

One \Ftodo{"specific" and "intuitively" are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much as
possible. It's normal to add them out of reflex initially, so it requires
active postprocessing.} specific approach is intuitively, points close
to \( \vec{x}_\text{Pr} \) should be similar to it. So we select the \(
k \) closest points in \( X_\text{Tr} \) to \( \vec{x}_\text{Pr} \) and
pool their labels (e.g., by averaging).

\begin{algorithm}
  \caption{\( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) closest to \( \vec{x}_\text{Pr} \)
    \item Compute \( \vec{y}_\text{Pr} \) by
      \( \frac{1}{k} \sum_{j = 1}^k y_{i_j} \)
  \end{enumerate}
\end{algorithm}

However, we can generalize the notion of ``closest'' with the \emph{kernel
trick}, by using an arbitrary kernel function to measure similarity. For
example, commonly used kernels like the Gaussian kernel and Mat{\'e}rn family
of covariance functions are \emph{isotropic}; they depend only on the distance
between the two vectors. If such isotropic kernels monotonically decrease with
distance, then selecting points based on the largest kernel similarity recovers
\( k \)-NN. However, kernels need not be isotropic in general --- they just
need to capture some sense of ``similarity'', motivating kernel \( k \)-NN.

\Stodo{not sure whether ``stationary'' or
``isotropic'' are the right word(s) to use here}

\begin{algorithm}
  \caption{Kernel \( k \)-NN regression}
  Given kernel function \( K(\vec{x}, \vec{y}) \)
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) most similar to \( \vec{x}_\text{Pr} \)
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by similarity
  \end{enumerate}
\end{algorithm}

Although the kernel \( k \)-NN approach is more general than its normed
counterpart, it still suffers from a fundamental issue. Suppose the closest
point to \( \vec{x}_\text{Pr} \) has many duplicates in the training set. Then
the algorithm will select the same point multiple times, even though in some
sense the duplicate point has stopped giving additional information about the
prediction point. In order to fix this issue, we should be selecting new points
\emph{conditional} on the points we've already selected. This preserves the
idealized algorithm of selecting points based on the information they tell us
about the prediction point --- once we've selected a point, conditioning on
it reduces the information similar points tells us, encouraging diverse point
selection.

\begin{algorithm}
  \caption{\textcolor{lightblue}{Conditional} kernel \( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \}  \subseteq X_\text{Tr} \) most \textcolor{lightblue}{informative} to
      \( \vec{x}_\text{Pr} \) \\ \textcolor{lightblue}{after conditioning on
      all points selected beforehand}
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by
      \textcolor{lightblue}{information}
  \end{enumerate}
\end{algorithm}

In order to make the notions of conditioning and information precise, we need a
specific framework. Kernel methods lead naturally to Gaussian processes, whose
covariance matrices naturally result from kernel functions and allows us to use
the rigorous statistical and information-theoretic notions of conditioning and
information.

\Stodo{mention sensor placement/spatial statistics perspective/literature}

\subsection{Sparse Gaussian process regression}
\label{subsec:gp_reg}

A \emph{Gaussian process} is a prior distribution over functions, such that
for any finite set of points, the corresponding function over the points
is distributed according to a multivariate Gaussian. In order to generate
such a distribution over an uncountable number of points consistently, a
Gaussian process is specified by a \emph{mean function} \( \mu(\vec{x}) \) and
\emph{covariance function} or \emph{kernel function} \( K(\vec{x}, \vec{y})
\). For any finite set of points \( X = \{ \vec{x}_1, \dotsc, \vec{x}_n \}
\) ,\( f(X) \sim \mathcal{N}(\vec{\mu}, \Theta) \), where \( \vec{\mu}_i =
\mu(\vec{x}_i) \) and \( \Theta_{ij} = K(\vec{x}_i, \vec{x}_j) \).

In order to compute a prediction at \( \vec{x}_\text{Pr} \), we can
simply condition the desired prediction \( \vec{y}_\text{Pr} \) on the
observed outputs and compute the conditional expectation. We can also
find the conditional variance, which will quantify the uncertainty of
our prediction. If we block our covariance matrix
\(
  \Theta =
  \begin{pmatrix}
    \Theta_{\text{Tr}, \text{Tr}} & \Theta_{\text{Tr}, \text{Pr}} \\
    \Theta_{\text{Pr}, \text{Tr}} & \Theta_{\text{Pr}, \text{Pr}}
  \end{pmatrix}
\)
where \( \Theta_{\text{Tr}, \text{Tr}}, \Theta_{\text{Pr}, \text{Pr}},
\Theta_{\text{Tr}, \text{Pr}}, \Theta_{\text{Pr}, \text{Tr}} \) are the
covariance matrices of the training data, testing data, and training and test
data respectively, then the conditional expectation and covariance are:
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &=
    \vec{\mu}_\text{Pr} +
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    (\vec{y}_\text{Tr} - \vec{\mu}_\text{Tr}) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &=
    \Theta_{\text{Pr}, \text{Pr}} -
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    \Theta_{\text{Tr}, \text{Pr}}
  \shortintertext{For brevity of notation, we will
    often denote the conditional covariance matrix as}
  \label{eq:cond_cov_notation}
  \Theta_{\text{Pr}, \text{Pr} \mid \text{Tr}} &:=
    \Theta_{\text{Pr}, \text{Pr}} -
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    \Theta_{\text{Tr}, \text{Pr}}
  \shortintertext{When conditioning on multiple sets, the sets are given in
    order of computation. Although the resulting covariance matrix is the same,
    a different order of conditioning means different intermediate results in
    repeated application of \cref{eq:cond_cov}. In general,}
  \Theta_{I, J \mid K_1, K_2, \dotsc, K_n} &:=
    \Cov[\vec{y}_I, \vec{y}_J \mid
         \vec{y}_{K_1 \cup K_2 \cup \dotsb \cup K_n}]
  \shortintertext{denotes the covariance between the variables in index
    sets \( I \) and \( J \), conditional on the variables in \( K_1, K_2,
    \dots, K_n \). Note that by the quotient rule of Schur complementation:}
  \label{eq:quotient_rule}
  \Theta_{I, J \mid K_{1 \dots n}} &=
    \Theta_{I, J \mid K_{1 \dots n - 1}} -
    \Theta_{I, K_n \mid K_{1 \dots n - 1}}
    \Theta_{K_n, K_n \mid K_{1 \dots n - 1}}^{-1}
    \Theta_{K_n, J \mid K_{1 \dots n - 1}}
\end{align}

Note that calculating the posterior mean and variance requires inverting the
training covariance matrix \( \Theta_{\text{Tr}, \text{Tr}} \), which costs
\( \mathcal{O}(N^3) \), where \( N \) is the number of training points. This
scaling is prohibitive for large datasets, so many \emph{sparse} Gaussian
process regression techniques have been proposed. These methods often focus
on selecting a subset of the training data that is most informative about the
prediction points, which naturally aligns with our \( k \)-NN perspective.
If \( s \) points are selected out of the \( N \), then the inversion will
cost \( \mathcal{O}(s^3) \), which could be substantially cheaper if \( s \)
is significantly smaller than \( N \). The question is then how to select as
few points as possible while maintaining predictive accuracy.

\Stodo{cite sparse Gaussian regression papers}

\subsection{Problem: optimal selection}

The natural criterion justified from the \( k \)-NN perspective is to maximize
the \emph{mutual information} between the selected points and the target point
for prediction. Such information-theoretic objectives have seen success in
the spatial statistics community \cite{krause2008nearoptimal}, who use such
criteria to determine the best locations to place sensors in a Gaussian process
regression context. The mutual information, or \emph{information gain} is
defined as
\begin{align}
  \label{eq:info}
  \I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}] &= \entropy[\vec{y}_\text{Pr}] -
    \entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]
\end{align}
We can use the fact that the entropy of a multivariate Gaussian is
monotonically increasing with the log determinant of its covariance matrix to
efficiently compute these entropies. Because the entropy of \( \vec{y}_\text{Pr}
\) is constant, maximizing the mutual information is equivalent to minimizing
the conditional entropy. From \cref{eq:cond_cov} we see that minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix. Note that for a single predictive point, this
is monotonic with its variance. So another justification is that we are
reducing the \emph{conditional variance} of the desired point as much as
possible. In particular, because our estimator is the conditional expectation
\cref{eq:cond_mean}, it is unbiased because \( \E[\E[\vec{y}_\text{Pr} \mid
\vec{y}_\text{Tr}]] = \E[\vec{y}_\text{Pr}] \). Because it is unbiased, its
expected mean squared error is simply the conditional variance since \(
\E[(\vec{y}_\text{Pr} - \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}])^2 \mid
\vec{y}_\text{Tr}] = \Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] \) where
the expectation is taken under conditioning because of the assumption that
\( \vec{y}_\text{Pr} \) is distributed according to the Gaussian process. So
maximizing the mutual information is equivalent to minimizing the conditional
variance which is in turn equivalent to minimizing the expected mean squared
error of the prediction. Another perspective on the objective can be derived
from comparing the mutual information to the EV-VE identity, which states
\begin{align*}
  \textcolor{orange}{\entropy[\vec{y}_\text{Pr}]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]} +
    \textcolor{rust}{\I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}]} \\
  \textcolor{orange}{\Var[\vec{y}_\text{Pr}]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]}
\end{align*}
On the left hand side, entropy is monotone with variance. On the right hand
side, the expectation of the conditional variance can be interpreted to be
the fluctuation of the prediction point after conditioning, and is monotone
with the conditional entropy. Because the expectation of conditional variance
and variance of conditional expectation add to a constant, minimizing the
expectation of the conditional variance is equivalent to maximizing the
variance of conditional expectation, which we see corresponds to the mutual
information term. Supposing \( \vec{y}_\text{Pr} \) was independent of \(
\vec{y}_\text{Tr} \), then the conditional expectation becomes simply the
expectation, whose variance is 0. Thus, the variance of the conditional
expectation can be interpreted to be the ``information'' shared between \(
\vec{y}_\text{Pr} \) and \( \vec{y}_\text{Tr} \), as the larger it is, the
more the prediction for \( \vec{y}_\text{Pr} \) (the conditional expectation)
depends on the observed results of \( \vec{y}_\text{Tr} \).

\subsection{A greedy approach}
\label{subsec:greedy}

We now consider how to efficiently minimize the conditional variance
objective using a greedy approach. At each iteration, we pick the training
point which most reduces the conditional variance of the prediction point.
Let \( I = \{ i_1, i_2, \dotsc, i_j \} \) be the set of indexes of training
points selected already. Let the prediction point have index \( n + 1 \),
the last index. For a candidate index \( k \), we update the covariance
matrix after conditioning on \( y_k \), in addition to the indices already
selected according to \cref{eq:cond_cov}:
\begin{align}
  \nonumber
  \Theta_{:, : \mid I, k} &= \Theta_{:, : \mid I} -
    \Theta_{:, k \mid I} \Theta_{k, k \mid I}^{-1} \Theta_{k, : \mid I} \\
  \label{eq:cond_select}
                          &= \Theta_{:, : \mid I} -
    \frac{\Theta_{:, k \mid I} \Theta_{:, k \mid I}^{\top}}{\Theta_{kk \mid I}}
\end{align}

We see that conditioning on a new entry is a rank-one update on the current
covariance matrix \( \Theta_{\mid I} \), given by the vector \( \vec{u} =
\frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk \mid I}}} \). Thus, the amount
that the variance of \( \vec{y}_\text{Pr} \) will decrease after selecting
\( k \) is given by \( u_{n + 1}^2 \), or
\begin{align}
  \label{eq:obj_gp}
  \frac{\Cov[\vec{y}_{\text{Tr}}[k], \vec{y}_\text{Pr}]^2}
       {\Var[\vec{y}_{\text{Tr}}[k], \vec{y}_{\text{Tr}}[k]]} &=
  \frac{\Theta_{k, n + 1 \mid I}^2}{\Theta_{kk \mid I}}
  = \Var[\vec{y}_\text{Pr}]
    \mathrm{Corr}[\vec{y}_\text{Tr}[k], \vec{y}_\text{Pr}]^2
\end{align}
For each candidate \( k \), we need to keep track of its conditional variance
and conditional covariance with the prediction point after conditioning on
the points already selected to compute \cref{eq:obj_gp}. We then simply
choose the candidate with the largest decrease in predictive variance. To
keep track of the conditional variance and covariance, we can simply start
with the initial values given by \( \Theta_{kk} \) and \( \Theta_{nk} \) and
update after selecting an index \( j \). We compute \( \vec{u} \) for \( j
\) directly according to \cref{eq:cond_cov} and update \( k \)'s conditional
variance by subtracting \( u_k^2 \) and update its conditional covariance
by subtracting \( u_k u_{n + 1} \).

In order to efficiently compute \( \vec{u} \), we rely on two main
strategies. The direct method is to keep track of \( \Theta_{I, I}^{-1} \),
or the precision of the selected entries, and update the precision every
time a new index is added to \( I \). This can be done efficiently in \(
\mathcal{O}(s^2) \), see \cref{app:prec_insert}. Once \( \Theta_{I, I}^{-1}
\) has been computed, \( \vec{u} \) is computed trivially according to
\cref{eq:cond_cov}. For each of the \( s \) rounds of selection, it takes
\( s^2 \) to update the precision, and costs \( Ns \) to compute \( \vec{u}
\), costing \( \mathcal{O}(N s^2 + s^3) = \mathcal{O}(N s^2) \) overall.

The second approach is to take advantage of the quotient rule of Schur
complementation. Stated statistically, the quotient rule states that
conditioning on \( I \) and then conditioning on \( J \) is the same as
conditioning on \( I \cup J \). We then remind ourselves that Cholesky
factorization can be viewed as iterative conditioning:
\begin{align}
  \shortintertext{Re-writing the joint covariance matrix,}
  \label{eq:chol_schur}
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    I & \textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \shortintertext{so we see that the Cholesky factorization of the joint
    covariance \( \Theta \) is}
  \label{eq:chol}
  \chol(\Theta) &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \chol(\Theta_{1, 1}) & 0 \\
    0 & \chol(\textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    })
  \end{pmatrix} \\
  \nonumber
  &=
  \begin{pmatrix}
    \chol(\Theta_{1, 1}) & 0 \\
    \Theta_{2, 1} \chol(\Theta_{1, 1})^{-\top} & \chol(
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    )
  \end{pmatrix}
\end{align}
Here \( \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} \)
corresponds to the conditional expectation in \cref{eq:cond_mean} and \\ \(
\textcolor{lightblue}{
  \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
} \)
corresponds to the conditional covariance in \cref{eq:cond_cov}. Thus, we
see that Cholesky factorization is iteratively conditioning the Gaussian
process. From the iterative conditioning perspective, the \( i \)th column
of the Cholesky factor corresponds precisely to the corresponding \( \vec{u}
\) for \( i \) since a iterative sequence of conditioning on \( i_1, i_2
\dotsc \) is equivalent to conditioning on \( I \) by the quotient rule.

The Cholesky factorization can be efficiently computed without excess
dependence on \( N \) with left-looking, so the conditioning only
happens when we need it. For each of the \( s \) rounds of selection,
it costs \( \mathcal{O}(Ns) \) to compute the next column of the
Cholesky factorization, for a total cost of \( \mathcal{O}(Ns^2) \),
matching the time complexity of the explicit precision approach.

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Point selection by \\ explicit precision}
  \label{alg:gp_select}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}_\text{Tr}, \vec{x}_\text{Pr}, K(\cdot, \cdot), s \)
    \ENSURE \( I \)

    \STATE \( n \gets \lvert \vec{x}_\text{Tr} \rvert \)
    \STATE \(
      \vec{x} \gets
      \begin{pmatrix}
        \vec{x}_\text{Tr} \\
        \vec{x}_\text{Pr}
      \end{pmatrix}
    \)
    \STATE \( I \gets \emptyset \)
    \STATE \( -I \gets \{ 1, 2, \dotsc, n \} \)
    \STATE \( \Theta_{I, I}^{-1} \gets \mathbb{R}^{0 \times 0} \)
    \STATE \(
      \Theta_{\text{Tr}, \text{Pr} \mid I} \gets
      K(\vec{x}_\text{Tr}, \vec{x}_\text{Pr})
    \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I}) \gets
      \diag(K(\vec{x}_\text{Tr}, \vec{x}_\text{Tr}))
    \)
    \WHILE{\( \lvert -I \rvert > 0 \) and \( \lvert I \rvert <s \)}
      \STATE \(
        k \gets \max_{j \in -I}
        \frac{\Theta_{j, \text{Pr} \mid I}}{\Theta_{jj \mid I}}
      \)
      \STATE \( I \gets I \cup \{ k \} \)
      \STATE \( -I \gets -I - \{ k \} \)
      \STATE \(
        \vec{v} \gets \Theta_{I, I}^{-1}
        K(\vec{x}_\text{Tr}[I - \{ k \}], \vec{x}_\text{Tr}[k])
      \)
      \STATE \(
        \Theta_{I, I}^{-1} \gets
        \begin{pmatrix}
          \Theta_{I, I}^{-1} +
          \frac{\vec{v} \vec{v}^{\top}}{\Theta_{kk \mid I}} &
          \frac{-\vec{v}}{\Theta_{kk \mid I}} \\
          \frac{-\vec{v}^{\top}}{\Theta_{kk \mid I}} &
          \frac{1}{\Theta_{kk \mid I}}
        \end{pmatrix}
      \)
      \STATE \(
        \Theta_{:, k \mid I} \gets
        K(\vec{x}, \vec{x}_k) -
        K(\vec{x}, \vec{x}_{I - \{ k \}}) \vec{v}
      \)
      \STATE \(
        \vec{u} \gets
        \frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk \mid I}}}
      \)
      \FOR{\( j \in -I \)}
        \STATE \(
          \Theta_{jj \mid I} \gets
          \Theta_{jj \mid I} -
          \vec{u}_j^2
        \)
        \STATE \(
          \Theta_{j, \text{Pr} \mid I} \gets
          \Theta_{j, \text{Pr} \mid I} -
          \vec{u}_j \vec{u}_{n + 1}
        \)
      \ENDFOR
    \ENDWHILE
    \RETURN \( I \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Point selection by \\ Cholesky factorization}
  \label{alg:select_chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}_\text{Tr}, \vec{x}_\text{Pr}, K(\cdot, \cdot), s \)
    \ENSURE \( I \)

    \STATE \( n \gets \lvert \vec{x}_\text{Tr} \rvert \)
    \STATE \(
      \vec{x} \gets
      \begin{pmatrix}
        \vec{x}_\text{Tr} \\
        \vec{x}_\text{Pr}
      \end{pmatrix}
    \)
    \STATE \( I \gets \emptyset \)
    \STATE \( -I \gets \{ 1, 2, \dotsc, n \} \)
    \STATE \( L \gets \vec{0}^{(n + 1) \times s} \)
    \STATE \(
      \Theta_{\text{Tr}, \text{Pr} \mid I} \gets
      K(\vec{x}_\text{Tr}, \vec{x}_\text{Pr})
    \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I}) \gets
      \diag(K(\vec{x}_\text{Tr}, \vec{x}_\text{Tr}))
    \)
    \WHILE{\( \lvert -I \rvert > 0 \) and \( \lvert I \rvert <s \)}
      \STATE \(
        k \gets \max_{j \in -I}
        \frac{\Theta_{j, \text{Pr} \mid I}}{\Theta_{jj \mid I}}
      \)
      \STATE \( I \gets I \cup \{ k \} \)
      \STATE \( -I \gets -I - \{ k \} \)
      \STATE \( i \gets \lvert I \rvert \)
      \STATE \(
        L_{:, i} \gets
        K(\vec{x}, \vec{x}_k) - L_{:, :i - 1} L_{k, :i - 1}^{\top}
      \)
      \STATE \( L_{:, i} \gets \frac{L{:, i}}{\sqrt{L_{k, i}}} \)
      \FOR{\( j \in -I \)}
        \STATE \(
          \Theta_{jj \mid I} \gets
          \Theta_{jj \mid I} -
          L_{j, i}^2
        \)
        \STATE \(
          \Theta_{j, \text{Pr} \mid I} \gets
          \Theta_{j, \text{Pr} \mid I} -
          L_{j, i} L_{n, i}
        \)
      \ENDFOR
    \ENDWHILE
    \RETURN \( I \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}

While both approaches have the same time complexity, the explicit precision
algorithm uses \( \mathcal{O}(s^2) \) space to store the precision while the
Cholesky factorization uses \( \mathcal{O}(N s) \) to store the first \( s \)
columns of the Cholesky factorization of \( \Theta \), which is always more
memory than the precision (\( N > s \)). Both algorithms use an additional \(
\mathcal{O}(N) \) space to store the conditional variances and covariances.

Once the indices have been computed according to \cref{alg:gp_select}
or \cref{alg:select_chol}, inferring the conditional mean and
covariance of the unknown data can be done directly according to
\cref{eq:cond_mean} and \cref{eq:cond_cov} in time \( \mathcal{O}(s^3)
\) using \cref{alg:infer_select}.

This algorithm is in fact the covariance equivalent of the
signal recovery algorithm orthogonal matching pursuit (OMP)
\cite{tropp2007signal}, a connection elaborated in \cref{app:omp}.

\begin{algorithm}
  \caption{Gaussian process inference by selection}
  \label{alg:infer_select}
  \begin{algorithmic}[1]
    \REQUIRE \(
      \vec{x}_\text{Tr}, \vec{y}_\text{Tr},
      \vec{x}_\text{Pr}, K(\cdot, \cdot), s
    \)
    \ENSURE \(
        \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}],
      \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]
    \)

    \STATE Compute \( I \) using \cref{alg:gp_select} or \cref{alg:select_chol}
    \STATE \(
      \Theta_{\text{Tr}, \text{Tr}} \gets
      K(\vec{x}_\text{Tr}[I], \vec{x}_\text{Tr}[I])
    \)
    \STATE \(
      \Theta_{\text{Pr}, \text{Pr}} \gets
      K(\vec{x}_\text{Pr}, \vec{x}_\text{Pr})
    \)
    \STATE \(
      \Theta_{\text{Tr}, \text{Pr}} \gets
      K(\vec{x}_\text{Tr}[I], \vec{x}_\text{Pr})
    \)
    \STATE \(
      \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] \gets
      \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
      \vec{y}_\text{Tr}[I]
    \)
    \STATE \(
      \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] \gets
      \Theta_{\text{Pr}, \text{Pr}} -
      \Theta_{\text{Tr}, \text{Pr}}^{\top} \Theta_{\text{Tr}, \text{Tr}}^{-1}
      \Theta_{\text{Tr}, \text{Pr}}
    \)
    \RETURN \(
        \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}],
      \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]
    \)
  \end{algorithmic}
\end{algorithm}

\subsection{Supernodes and blocked selection}

We now consider how to efficiently deal with multiple prediction points. The
first question is how to generalize the previous objective for a single point
\cref{eq:obj_gp} to multiple points. Following the same mutual information
justification as before, a natural criterion is to minimize the log determinant
of the prediction points' covariance matrix after conditioning on the
selected points, or \( \logdet(\Theta_{\text{Pr}, \text{Pr} \mid I}) \). This
objective, known as D-optimal \cite{krause2008nearoptimal}, has many intuitive
interpretations --- for example, as the volume of the region of uncertainty
or as the scaling factor in the density function for the Gaussian process.

We now need to be able to efficiently compute the effect of selecting an
index \( k \) on the log determinant. From \cref{eq:cond_select}, we know
that selecting an index is a rank-one update on the prediction covariance.
Using the matrix determinant lemma,
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet
  \left (
    \Theta_{\text{Pr}, \text{Pr} \mid I \cup \{ k \}}
  \right ) &= \logdet
    \left (
      \Theta_{\text{Pr}, \text{Pr} \mid I} -
      \frac{\Theta_{\text{Pr}, k \mid I}
            \Theta_{\text{Pr}, k \mid I}^{\top}
           }{\Theta_{kk \mid I}}
    \right ) \\
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      1 -
      \frac{\Theta_{\text{Pr}, k \mid I}^{\top}
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \shortintertext{Focusing on the second term, we can turn
    the quadratic form into conditioning:}
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      \frac{\Theta_{kk \mid I} -
            \Theta_{k, \text{Pr} \mid I}
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \label{eq:greedy_mult}
  \shortintertext{By the quotient rule, we combine the conditioning:}
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      \frac{\Theta_{kk \mid I, \text{Pr}}}{\Theta_{kk \mid I}}
    \right )
\end{align}
The greedy objective \cref{eq:greedy_mult} tells us that to minimize the
log determinant, we can simply select the index \( k \) with the smallest
ratio between the conditional variance after conditioning on the previously
selected points as well as the prediction points, and the conditional
variance after just conditioning on the selected points. Intuitively
this tells us that we can place sensors \emph{backwards}, where we imagine
placing sensors at the \emph{prediction points} instead of the candidates.
We then measure the conditional variance at a candidate, and pick the
candidate whose conditional variance decreases the most (relative from
what it started out as). Intuitively, these candidates are likely to give
information about the prediction points, because the prediction points
give information about the candidate.

Re-writing the objective in this way also gives an efficient algorithm to
compute the necessary quantities. We condition on the prediction points
essentially the same as described in \cref{subsec:greedy}, by simply
maintaining two structures instead of one, one for the conditional variance
after conditioning on the previously selected points, and the other for the
conditional variance after also conditioning on the prediction points. By
the quotient rule, the order of conditioning does not matter as long as the
order is consistent. For the second structure, we therefore condition on
the prediction points \emph{first} before any points have been selected.
We again have two strategies, one which explicitly maintains precisions and
the other which relies on Cholesky factorization.

For the precision algorithm, using \cref{eq:cond_cov} directly, for \(
m \) prediction points it costs \( \mathcal{O}(m^3) \) to compute \(
\Theta_{\text{Pr}, \text{Pr}}^{-1} \) and then \( \mathcal{O}(N m^2) \)
to compute \( \Theta_{kk \mid \text{Pr}} \) for the \( N \) candidates \(
k \). For each of the \( s \) rounds of selecting candidates, it costs
\( s^2 \) and \( m^2 \) to update the precisions \( \Theta_{I, I}^{-1}
\) and \( \Theta_{\text{Pr}, \text{Pr}}^{-1} \) respectively, where the
details of efficiently updating \( \Theta_{\text{Pr}, \text{Pr}}^{-1}
\) after the rank-one update in \cref{eq:obj_gp_mult} are given in
\cref{app:prec_cond}. Given the precisions, \( \vec{u} = \frac{\Theta_{:,
k \mid I}}{\sqrt{\Theta_{kk \mid I}}} \) and \( \vec{u}_\text{Pr} =
\frac{\Theta_{:, k \mid I, \text{Pr}}}{\sqrt{\Theta_{kk \mid I, \text{Pr}}}}
\) are computed as usual according to \cref{eq:cond_cov} in time \( Ns
\) and \( Nm \). Finally, for each candidate \( j \), the conditional
variance \( \Theta_{jj \mid I} \) is updated by subtracting \( u_j^2 \),
the conditional covariance \( \Theta_{\text{Pr}, k \mid I} \) is updated
for each prediction point index \( c \) each by subtracting \( u_j u_c
\), and the conditional variance \( \Theta_{jj \mid I, \text{Pr}} \) is
updated by subtracting \( {u_\text{Pr}}_j^2 \). The total time complexity
after simplification is \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two Cholesky factorizations are stored. We first
compute the Cholesky factorization after selecting each prediction point,
for a cost of \( (n + m) m \) for each of the \( m \) columns. We then begin
selecting candidates, which requires updating both Cholesky factors in time \(
(n + m)(m + s) \) which is dominated by updating the preconditioned Cholesky
factor. The columns of the Cholesky factors correspond precisely to \( \vec{u}
\) and \( \vec{u}_\text{Pr} \) and both conditional variances \( \Theta_{jj
\mid I} \) and \( \Theta_{jj \mid I, \text{Pr}} \) can be computed as above.
The conditional covariances do not need to be computed. Over \( s \) rounds
the total time complexity is \( \mathcal{O}((N + m) m^2 + s(N + m)(m + s)) \)
which simplifies to \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

Although both approaches have the same time complexity, like the single
point case they differ in memory usage. The explicit precision requires \(
\mathcal{O}(s^2 + m^2) \) memory to store both precisions, as well as \(
\mathcal{O}(N m) \) memory to store the conditional covariances. The Cholesky
algorithm, on the othe rhand, requires \( \mathcal{O}((n + m)(m + s)) \) to
store the first \( m + s \) columns of the Cholesky factorization of the joint
covariance matrix between training and prediction points, which simplifies
to \( \mathcal{O}(N s + N m + m^2) \). Comparing the memory usage, they are
the same except for \( s^2 \) versus \( N s \), so the Cholesky algorithm
again uses more memory than the explicit precision algorithm.

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Multiple prediction point selection by explicit precision}
  \label{alg:gp_select_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}_\text{Tr}, \vec{x}_\text{Pr}, K(\cdot, \cdot), s \)
    \ENSURE \( I \)

    \STATE \( n \gets \lvert \vec{x}_\text{Tr} \rvert \)
    \STATE \( m \gets \lvert \vec{x}_\text{Pr} \rvert \)
    \STATE \(
      \vec{x} \gets
      \begin{pmatrix}
        \vec{x}_\text{Tr} \\
        \vec{x}_\text{Pr}
      \end{pmatrix}
    \)
    \STATE \( I \gets \emptyset \)
    \STATE \( -I \gets \{ 1, 2, \dotsc, n \} \)
    \STATE \( \Theta_{I, I}^{-1} \gets \mathbb{R}^{0 \times 0} \)
    \STATE \(
      \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1} \gets
      K(\vec{x}_\text{Pr}, \vec{x}_\text{Pr})
    \)
    \STATE \(
      \Theta_{\text{Tr}, \text{Pr} \mid I} \gets
      K(\vec{x}_\text{Tr}, \vec{x}_\text{Pr})
    \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I}) \gets
      \diag(K(\vec{x}_\text{Tr}, \vec{x}_\text{Tr}))
    \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I, \text{Pr}}) \gets
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I})
    \) \\  \(
      - \diag(
        \Theta_{\text{Tr}, \text{Pr} \mid I}
        \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
        \Theta_{\text{Pr}, \text{Tr} \mid I}
      )
    \)
    \WHILE{\( \lvert -I \rvert > 0 \) and \( \lvert I \rvert <s \)}
      \STATE \(
        k \gets \min_{j \in -I}
        \frac{\Theta_{jj \mid I, \text{Pr}}}{\Theta_{jj \mid I}}
      \)
      \STATE \( I \gets I \cup \{ k \} \)
      \STATE \( -I \gets -I - \{ k \} \)
      \STATE \(
        \vec{v} \gets \Theta_{I, I}^{-1}
        K(\vec{x}_\text{Tr}[I - \{ k \}], \vec{x}_\text{Tr}[k])
      \)
      \STATE \(
        \Theta_{I, I}^{-1} \gets
        \begin{pmatrix}
          \Theta_{I, I}^{-1} +
          \frac{\vec{v} \vec{v}^{\top}}{\Theta_{kk \mid I}} &
          \frac{-\vec{v}}{\Theta_{kk \mid I}} \\
          \frac{-\vec{v}^{\top}}{\Theta_{kk \mid I}} &
          \frac{1}{\Theta_{kk \mid I}}
        \end{pmatrix}
      \)
      \STATE \(
        \vec{w} \gets \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
        \Theta_{k, \text{Pr} \mid I}^{\top}
      \)
      \STATE \(
        \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1} \gets
        \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1} +
        \frac{\vec{w} \vec{w}^{\top}}{\Theta_{kk \mid I, \text{Pr}}}
      \)
      \STATE \(
        \Theta_{:, k \mid I} \gets
          K(\vec{x}, \vec{x}_k) -
          K(\vec{x}, \vec{x}_{I - \{ k \}}) \vec{v}
      \)
      \STATE \(
          \Theta_{:, k \mid I, \text{Pr}} \gets
          \Theta_{:, k \mid I} -
          \Theta_{:, \text{Pr} \mid I} \vec{w}
      \)
      \STATE \(
        \vec{u} \gets
        \frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk \mid I}}}
      \)
      \STATE \(
        \vec{u}_\text{Pr} \gets
        \frac{\Theta_{:, k \mid I, \text{Pr}}}
             {\sqrt{\Theta_{kk \mid I, \text{Pr}}}}
      \)
      \FOR{\( j \in -I \)}
        \STATE \(
          \Theta_{jj \mid I} \gets
          \Theta_{jj \mid I} -
          \vec{u}_j^2
        \)
        \STATE \(
          \Theta_{jj \mid I, \text{Pr}} \gets
          \Theta_{jj \mid I, \text{Pr}} -
          (\vec{u}_\text{Pr})_j^2
        \)
        \FOR{\( c \in \{ 1, 2, \dotsc, m \} \)}
          \STATE \(
            \Theta_{j, \text{Pr}[c] \mid I} \gets
            \Theta_{j, \text{Pr}[c] \mid I} -
            \vec{u}_j \vec{u}_{n + c}
          \)
        \ENDFOR
      \ENDFOR
    \ENDWHILE
    \RETURN \( I \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Multiple prediction point selection by Cholesky factorization}
  \label{alg:select_mult_chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}_\text{Tr}, \vec{x}_\text{Pr}, K(\cdot, \cdot), s \)
    \ENSURE \( I \)

    \STATE \( n \gets \lvert \vec{x}_\text{Tr} \rvert \)
    \STATE \( m \gets \lvert \vec{x}_\text{Pr} \rvert \)
    \STATE \(
      \vec{x} \gets
      \begin{pmatrix}
        \vec{x}_\text{Tr} \\
        \vec{x}_\text{Pr}
      \end{pmatrix}
    \)
    \STATE \( I \gets \emptyset \)
    \STATE \( -I \gets \{ 1, 2, \dotsc, n \} \)
    \STATE \( L \gets \vec{0}^{(n + m) \times s} \)
    \STATE \( L_\text{Pr} \gets \vec{0}^{(n + m) \times (s + m)} \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I}) \gets
      \diag(K(\vec{x}_\text{Tr}, \vec{x}_\text{Tr}))
    \)
    \STATE \(
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I, \text{Pr}}) \gets
      \diag(\Theta_{\text{Tr}, \text{Tr} \mid I})
    \)
    \FOR{\( i \in \{ 1, 2, \dotsc, m \} \)}
      \STATE Update \( L_\text{Pr} \) and \( \diag(\Theta_{\text{Tr}, \text{Tr}
        \mid I, \text{Pr}}) \) with \( k = n + i \) by \cref{alg:chol_update}.
    \ENDFOR
    \WHILE{\( \lvert -I \rvert > 0 \) and \( \lvert I \rvert <s \)}
      \STATE \(
        k \gets \max_{j \in -I}
        \frac{\Theta_{j, \text{Pr} \mid I}}{\Theta_{jj \mid I}}
      \)
      \STATE \( I \gets I \cup \{ k \} \)
      \STATE \( -I \gets -I - \{ k \} \)
      \STATE \( i \gets \lvert I \rvert \)
      \STATE Update \( L \) and \( \diag(\Theta_{\text{Tr}, \text{Tr}
        \mid I}) \) by \\ \cref{alg:chol_update}.
      \STATE Update \( L_\text{Pr} \) and \( \diag(\Theta_{\text{Tr}, \text{Tr}
        \mid I, \text{Pr}}) \) with \( i = i + m \) by \cref{alg:chol_update}.
    \ENDWHILE
    \RETURN \( I \)
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Update Cholesky factor}
  \label{alg:chol_update}
  \begin{algorithmic}
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), i, k, L, \diag(\Theta) \)
    \ENSURE \( L_{:, i}, \diag(\Theta_{\mid k}) \)

    \STATE \( n \gets \lvert \diag(\Theta) \rvert \)
    \STATE \(
      L_{:, i} \gets
      K(\vec{x}, \vec{x}_k) - L_{:, :i - 1} L_{k, :i - 1}^{\top}
    \)
    \STATE \( L_{:, i} \gets \frac{L{:, i}}{\sqrt{L_{k, i}}} \)
    \FOR{\( j \in \{ 1, 2, \dotsc, n \} \)}
      \STATE \( \Theta_{jj} \gets \Theta_{jj} - L_{j, i}^2 \)
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\subsection{Near optimality by submodularity}

\section{Greedy selection for \emph{global} approximation by KL-minimization}

We have a covariance matrix \( \Theta \) and wish to compute the Cholesky
factorization of \( \Theta \) into a lower triangular factor \( L \) such that
\( \Theta = L L^{\top} \). \Stodo{justify importance/downstream applications
of Cholesky factorization}. This can be done in \( \mathcal{O}(N^3) \) with
standard algorithms, which is often prohibitive. Recall the problem of
inference in Gaussian process regression as described in \cref{subsec:gp_reg}
also took \( \mathcal{O}(N^3) \) to invert the covariance matrix \( \Theta
\). Thus, similar to Guassian process regression, we will use \emph{sparsity}
to mitigate the computational cost. In fact, we will be able to re-use our
previous algorithms \cref{alg:gp_select,alg:gp_select_mult} on each column
of the Cholesky factorization.

We will first compute the Cholesky factorization of \( \Theta^{-1} \),
also known as the \emph{precision matrix}, and use the resulting sparse
factorization to efficiently compute an approximation for \( \Theta
\). Because the precision matrix encodes the distribution of the full
conditionals, the \( (i, j) \)th entry of the precision matrix is 0 if and
only if the variables \( x_i \) and \( x_j \) are conditionally independent,
conditional on the rest of the variables. Thus, the precision matrix \(
\Theta^{-1} \) can be sparse as a result of conditional independence even
if the original covariance matrix \( \Theta \) is dense. It therefore
makes sense to attempt to approximately ``sparsify'' \( \Theta^{-1} \)
instead of \( \Theta \) with iterated conditioning.

Because of sparsity, we can only get an approximate Cholesky factor \( L \),
\( \hat{L} \) belonging to a pre-specified sparsity pattern \( S \) --- a set
of (row, column) indices that are allowed to be nonzero. In order to measure
the performance of the estimator, we treat the matrices as covariance matrices
of centered Gaussian processes (mean \( \vec{0} \)). In order to compare the
resulting distributions, we use the \emph{KL-divergence} according to
\cite{schafer2021sparse}, or the expected difference in log-densities:
\begin{align}
  \label{eq:L_obj}
  L \coloneqq \argmin_{\hat{L} \in S} \, \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})
  \right )
\end{align}

Note that here we are computing the Cholesky factorization
of \( \Theta^{-1} \). Surprisingly enough, it is possible to
exactly compute \( L \). First, we re-write the KL-divergence:
\begin{align}
  \label{eq:kl}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  = \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
\end{align}
where \( \Theta_1 \) and \( \Theta_2 \) are both of size
\( N \times N \). See \cref{app:kl_obj} for details.

\begin{theorem}
  \label{thm:L}
  \cite{schafer2021sparse}.
  The non-zero entries of the \( i \)th
  column of \( L \) in \cref{eq:L_obj} are:
  \begin{align}
    \label{eq:L_col}
    L_{s_i, i} = \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
  \end{align}
\end{theorem}

Plugging the optimal \( L \) \cref{eq:L_col} back
into the KL-divergence \cref{eq:kl}, we obtain:
\begin{align}
  \label{eq:obj_chol}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\Theta)
\end{align}

See \cref{app:kl_L} for details. In particular, it is important
which direction the KL-divergence is or else cancellation of
the \( \trace(\Theta_2^{-1} \Theta_1) \) term may not occur.

In order to maximize \cref{eq:obj_chol}, we can ignore \( \logdet(\Theta)
\) since it does not depend on \( L \) and maximize over each column
independently, since each term in the sum only depends on a single
column. We want to minimize \( (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1}
\vec{e}_1)^{-1} \), the term corresponding to the diagonal entry in the
inverse of the submatrix of \( \Theta \) corresponding to the entries we've
taken. We can give this value statistical interpretation by using the
fact that marginalization in covariance is conditioning in precision.
\begin{align}
  \label{eq:inverse_cond}
  \Theta_{1, 1 \mid 2} &=
    ((\Theta^{-1})_{1, 1})^{-1}
  \shortintertext{where \( \Theta \) is blocked according to}
  \label{eq:blocking}
  \Theta &=
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix}
  \shortintertext{Thus, we see that}
  \nonumber
  (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= ((\Theta_{s_i, s_i}^{-1})_{11})^{-1} \\
         &= \Theta_{ii \mid s_i - \{ i \}}
\end{align}

So our objective on each column is to minimize the conditional variance of
the \( i \)th variable, conditional on the entries we've selected --- \( s_i
\) contains \( i \) to begin with, so \( s_i - \{ i \} \) is the selected
entries. We can therefore use algorithm \cref{alg:gp_select} directly on each
column, where the prediction point is the \( i \) variable and the number
of points selected is the number of nonzero entries per column. The only
difference is that the candidates is limited to indices lower than \( i \),
that is, candidate indices \( k \) such that \( k > i \) to maintain the
lower triangularity of \( L \). Once \( s_i \) has been computed for each \(
i \), \( L \) can be constructed according to \cref{thm:L}. Each column costs
\( \mathcal{O}(s^3) \) to compute \( \Theta_{s_i, s_i}^{-1} \) for a total
cost of \( \mathcal{O}(N s^3) \) for the \( N \) columns of \( L \).

\subsection{Aggregated sparsity pattern}

We can also use the Gaussian process regression viewpoint to efficiently
aggregate multiple columns, that is, to use the same sparsity pattern for
multiple columns. We denote aggregating the column indices \( i_1, \dotsc, i_m
\) into the same group as \( \tilde{i} = \{i_1, i_2, \dots i_m \} \), letting
\( s_{\tilde{i}} = \bigcup_{i \in \tilde{i}} s_i \) be the aggregated sparsity
pattern, and letting \( \tilde{s} = s_{\tilde{i}} - \tilde{i} \) be the set
of selected entries excluding the diagonal entries. Each \( s_i = \tilde{s}
\cup \, \{ j \in \tilde{i} \mid j \geq i \} \), that is, the sparsity pattern
of the \( i \) column is the selected entries plus all the diagonal entries
lower than it. We will enforce that all the selected entries, excluding the
indices of the diagonals of the columns themselves, are below the lowest index
so that indices are not selected ``partially'' --- that is, an index could be
above some indices in the aggregated columns, and therefore invalid to add to
their column, but below others. That is, we restrict the candidate indices \(
k > \max \tilde{i} \) so that the selected index can be added to each column
in \( \tilde{i} \) without violating the lower triangularity of \( L \). It
is in fact possible to properly account for these partial updates, but the
reasoning and eventual algorithm becomes more complicated. We defer a detailed
discussion of the partial update case to \cref{app:partial}.

We now show that the KL-minimization objective on the aggregated indices
corresponds precisely to \cref{eq:obj_gp_mult}, the objective multiple
point Gaussian regression with the chain rule of log determinant through
conditioning.
\begin{align}
  \label{eq:det_chain}
  \logdet(\Theta) &= \logdet(\Theta_{1, 1 \mid 2}) + \logdet(\Theta_{2, 2})
  \shortintertext{where \( \Theta \) is blocked according to
    \cref{eq:blocking}. The KL-divergence objective for \( \tilde{i} \) is:}
  \nonumber
  \sum_{i \in \tilde{i}} \log(\Theta_{ii \mid s_i - \{ i \} })
  &= \log(\Theta_{i_m i_m \mid \tilde{s}}) +
     \log(\Theta_{i_{m - 1} i_{m - 1} \mid \tilde{s} \cup \{ i_m \}})
     + \dotsb \\
  \nonumber
  &= \logdet(\Theta_{\{ i_m, i_{m - 1} \}, \{ i_m, i_{m - 1} \}
             \mid \tilde{s}}) +
    \log(\Theta_{i_{m - 2} i_{m - 2} \mid \tilde{s} \cup \{ i_m, i_{m - 1} \}})
    + \dotsb \\
  \label{eq:obj_mult}
  &= \logdet(\Theta_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}

We see that the objective \cref{eq:obj_mult} is equivalent to the
objective \cref{eq:obj_gp_mult}, that is, to minimize the log
determinant of the conditional covariance matrix corresponding to a
set of prediction points, conditional on the selected entries. We can
therefore directly use \cref{alg:gp_select_mult} on the aggregated
columns, where the prediction points correspond to indices in the
aggregation and where we restrict the candidates \( k \) to those
below each column in the aggregation, \( k > \max \tilde{i} \).

Hence the sparse Cholesky factorization motivated by KL-divergence can be
viewed as sparse Gaussian process selection over each column, where entries are
selected to maximize mutual information with the entry on the diagonal of the
current column. In the aggregated case, the multiple columns in the aggregated
group correspond directly to predicting for multiple prediction points, where
entries are again selected to maximize mutual information with each diagonal
entry in the aggregation. This viewpoint leads directly to \cref{alg:chol}.

\begin{algorithm}
  \caption{Cholesky factorization by selection}
  \label{alg:chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s,
      g = \{ \tilde{i}_1, \dotsc, \tilde{i}_{N/m} \} \)
    \ENSURE \( L \) such that
      \( (L L^{\top})^{-1} \approx K(\vec{x}, \vec{x}) \)

    \STATE \( n \gets \lvert \vec{x} \rvert \)
    \FOR{ \( \tilde{i} \in g \)}
      \STATE \( J \gets
        \{ \max(\tilde{i}) + 1, \max(\tilde{i}) + 2, \dotsc, n \}
      \)
      \STATE Compute \( I \) using
        \cref{alg:gp_select_mult} or \cref{alg:select_mult_chol} \\ where
        \( \vec{x}_\text{Tr} = \vec{x}[J],
           \vec{x}_\text{Pr} = \vec{x}[\tilde{i}],
           s = s - \lvert \tilde{i} \rvert
        \)
      \STATE \( \tilde{s} \gets J[I] \)
      \FOR{\( i \in \text{reversed}(\text{sorted}(\tilde{i})) \)}
        \STATE \( \tilde{s} \gets \tilde{s} \cup \{ i \} \)
        \STATE \( s_i \gets \text{reversed}(\tilde{s}) \)
      \ENDFOR
    \ENDFOR
    \RETURN \( L \) computed with \cref{alg:L_mult}
  \end{algorithmic}
\end{algorithm}

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ without aggregation}
  \label{alg:L}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s_i \)
    \ENSURE \( L_{s_i, i} \)

    \STATE \( \Theta_{s_i, s_i}^{-1} \gets
      K(\vec{x}[s_i], \vec{x}[s_i])^{-1}
    \)
    \STATE \( L_{s_i, i} \gets \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
      {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \)
    \RETURN \( L_{s_i, i} \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ with aggregation}
  \label{alg:L_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), \tilde{s}, \tilde{i} \)
    \ENSURE \( L_{s_i, i} \) for all \( i \in \tilde{i} \)

    \STATE \( s \gets \tilde{i} \cup \tilde{s} \)
    \STATE \( U \gets P^{\updownarrow}
      \chol(P^{\updownarrow} \Theta_{s, s} P^{\updownarrow}) P^{\updownarrow}
    \)
    \FOR{\( i \in \tilde{i} \)}
      \STATE \( k \gets \) index of \( i \) in \( \tilde{i} \)
      \STATE \( L_{s_i, i} \gets U^{-\top} \vec{e}_k \)
    \ENDFOR
    \RETURN L
  \end{algorithmic}
\end{algorithm}
\end{minipage}

Once the sparsity pattern has been determined, we need to compute each column
of \( L \) according to \cref{thm:L}. Because the sparsity pattern for each
column in the same group are subsets of each other, we can efficiently compute
all their columns at once. The observation is that the smallest index in the
group (corresponding to the entry highest in the matrix) will have the largest
sparsity pattern, the next index will have one less entry (lacking the entry
above it, which would violate lower triangularity), and so on. We need to
compute \( \Theta_{s_i, s_i}^{-1} \vec{e}_1 \) for each \( i \in \tilde{i} \),
or the precision of the marginalized covariance corresponding to the selected
entries. By \cref{eq:inverse_cond}, we can turn marginalization in covariance
into conditioning in precision:
\begin{align}
  \nonumber
  \label{eq:L_precision}
  L_{s_i, i} &= \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}} \\
             &= \frac{(\Theta_{s, s})^{-1}_{k:, k: \mid :k - 1} \vec{e}_1}
             {\sqrt{\vec{e}_1^{\top} (\Theta_{s, s})^{-1}_{k:, k: \mid :k - 1}
                    \vec{e}_1}}
\end{align}
where \( s = \tilde{i} \cup \tilde{s} \) and \( k \) is \( i \)'s
index in \( \tilde{i} \). So we want the \( k \)th column of
the precision of the marginalized covariance, conditional on all the
entries before it. From \cref{eq:chol}, this can be directly read
off the Cholesky factorization. Thus, we can simply compute:
\begin{align}
  \label{eq:L_chol}
  L &= \chol \left ( \Theta_{s, s}^{-1} \right )
\end{align}
and read off the \( k \)th column to compute \cref{eq:L_precision} for each
\( i \in \tilde{i} \). However, instead of computing a lower triangular
factor for the precision, we can compute an \emph{upper} triangular factor
the covariance whose inverse transpose will be a \emph{lower} triangular
factor for the original matrix. In particular, we see that
\begin{align}
  \label{eq:U_chol}
  U &= P^{\updownarrow} \chol(P^{\updownarrow} \Theta_{s, s} P^{\updownarrow})
       P^{\updownarrow}
  \shortintertext{satisfies \( U U^{\top} = \Theta_{s, s} \) where
  \( P^{\updownarrow} \) is the order-reversing permutation. Thus,}
  \nonumber
  \Theta_{s, s}^{-1} &= U^{-\top} U^{-1}
\end{align}
where \( U^{-\top} \) is an \emph{lower} triangular factor for \( \Theta_{s,
s}^{-1} \) equal to \cref{eq:L_col} because the Cholesky factorization is
unique. Computing \( U^{-\top} \) leads directly to \cref{alg:L_mult}.

Recall that the complexity of selecting \( s \) out of \( N \) total training
points for \( m \) prediction points using \cref{alg:gp_select_mult} or
\cref{alg:select_mult_chol} was \( \mathcal{O}(N s^2 + N m^2 + m^3) \). In the
context of Cholesky factorization, \( N \) is the size of the matrix, \( m \)
is the number of columns to aggregate, and \( s \) is the number of nonzero
entries in each column of \( L \). We therefore need to do \( \frac{N}{m} \)
selections, one for each aggregated group, where we only need to select \( s
- m \) entries (since the \( m \) prediction points are automatically added).
We then need to actually construct each column of \( L \) after determining
the sparsity pattern, with \cref{alg:L_mult}. This costs \( \mathcal{O}(s^3)
\) for each aggregated group to compute the Cholesky factor of the submatrix,
which dominates the time to compute each column of \( L \) for the \( m \)
columns in the group, \( \mathcal{O}(m s^2) \) (\( N > s > m \)). Thus, the
overall complexity is \( \mathcal{O}(\frac{N}{m} (N (s - m)^2 + N m^2 + m^3 +
s^3)) \), which simplifies to \( \mathcal{O}(\frac{N^2 s^2}{m}) \) by making
use of the bound that \( (s - m)^2 = \mathcal{O}(s^2 + m^2) \).

Note that the non-aggregated factorization is equivalent to \( m = 1 \),
which yields \( \mathcal{O}(N^2 s^2) \) (using the non-aggregated algorithms
\cref{alg:gp_select,alg:L}, but one can also use the aggregated versions
\cref{alg:gp_select_mult,alg:L_mult} with \( m = 1 \) and achieve equivalent
complexity). Thus, we see that the aggregated version is \( m \) times faster
than its non-aggregated counterpart, at the cost that the resulting sparsity
pattern will be lower quality (since the algorithm is forced to select the
same entry for \emph{all} columns in the group).

Unlike the geometric algorithms of \cite{schafer2021sparse,
schafer2020compression} which rely on the pairwise distance between points,
and whose covariance matrix is implicitly determined by a list of points and
kernel function, this algorithm relies only on the entries of the covariance
matrix \( \Theta \). Thus, it can factor arbitrary symmetric positive definite
matrices without access to points or an explicit kernel function.

\subsection{Review of KL approximation}

\section{Numerical experiments}

All experiments were run on the PACE Phoenix cluster, with 8
cores of a Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz and 6 GB of
RAM per core. Python code for all numerical experiments can be
found at \href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\Stodo{cite python libraries that want citations (e.g. scikit-learn)}

\subsection{\textit{k}th-nearest neighbors selection}

We justify that diverse point selection based on conditional information can
lead to better performance than simply selecting the nearest neighbor in a toy
example on the MNIST dataset. We compare \( k \)th-nearest neighbors (KNN)
directly to conditional \( k \)th-nearest neighbors (CKNN) in the following
experiment. We randomly select 1000 images to form the training set and 100
to form the testing set. For each image in the testing set, we select the
\( k \) ``closest'' training points with either KNN or CKNN. For KNN we use
the standard Euclidean distance and for CKNN we use Mat{\'e}rn kernel with
smoothness \( \nu = 1.5 \) and length scale \( l = 2^{10} \). Finally, we
predict the label of the test point by taking the most frequently occurring
label in the \( k \) selected points.

\Stodo{cite mnist dataset}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/graphs/mnist/k_acc.png}
  \caption{Accuracy of classification with increasing \( k \).}
\end{figure}

\Stodo{replace images with pgfplots/tikz}

As \( k \) increases, KNN degrades near-linearly in accuracy. We hypothesize
that nearby images are more likely to have the same label as a given test
image. By forcing the algorithm to select more points, it increases the
likelihood that the algorithm becomes confused by differently labeled
images. However, CKNN is more accurate than KNN for nearly every \( k
\), suggesting that conditional selection is able to take advantage of
selecting more points. We emphasize that the difference in accuracy is
solely a result of conditional selection --- because the Mat{\'e}rn kernel
degrades monotonically with distance, sorting by covariance is identical
to sorting by distance. In addition, we use the mode to aggregate the
labels of the selected points, rather than performing Gaussian process
classification. The difference in accuracy can therefore be attributed
to precisely the difference in which points were selected.

\subsection{Gaussian process regression}

\subsection{Recovery of sparse Cholesky factors}

\subsection{Cholesky factorization}

\subsection{Preconditioning for conjugate gradient}

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{cholesky}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Computation in sparse Gaussian process selection}

\subsection{Updating precision after insertion}
\label{app:prec_insert}

We have the matrix \( \Theta_{I, I}^{-1} \) corresponding to the precision of
the selected entries, and wish to take into account the addition of a new entry
\( k \) into \( I \). That is, we wish to compute \( \Theta_{I', I'}^{-1} \)
for \( I' = I \cup \{ k \} \), which in effect adds a new row and column to \(
\Theta_{I, I}^{-1} \). In order to invert the new matrix efficiently, we can
block the matrix to separate the new and old information.
\begin{align}
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    I & \textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \shortintertext{For notational convenience, we denote the Schur
    complement \textcolor{lightblue}{\( \Theta_{2, 2} - \Theta_{2, 1}
    \Theta_{1, 1}^{-1} \Theta_{1, 2} \)} as \textcolor{lightblue}{\(
    \Theta_{2, 2 \mid 1} \)}. Inverting both sides of the equation,}
  \Theta^{-1} &=
  \begin{pmatrix}
    I & -\textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    I & 0 \\
    -\textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \Theta_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    } \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    } \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry to the matrix,
    \( \Theta_{1, 1} = \Theta_{I, I} \), \( \Theta_{1, 2} = \Theta_{I,
    k} \), and \( \Theta_{2, 2} = \Theta_{kk} \). Also note that \(
    \textcolor{lightblue}{\Theta_{kk \mid I}^{-1}} \) is the inverse
    of the variance of \( k \) conditional on the entries in \( I \),
    which has already been computed in \cref{alg:gp_select}. If we let
    \( \vec{v} = \textcolor{darkorange}{\Theta_{I, I}^{-1} \Theta_{I,
    k}} \), then we can write the update as:}
  &=
  \begin{pmatrix}
    \Theta_{I, I}^{-1} + \Theta_{kk \mid I}^{-1} \vec{v} \vec{v}^T &
    -\Theta_{kk \mid I}^{-1} \vec{v} \\
    -\Theta_{kk \mid I}^{-1} \vec{v}^{\top} & \Theta_{kk \mid I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:gp_select}. Note that
the update is a rank-one update to \( \Theta_{1, 1}^{-1} \), which can
be computed in \( \mathcal{O}(\lvert I \rvert^2) = \mathcal{O}(s^2) \).

\subsection{Updating precision after marginalization}
\label{app:prec_delete}

Suppose we have the precision \( \Theta^{-1} \) and wish to compute the
precision of the marginalized covariance after ignoring an index \( k \).
That is, we wish to compute the inverse of a matrix after deleting a row and
column, given the inverse of the original matrix. We could use the result
in \cref{app:prec_insert} by ``reading'' the update backwards. That is, we
could identify \( \Theta_{2, 2 \mid 1}^{-1} \) from \( (\Theta^{-1})_{kk}
\) and \( \vec{v} = \Theta_{1, 1}^{-1} \Theta_{1, 2} \) from \( -
\frac{(\Theta^{-1})_{-k, k}}{\Theta_{2, 2 \mid 1}^{-1}} \) where \( -k \)
denotes all rows excluding the \( k \)th row. We can then revert the rank-one
update by subtracting out the update, computing \( \Theta_{-k, -k}^{-1} =
(\Theta^{-1})_{-k, -k} - \Theta_{kk \mid I}^{-1} \vec{v} \vec{v}^{\top} \).
However, a more intuitive derivation relies on the fact that marginalization
in covariance is conditioning in precision. Using \cref{eq:inverse_cond},
we see that \( \Theta_{-k, -k}^{-1} = (\Theta^{-1})_{-k, -k \mid k} \), or
the precision conditional on the deleted entry. By \cref{eq:cond_cov}, we
immediately obtain the equivalent update
\begin{align}
  (\Theta^{-1})_{-k, -k \mid k} &= \Theta^{-1}_{-k, -k} -
    \frac{(\Theta^{-1})_{-k, k}
          (\Theta^{-1})_{-k, k}^{\top}}{(\Theta^{-1})_{kk}}
\end{align}
Since this is a rank-one update to the precision \( \Theta^{-1} \), this
can be computed in \\ \( \mathcal{O}(\text{\# rows}(\Theta^{-1}))^2 \).

\Stodo{this is not used in the paper but is nice to know +
used in the sensor placement}

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

We have the matrix \( \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1} \), or
the precision of the prediction points, conditional on the selected
entries. We want to take into account selecting an entry \( k \), or to compute
\( \Theta_{\text{Pr}, \text{Pr} \mid I \cup \{ k \}}^{-1} \) which is a rank-one
update to the original matrix from \cref{eq:obj_gp_mult}.
We can directly apply
the Sherman–Morrison–Woodbury formula which states that:
\begin{align}
  \Theta_{1, 1 \mid 2}^{-1} &= \Theta_{1, 1}^{-1} +
    \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    \Theta_{2, 2 \mid 1}^{-1}
    \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning by definition,}
  \left (
    \Theta_{1, 1} - \Theta_{1, 2} \Theta_{2, 2}^{-1} \Theta_{2, 1}
  \right )^{-1} &=
    \Theta_{1, 1}^{-1} +
    \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    \Theta_{2, 2 \mid 1}^{-1}
    \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
  \shortintertext{Letting \( \vec{u} = \Theta_{1, 2} \) and \( \vec{v}
    = \Theta_{1, 1}^{-1} \Theta_{1, 2} = \Theta_{1, 1}^{-1} \vec{u} \),}
  (\Theta_{1, 1} - \Theta_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \Theta_{1, 1}^{-1} + \Theta_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{So we see that a rank-one update to \( \Theta_{1, 1} \)
    then inverting is a rank-one update to \( \Theta_{1, 1}^{-1} \). In our
    context, \( \Theta_{1, 1} = \Theta_{\text{Pr}, \text{Pr} \mid I}, \vec{u} =
    \Theta_{\text{Pr}, k \mid I}, \Theta_{2, 2} = \Theta_{kk \mid I} \) so \(
    \Theta_{2, 2 \mid 1}^{-1} = \Theta_{kk \mid \text{Pr}, I}^{-1} \) (this can
    be rigorously shown by expanding the Schur complement and taking advantage
    of the quotient rule as in \cref{eq:greedy_mult}). \( \vec{v} \) can be
    computed according to definition as \( \Theta_{\text{Pr}, \text{Pr} \mid
    I}^{-1} \vec{u} \). Thus, we can write the update as}
  \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} -
    \frac{\Theta_{\text{Pr}, k \mid I} \Theta_{\text{Pr}, k \mid I}^{\top}}
    {\Theta_{kk \mid I}}
  \right )^{-1} &=
    \Theta_{1, 1}^{-1} +
    \Theta_{kk \mid \text{Pr}, I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:gp_select_mult}.
Since the update is a rank-one update, it can be computed in \(
\mathcal{O}(\lvert \text{Pr} \rvert^2) = \mathcal{O}(m^2) \).

\subsection{Updating Cholesky Factorization after Rank-one Downdate}
\label{app:chol_downdate}

\Stodo{remove because unnecessary, describe insertion insertead}

We use the approach from Lemma 1 of \cite{krause2015more}, slightly adapted
to use in-place operations and to make no assumption on the particular row
ordering of the Cholesky factor. Let \( L \) be a Cholesky factorization of \(
\Theta \), that is, \( L = \chol(\Theta) \). We wish to compute the updated
Cholesky factor \( L' = \chol(\Theta') \) where \( \Theta' = \Theta - \vec{u}
\vec{u}^{\top} \). To do so, assume \( L \) and \( L' \) are blocked according
to the same block structure:
\begin{align}
  L &=
  \begin{pmatrix}
    r_1 & \vec{0} \\
    \vec{r}_2 & L_2
  \end{pmatrix}
  & L' &=
  \begin{pmatrix}
    r_1' & \vec{0} \\
    \vec{r}_2' & L_2'
  \end{pmatrix}
  \shortintertext{Multiplying, we find}
  L L^{\top} = \Theta &=
  \begin{pmatrix}
    r_1^2 & r_1 \vec{r}_2^{\top} \\
    r_1 \vec{r}_2 & L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
  \end{pmatrix}
  \\ L' L'^{\top} = \Theta' &=
  \begin{pmatrix}
    r_1'^2 & r_1' \vec{r}_2'^{\top} \\
    r_1' \vec{r}_2' & L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top}
  \end{pmatrix}
  \shortintertext{From here, we solve for
    \( r'_1 \), \( \vec{r}' \), and \( L_2' \)}
  r_1'^2 &= \Theta'_{11} = \Theta_{11} - u_1^2 \\
         &= r_1^2 - u_1^2 \\
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  r_1' \vec{r}_2' &= \Theta'_{2:, 1} = \Theta_{2:, 1} - u_1 \vec{u}_2 \\
                  &= r_1 \vec{r}_2 - u_1 \vec{u}_2 \\
  \vec{r}_2' &= \frac{1}{r_1'} (r_1 \vec{r}_2 - u_1 \vec{u}_2) \\
  L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top} &= \Theta'_{22}
    = \Theta_{22} - \vec{u}_2 \vec{u}_2^{\top} \\
  L_2' L_2'^{\top} &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \vec{r}_2' \vec{r}_2'^{\top}
  \shortintertext{After some simplification we find}
  L_2' L_2'^{\top} &= L_2 L_2^{\top} - \left (
    \frac{u_1}{r_1'} \vec{r}_2 - \frac{r_1}{r_1'} \vec{u}_2
  \right ) \left (
    \frac{u_1}{r_1'} \vec{r}_2 - \frac{r_1}{r_1'} \vec{u}_2
  \right )^{\top}
  \shortintertext{which is a rank-one downdate to the subfactor \( L_2 \).
    Recursively updating \( L_2 \) yields a \( \mathcal{O}(N^2) \) algorithm.
    We now re-write the algorithm to be in-place to take advantage of BLAS
    routines. The updates can be summarized as:}
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  \vec{r}' &= \frac{r_1}{r_1'} \vec{r} - \frac{u_1}{r_1'} \vec{u} \\
  \vec{u}' &= \frac{u_1}{r_1'} \vec{r} - \frac{r_1}{r_1'} \vec{u}
  \shortintertext{Note that we drop the subscripting on \( \vec{r} \) and \(
    \vec{u} \). By updating the entire vector on each iteration, we can avoid
    keeping track of the lower triangular structure of \( L \). We will first
    update \( \vec{r}' \) and then use it to update \( \vec{u} \). Solving for
    \( \vec{r} \) in terms of \( \vec{r}' \),}
  \vec{r} &= \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \\
  \vec{u}' &= -\frac{u_1}{r_1} \vec{r}' + \frac{r_1'}{u_1} \vec{u}
  \shortintertext{Thus, the updates proceed sequentally as follows:}
  \gamma &\gets \sqrt{r_1^2 - u_1^2} \\
  \alpha &\gets \frac{r_1}{\gamma} \\
  \beta &\gets \frac{u_1}{\gamma} \\
  \vec{r} &\gets \alpha \vec{r} - \beta \vec{u} \\
  \vec{u} &\gets -\frac{\beta}{\alpha} \vec{r} + \frac{1}{\alpha} \vec{u}
\end{align}
These can be efficiently performed in-place by
BLAS as level-one \texttt{daxpy} operations.

\subsection{Partial Updates in the Selection Algorithm}
\label{app:partial}

In the context of the selection algorithm, we have \( M \) prediction points
and wish to minimize the log determinant of the resulting covariance matrix
of the prediction points, conditional on the points we've selected from the
training data. In the specific context of Cholesky factorization, it is
possible to add a training point and have it apply \emph{partially} on the
prediction points. If nonadjacent columns indices are aggregated, a entry
selected between two indices can be higher than one column, but lower than
another. Adding the entry to the sparsity pattern would therefore only add
to some, but not all, columns in the aggregation. We will model this as
partially conditioning the variables of interest. In particular, if we have
prediction variables \( y_1, y_2, \dotsc, y_M \), a partial condition ignoring
the first \( j \) variables on the selected index \( k \) would result in
\( y_1 , y_2, \dotsc, y_{j}, y_{j + 1 \mid k}, \dotsc, y_{M \mid k} \).

The first question is to compute the resulting covariance matrix. We know \(
\vec{y} \sim \mathcal{N}(\vec{0}, \Theta) \) and \( \vec{y}_{\mid k} \) has
conditional distribution according to \cref{eq:cond_cov}, \( \vec{y}_{\mid k}
\sim \mathcal{N}(\mu, \Theta - \Theta_{:, k} \Theta_{k, k}^{-1} \Theta_{k,
:}) \). Taking the Cholesky factorization of both covariance matrices, let \(
L = \chol(\Theta) \) and \( L_{\mid k} = \chol(\Theta_{\mid k}) \). We can
then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z} \) is distributed
according to \( \mathcal{N}(\vec{0}, I) \). Similarly, \( \vec{y}_{\mid k} =
L_{\mid k} \vec{z} + \vec{\mu} \). For unconditioned \( y_i \) and \( y_j \), the
covariance between them is defined to be \( \Theta_{ij} \). Similarly, for
conditioned \( y_i \) and \( y_j \), the covariance is \( \Theta_{ij \mid k}
\). The only question is what the covariance between unconditioned \( y_i \)
and conditioned \( y_j \) is. By definition,
\begin{align}
  \Cov[y_i, y_j] &= \E[(y_i - \E[y_i])(y_j - \E[y_j])] \\
                 &= \E[(L_i \vec{z}) (L_{i \mid k} \vec{z})] \\
                 &= \E[(L_{1, i} z_1 + \dotsb + L_{N, i} z_N)
                       (L_{1, j \mid k} z_1 + \dotsb + L_{N, j \mid k} z_N)]
  \shortintertext{For \( i \neq j \), \( E[z_i z_j] = \E[z_i] \E[z_j] = 0\)
    since \( z_i \) is independent of \( z_j \) and has mean 0.}
                 &= \E[L_{1, i} L_{1, j \mid k} z_1^2 + \dotsb +
                       L_{N, i} L_{N, j \mid k} z_N^2] \\
                 &= L_{1, i} L_{1, j \mid k}  \E[z_1^2] + \dotsb +
                    L_{N, i} L_{N, j \mid k} \E[z_N^2]
  \shortintertext{For any \( i \), \( \E[z_i^2]
    = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \)}
                 &= L_{1, i} L_{1, j \mid k} + \dotsb +
                    L_{N, i} L_{N, j \mid k} \\
                 &= L_i \cdot L_{j \mid k}
  \shortintertext{Thus, the new covariance matrix can be written as:}
  \label{eq:chol_partial}
  \begin{pmatrix}
    L_{:j} L_{:j}^{\top} & L_{:j} L_{j: \mid k}^{\top} \\
    L_{j: \mid k} L_{:j}^{\top} & L_{j: \mid k} L_{j: \mid k}^{\top}
  \end{pmatrix} &=
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}^{\top}
  \shortintertext{We will denote a partially conditioned matrix as}
  \Theta_{:, :, \shortmid k}
\end{align}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[scale=4]
    \input{figures/cholesky_factor.tex}
  \end{tikzpicture}
  \caption{Illustration of the Cholesky
    factorization of a partially conditioned matrix.}
\end{figure}

We can now connect minimization of the log determinant of the partially updated
covariance matrix to the KL-divergence objective of Cholesky factorization.
Computing the log determinant of the partially updated covariance
matrix, we make use of \cref{eq:chol_partial} and make use of the fact
that the determinant of a  triangular matrix is the product of its
diagonal entries:
\begin{align}
  \label{eq:partial_logdet}
  \frac{1}{2} \logdet(\Theta_{:, : \shortmid k}) &=
  \underbrace{\log(L_{11}) + \dotsb + \log(L_{jj})}_{\text{the same}} +
  \underbrace{
    \log(L_{j + 1, j + 1 \mid k}) + \dotsb + \log(L_{M, M \mid k})
  }_{\text{conditioned}}
  \shortintertext{Comparing to the KL-divergence \cref{eq:obj_chol},
    \( \mathbb{D}_{\text{KL}}
      \left (
        \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
        \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
      \right )
    \) which is equivalent to maximizing}
  &= \sum_{i = 1}^M
      \log \left (
        \Theta_{ii \mid s_i - \{ i \}}
      \right )
  \shortintertext{Recalling that \( k \) is added partially to some \( s_i \),
    only those \( i > j \)}
  &= \underbrace{
      \log \left ( \Theta_{11 \mid s_1 - \{ 1 \}} \right ) + \dotsb +
      \log \left ( \Theta_{jj \mid s_j - \{ j \}} \right )
     }_\text{the same} + \\
  \nonumber
  &  \underbrace{
      \log \left ( \Theta_{j + 1, j + 1 \mid s_{j + 1} - \{ j + 1 \}} \right )
      + \dotsb + \log \left ( \Theta_{MM \mid s_M - \{ M \}} \right )
     }_\text{conditioned}
  \shortintertext{Since \( L_{ii} \) is the square root
    of the variance of the \( i \)th variable conditional
    on each entry before it in the ordering, we have}
  2 \log(L_{ii}) &= \log(\Theta_{ii \mid s_i - \{ i \}})
\end{align}
So minimizing the log determinant of the partially
conditioned covariance matrix \cref{eq:partial_logdet} is
the same as minimizing the KL-divergence \cref{eq:obj_chol}.

\subsection{Algorithm for Partial Updates}
\label{app:partial_alg}

We now need an efficient algorithm to keep track of partial updates.
The key idea is to maintain the prediction matrix with selected
points inserted to maintain proper ordering, and keep track of
the log determinant throughout selection. We first give how this
different perspective affects the interpretation of the multiple
point selection algorithm. In the example, let \( x \) and \( y \)
be selected points and \( 1 \) and \( 2 \) be prediction points.
\begin{align}
  \Theta &=
  \begin{pmatrix}
    \Theta_{xx} & \Theta_{xy} & \Theta_{x1} & \Theta_{x2} \\
    \Theta_{yx} & \Theta_{yy} & \Theta_{y1} & \Theta_{y2} \\
    \Theta_{1x} & \Theta_{1y} & \Theta_{11} & \Theta_{12} \\
    \Theta_{2x} & \Theta_{2y} & \Theta_{21} & \Theta_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\Theta) &= \log(\Theta_{xx}) + \log(\Theta_{yy \mid x}) +
    \log(\Theta_{11 \mid x, y}) + \log(\Theta_{22 \mid x, y, 1})
  \shortintertext{Isolating the objective --- the variances of the
    prediction points}
  \log(\Theta_{11 \mid x, y}) + \log(\Theta_{22 \mid x, y, 1}) &=
    \logdet(\Theta) - \log(\Theta_{xx}) - \log(\Theta_{yy \mid x})
  \shortintertext{Now consider how inserting \( y \) changed the objective
    from when it was just \( x \).}
  \log(\Theta_{11 \mid x}) + \log(\Theta_{22 \mid x, 1}) &=
    \logdet(\Theta_{-y, -y}) - \log(\Theta_{xx}) \\
  \Delta &=
    \logdet(\Theta) - \log(\Theta_{yy \mid x}) - \logdet(\Theta_{-y, -y})
  \shortintertext{But from \cref{eq:greedy_mult} we know}
  \Delta &= \log \left (
      \frac{\Theta_{yy \mid x, 1, 2}}{\Theta_{yy \mid x}}
    \right )
  \shortintertext{Substituting,}
  \log(\Theta_{yy \mid x, 1, 2}) - \log(\Theta_{yy \mid x}) &=
    \logdet(\Theta) - \log(\Theta_{yy \mid x}) - \logdet(\Theta_{-y, -y}) \\
  \log(\Theta_{yy \mid x, 1, 2}) &=
    \logdet(\Theta) - \logdet(\Theta_{-y, -y})
  \shortintertext{In general,}
  \label{eq:logdet_diff}
  \log(\Theta_{kk \mid I, \text{Pr}}) &=
    \logdet(\Theta) - \logdet(\Theta_{-k, -k})
\end{align}
Another way to arrive at the same result is to note that if we inserted
\( y \) at the \emph{end} of \( \Theta_{-y, -y} \), to compute the log
determinant of the new, bigger matrix \( \Theta \) we would add the
variance of \( y \) conditional on every entry in the matrix to the
old determinant by chain rule. Since the determinant is invariant to
symmetric permutation, the matrix inserting \( y \) at the end has the
same determinant as inserting \( y \) where it should be.

So we see that the conditional variance of a candidate point conditional
on everything else in the matrix is the difference in log determinant
between the matrix with the candidate inserted and the original matrix.
The multiple prediction point algorithm can therefore be interpreted as
we insert the candidate \emph{after} all the previously selected points
(so it is conditional on all the previous points) and \emph{before}
the prediction points (which conditions all of them). We then compute
\( \log(\Theta_{kk \mid I, \text{Pr}}) \) for some candidate \( k \)
which represents the difference in log determinant and then subtract \(
\log(\Theta_{kk \mid I}) \) which is the spurious variance introduced by
inserting \( k \) into the matrix. We do not need to subtract the spurious
variances from the previously selected points because \( k \) does not
affect them, and we select candidates by \emph{relative} score.

We now apply this result to partial selection. In the example,
let \( 1 \) and \( 2 \) be prediction points while \( x \) and \(
y \) are both a selected points below \( 2 \) but above \( 1 \), where
\( x \) has already been selected and \( y \) is a candidate.
\begin{align}
  \Theta &=
  \begin{pmatrix}
    \Theta_{11} & \Theta_{1y} & \Theta_{1x} & \Theta_{12} \\
    \Theta_{y1} & \Theta_{yy} & \Theta_{yx} & \Theta_{y2} \\
    \Theta_{x1} & \Theta_{xy} & \Theta_{xx} & \Theta_{x2} \\
    \Theta_{21} & \Theta_{2y} & \Theta_{2x} & \Theta_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\Theta) &= \log(\Theta_{11}) + \log(\Theta_{yy  \mid 1}) +
    \log(\Theta_{xx \mid 1, y}) + \log(\Theta_{22 \mid 1, y, x})
  \shortintertext{We see that \( y \) conditions \( 2 \) but not \( 1 \),
    precisely what we want to encode. However, we have introduced a spurious
    term \( \log(\Theta_{yy \mid 1}) \) and changed the variance of \( x \),
    both of which must be subtracted out.}
  \log(\Theta_{11}) + \log(\Theta_{22 \mid 1, y, x}) &=
  \logdet(\Theta) - \log(\Theta_{yy \mid 1}) - \log(\Theta_{xx \mid 1, y})
  \shortintertext{We can substitute \( \log(\Theta_{yy \mid 1, x, 2}) \)
    for \( \logdet(\Theta) \) by \cref{eq:logdet_diff}. Athough
    it differs by a constant, this does not change the objective.}
  &= \log(\Theta_{yy \mid 1, x, 2}) -
    \log(\Theta_{yy \mid 1}) - \log(\Theta_{xx \mid 1, y})
\end{align}

As long as we can compute conditional variances of our candidate on each
\emph{prefix} of the current ordering of prediction points interspersed with
selected points, we can use the conditional variances to compute the updated
conditional variances of the selected points by using their conditional
covariances with the candidate. We are then able to compute every term in the
objective. To do so, we maintain a partial Cholesky factor whose ordering
is given by the current ordering. When we select a new point, we insert it
in its appropriate place in the Cholesky factor. To update the Cholesky
factor after an insertion efficiently, we left-look to get the column of
its insertion position, and then update all columns right of the column by
a rank-one downdate as described in \cref{app:chol_downdate} which touches
every entry in the Cholesky factor, \( \mathcal{O}(N(m + s) \) per update for
a total cost of \( \mathcal{O}(N(m + s)(s)) \) over \( s \) selections.

By inspecting the Cholesky factor, we get the covariance of a selected point
with a candidate, conditional on all the points prior to the selected point in
the ordering. The conditional variance of the selected point is the diagonal
entry. We can then compute the new conditional variance given the variance of
the candidate, conditional on all points prior to the selected point. Suppose
we are at index \( i \) and the candidate is index \( j \), the updates are as
follows:
\begin{align}
  \Theta_{ii \mid :i - 1} &= (L_{ii})^2 \\
  \Theta_{ij \mid :i - 1} &= L_{ij} \cdot L_{ii} \\
  \Theta_{ii \mid :i - 1, j} &= \Theta_{ii \mid :i - 1} -
    \frac{\Theta_{ij \mid :i - 1}^2}{\Theta_{jj \mid :i - 1}} \\
  \Theta_{jj \mid :i - 1, i} &= \Theta_{jj \mid :i - 1} -
    \frac{\Theta_{ij \mid :i - 1}^2}{\Theta_{ii \mid :i - 1}} \\
                             &= \Theta_{jj \mid :i}
\end{align}
Of course, the base case \( \Theta_{jj} \) is simply \(
K(\vec{x}_j, \vec{x}_j) \), the variance of the \( j \)th point.

For each of the \( N \) candidates, it requires \( m + s \) operations from
the above updates to compute the objective. Over \( s \) selections, the
total time is the same as the cost to update the Cholesky factor, matching
the complexity of the non-partial multiple point algorithm. However, the
asymptotic work in the non-partial algorithm can be implemented as BLAS
level-2 calls, while the partial algorithm relies heavily on vector (level-1)
calls, affecting the constant-factor performance of the algorithm.

\subsection{Equivalence of Selection and Orthogonal Matching Pursuit}
\label{app:omp}

We show that the single-point selection algorithm described in
\cref{alg:gp_select} is the covariance space equivalent to the
feature space orthogonal matching pursuit (OMP) algorithm described
in \cite{tropp2007signal}. The equivalence comes from the fact
that Cholesky factorization is Gram-Schmitt in feature space.

\begin{align}
  \shortintertext{Let \( \Theta \) be a symmetric
    positive definite matrix such that}
  \Theta &= F^{\top} F
  \shortintertext{For some matrix \( F \)
    whose columns are vectors in feature space,}
  F &=
  \begin{pmatrix}
    \vec{x}_1 & \vec{x}_2 & \hdots & \vec{x}_N
  \end{pmatrix}
  \shortintertext{Immediately we have}
  \Theta_{ij} &= \langle \vec{x}_i, \vec{x}_j \rangle
  \shortintertext{where \( \langle \cdot, \cdot \rangle \)
    denotes the ordinary inner product on \( \mathbb{R}^N \).}
  \shortintertext{It suffices to see a single step of Cholesky
    factorization. Selecting \( \vec{x}_1 \),}
  \Theta' &= \Theta - \frac{\vec{x}_1 \vec{x}_1^{\top}}
    {\Theta_{11}} \\
  \label{eq:cov_step}
  \Theta_{ij}' &= \Theta_{ij} -
    \frac{\Theta_{i1} \Theta_{j1}}{\Theta_{ii}}
  \shortintertext{Switching to the feature space perspective,
    if we select \( \vec{x}_1 \) we force the rest of the
    feature vectors to be orthogonal to \( \vec{x}_1 \),}
  \vec{x}_i' &= \vec{x}_i -
    \frac{\langle \vec{x}_i, \vec{x}_1 \rangle}
         {\langle \vec{x}_1, \vec{x}_1 \rangle} \vec{x}_1 \\
  \label{eq:feature_step}
  \langle \vec{x}_i', \vec{x}_j' \rangle &=
    \langle \vec{x}_i, \vec{x}_j \rangle -
      \frac{\langle \vec{x}_i, \vec{x}_1 \rangle
            \langle \vec{x}_j, \vec{x}_1 \rangle}
          {\langle \vec{x}_1, \vec{x}_1 \rangle}
  \shortintertext{Comparing \cref{eq:cov_step} and \cref{eq:feature_step},
    we see that they are the same as expected. As a corollary, the objective
    of selecting the point \( \vec{x}_k \) that minimizes the residual of some
    target point \( \vec{x}_\text{Pr} \) can be written as}
  \lVert
    \vec{x}_\text{Pr} - \text{proj}_{\vec{x}_k} \vec{x}_\text{Pr}
  \rVert &= \langle \vec{x}_\text{Pr}, \vec{x}_\text{Pr} \rangle -
    \frac{\langle \vec{x}_\text{Pr}, \vec{x}_k \rangle^2}
        {\langle \vec{x}_k, \vec{x}_k \rangle}
  \shortintertext{Which is precisely the squared
    covariance of the candidate with the prediction over
    the variance of the candidate, as in \cref{eq:obj_gp}.}
  % \shortintertext{By induction, one can show}
  % F_2' = F_2 - P_1 F_2
  % \shortintertext{where \( F_2 \) is a set of feature
  %   vectors being conditioned and \( P_1 \) is the projection
  %   matrix onto the subspace spanned by \( F_1 \).}
\end{align}
This shows the equivalence as the objective is the same.

\section{Derivations in KL-minimization}

\subsection{Linear-algebraic formulation of objective}
\label{app:kl_obj}

We want to show that the KL-divergence between two multivariate
Gaussians centered at \( \vec{0} \) with covariance matrices
\( \Theta_1 \) and \( \Theta_2 \) can be written as
\begin{align}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  &= \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
  \shortintertext{where \( \Theta_1 \) and \( \Theta_2 \) are both
    of size \( N \times N \). Recall that the log density \( \log
    \pi(\vec{x}) \) for \( \vec{x} \sim \mathcal{N}(\vec{0}, \Theta) \) is}
  \log \pi(\vec{x}) &= -\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta) +
    \vec{x}^{\top} \Theta^{-1} \vec{x})
  \shortintertext{By the definition of KL-divergence,}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right ) &= 2 \E_P[\log P - \log Q]
  \shortintertext{where \( P \) and \( Q \) are the corresponding
    densities for \( \Theta_1 \) and \( \Theta_2 \) respectively,
    and \( \E_P \) denotes expectation under \( P \).}
  &= 2 \E_P[-\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta_1) +
            \vec{x}^{\top} \Theta_1^{-1} \vec{x}) \\
  \nonumber
  & \hphantom{= 2 \E_P[}
            +\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta_2) +
            \vec{x}^{\top} \Theta_2^{-1} \vec{x})] \\
  \label{eq:kl_after_logdet}
  &= \E_P[\vec{x}^{\top} \Theta_2^{-1} \vec{x} -
          \vec{x}^{\top} \Theta_1^{-1} \vec{x}]
          + \logdet(\Theta_2) - \logdet(\Theta_1) \\
  \E_P[\vec{x}^{\top} \Theta_2^{-1} \vec{x} -
          \vec{x}^{\top} \Theta_1^{-1} \vec{x}]
  &=
  \E_P[\trace(\vec{x}^{\top} \Theta_2^{-1} \vec{x}) -
       \trace(\vec{x}^{\top} \Theta_1^{-1} \vec{x})]
\end{align}
because the trace of a scalar is a scalar, and the linearity of trace.
\begin{align}
  &=
  \E_P[\trace(\Theta_2^{-1} \vec{x} \vec{x}^{\top}) -
       \trace(\Theta_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{cyclic property of trace} \\
  &=
  \E_P[\trace(\Theta_2^{-1} \vec{x} \vec{x}^{\top} -
              \Theta_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{linearity of trace} \\
  &= \E_P[\trace \left (
    (\Theta_2^{-1} - \Theta_1^{-1}) \vec{x} \vec{x}^{\top} \right )]
  && \text{factoring} \\
  &= \trace(\E_P \left [
    (\Theta_2^{-1} - \Theta_1^{-1}) \vec{x} \vec{x}^{\top} \right])
  && \text{swapping trace and expectation} \\
  &= \trace((\Theta_2^{-1} - \Theta_1^{-1})
    \E_P \left [ \vec{x} \vec{x}^{\top} \right])
  && \text{linearity of expectation} \\
  &= \trace((\Theta_2^{-1} - \Theta_1^{-1}) \Theta_1)
  && \text{\( \Theta_1 = \E_P[\vec{x} \vec{x}^{\top} \)]} \\
  &= \trace(\Theta_2^{-1} \Theta_1 - I)
  && \text{multiplying} \\
  &= \trace(\Theta_2^{-1} \Theta_1) - \trace(I)
  && \text{linearity of trace} \\
  &= \trace(\Theta_2^{-1} \Theta_1) - N
  \label{eq:kl_after_trace}
  && \text{trace of \( N \times N \) identity \( N \)}
\end{align}
Combining \cref{eq:kl_after_trace} with \cref{eq:kl_after_logdet}, we obtain
\begin{align}
  \nonumber
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  &= \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
\end{align}
as desired.

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL-divergence between \( \Theta \) and the Cholesky
factor \( L \) computed according to \cref{thm:L}. From \cref{eq:kl},
\begin{align}
 \nonumber
  \label{eq:kl_L}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \trace(L L^{\top} \Theta) - \logdet(L L^{\top}) - \logdet(\Theta) - N
  \shortintertext{Ignoring terms not depending on \( L \),}
  &= \trace(L L^{\top} \Theta) - \logdet(L L^{\top})
  \shortintertext{By the cyclic property of trace,}
  &= \trace(L \Theta L^{\top}) - \logdet(L L^{\top})
  \shortintertext{Focusing on \( \trace(L \Theta
    L^{\top}) \) and expanding on the columns of \( L \),}
  \trace(L \Theta L^{\top}) &= \sum_{i = 1}^N
    \left ( L_{s_i, i}^{\top} \Theta_{s_i, s_i} L_{s_i, i} \right )
  \shortintertext{Plugging in \( L_{s_i, i} \) from \cref{thm:L},}
  &= \sum_{i = 1}^N
  \left [
    \left (
      \frac{\left ( \Theta_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \Theta_{s_i, s_i}
    \left (
      \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
  \right ] \\
  &= \sum_{i = 1}^N
  \left [
    \frac{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1}
          \Theta_{s_i, s_i} \Theta_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}
  \right ] \\
  &= \sum_{i = 1}^N 1 = N
  \shortintertext{Using \( N \) for \( \trace(L
    L^{\top} \Theta) \) in \cref{eq:kl_L},}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= -\logdet(L L^{\top}) - \logdet(\Theta)
  \shortintertext{\( L^{\top} \) has the same log determinant
    as \( L \), and because \( L \) is lower triangular, its
    log determinant is just the sum of its diagonal entries:}
  &= -2 \sum_{i = 1}^N \left [ \log(L_{ii}) \right ] - \logdet(\Theta)
  \shortintertext{Plugging \cref{eq:L_col} for the diagonal entries,}
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\Theta)
  \shortintertext{Bringing the negative inside,}
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\Theta)
\end{align}
So minimizing the KL-divergence (given optimal \( L \)) corresponds to
minimizing the sum of the (inverse) of the diagonal entries. Intuitively,
because the Cholesky factorization is iterative conditioning \cref{eq:chol},
we can view the sum as the accumulated prediction error for a series of
prediction problems, where each prediction problem is to predict the value
of the \( i \)th variable given variables \( 1, 2, \dotsc, i - 1 \). This
gives a natural way to measure the quality of a sparsity pattern as a good
sparsity pattern should maintain predictive accuracy while subject to
the constraint that some variables have no interaction with others.

Because the KL-divergence is not symmetric, it matters which way the
KL-divergence is taken as well as whether both matrices have been
inverted or not. This seems to imply that there are four possible
ways to compare two covariance matrices. However, note that
\begin{align}
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right ) &=
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, L L^{\top}) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta^{-1})
  \right )
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL-divergence. There are
therefore only two possible ways to compare the two, which depends on
the order of the arguments. A statistical interpretation comes from the
fact that the KL-divergence can be interpreted as the likelihood-ratio
test, so the non-symmetry of the order of the arguments corresponds to
the asymmetry between the null and alternative hypotheses.

\end{document}
