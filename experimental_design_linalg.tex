% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Sparse Cholesky factorization by greedy conditional selection},
  pdfauthor={S. Huan, J. Guinness, M. Katzfuss, H. Owhadi, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Dense kernel matrices resulting from pairwise evaluations of a
  kernel function arise naturally in machine learning and statistics.
  Previous work in constructing sparse transport maps or sparse approximate
  inverse Cholesky factors of such matrices by minimizing Kullback-Leibler
  divergence recovers the Vecchia approximation for Gaussian processes.
  However, these methods often rely only on geometry to construct the
  sparsity pattern, ignoring the conditional effect of adding an entry.
  In this work, we construct the sparsity pattern by leveraging a
  greedy selection algorithm that maximizes mutual information with
  target points, conditional on all points selected previously.
  For selecting \( k \) points out of \( N \), the naive time
  complexity is \( \BigO(N k^4) \), but by maintaining a
  partial Cholesky factor we reduce this to \( \BigO(N k^2) \).
  Furthermore, for multiple (\( m \)) targets we achieve a time complexity
  of \( \BigO(N k^2 + N m^2 + m^3) \) which is maintained in the setting of
  aggregated Cholesky factorization where a selected point need not condition
  every target.
  We directly apply the selection algorithm to image
  classification and recovery of sparse Cholesky
  factors, improving upon \( k \)-th nearest neighbors.
  By minimizing Kullback-Leibler divergence, we apply the
  algorithm to Cholesky factorization, Gaussian process
  regression, and preconditioning with the conjugate gradient.
\end{abstract}

% REQUIRED
\begin{keywords}
  \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}

\paragraph{The problem}

Gaussian processes enjoy widespread application in spatial
statistics and geostatistics \cite{rue2005gaussian}, machine
learning through kernel methods \cite{rasmussen2006gaussian},
optimal experimental design \cite{mutny2022experimental}, and
sensor placement \cite{krause2008nearoptimal}.
However, Gaussian process statistics from \( N \) data points requires
computing with the covariance matrix \( \CM \in \Reals^{N \times N} \)
to obtain quantities such as \( \CM \vec{v} \), \( \CM^{-1} \vec{v} \),
\( \logdet(\CM) \).
For dense \( \CM \), directly computing these quantities has a
computational cost of \( \BigO(N^3) \) and a memory cost of \(
\BigO(N^2) \), which is prohibitively expensive for large \( N \).
Beyond Gaussian processes, computations with large positive-definite
matrices are required across computational mathematics,
motivating the search for faster, approximate algorithms.

\paragraph{Existing work}

\todo{I would put knothe rosenblatt, FSAI, and sparse Cholesky
factorization into the Vecchia paragraph and use the ``existing work''
paragraph to disucuss other approaches such as inducing points,
wavelet method, h matrices, fast multiplol, random features, Nystrom
approximation, sparse Cholesky of covariance, etc.}
TODO

\paragraph{Vecchia approximation}

\todo{I would make this one integrated section on
Vecchia, FSAI, KL-Cholesky, and transport maps}
As \cite{katzfuss2021general} points out, many sparse
factorizations of covariance matrices can be viewed through the
lens of the Vecchia approximation \cite{vecchia1988estimation}.
The Vecchia approximation decomposes the joint distribution
into the product of univariate densities, each density
conditional on only a subset of those prior in the ordering.
If an ordering and sparsity pattern are determined, then an approximate
Cholesky factor obeying the sparsity can be computed by optimizing a
chosen objective, for example, minimizing the Kullback-Leibler (KL)
divergence \cite{schafer2021sparse} is a natural objective for Gaussian
process regression.
Independently of the Vecchia approximation, within the context of factorized
sparse approximate inverse (FSAI) preconditioners, the Kaporin condition number
\cite{kaporin1990alternative} and the Frobenius norm with an additional Jacobi
scaling constraint \cite{yeremin2000factorized} have also been proposed as
objectives.
These three objectives actually converge to the same closed-form expression
for the entries of the resulting Cholesky factor \cite{schafer2021sparse},
and this expression is equivalent to the formula used to compute the Vecchia
approximation for Gaussian processes \cite{vecchia1988estimation}.
In addition, using the KL divergence to compute Cholesky factors
can be viewed as a special case of computing sparse transport
maps in \cite{marzouk2016introduction, katzfuss2022scalable}.
Transport maps with Knothe-Rosenblatt structure generalize Cholesky factors to
nonlinear and non-Gaussian distributions while preserving lower triangularity
and sparsity \cite{spantini2018inference}, and have been applied to simulation
and sampling problems \cite{marzouk2016introduction, katzfuss2022scalable}.
Finally, exploiting the independence of the Vecchia approximation allows
for embarrassingly parallel algorithms for Gaussian process regression
\cite{katzfuss2021general}, Cholesky factorization \cite{schafer2021sparse},
and transport map construction \cite{marzouk2016introduction}.

\paragraph{Ordering and sparsity selection by geometry}

\todo{Might want to add a quick comment explaining the maximin ordering here?}
The selected ordering and sparsity pattern have a
significant effect on the quality of the resulting factor.
Vecchia originally proposed ordering points lexicographically
\cite{vecchia1988estimation}, but more recent work exploits space-covering
orderings like the maximum-minimum ordering \cite{guinness2018permutation},
which has become popular \cite{schafer2020compression, schafer2021sparse,
katzfuss2021general, kang2021correlationbased, katzfuss2022scalable}.
Once the ordering is fixed, the sparsity set for a particular
point can include any point prior to it in the ordering.
Following Vecchia's recommendation, the points are often chosen to be
the closest points by Euclidean distance \cite{vecchia1988estimation,
schafer2020compression, schafer2021sparse, katzfuss2022scalable}.
This choice is often justified by noting that popular kernel functions
like the Mat{\'e}rn family decay exponentially with increasing distance.
For more general kernel functions, geostatisticians have long observed the
``screening effect'', or the observation that conditional on points close to
the point of interest, far away points are nearly conditionally independent
\cite{stein2002screening, stein20112010} (see \cref{fig:screening}).
However, selecting by distance ignores the conditional
effect of adding new points to the sparsity set.
As an illustrative example, imagine the closest point
to the point of interest is duplicated multiple times.
Conditional on the duplicated point, the
duplicates provide no additional information.
But they are still closest to the point of interest, so selecting by
distance alone would still add these redundant points to the sparsity set.

\paragraph{Conditional selection}

\todo{We should start with something like ``In this work we propose
a conditional...'' or so. That is, first make it clear that applying
conditional selection is the main contribution of the present work
and then relate it to all the other approaches / techniques.}
Instead of adding points to the sparsity pattern by distance, we propose
greedily selecting points that maximize mutual information with the
point of interest, conditional on all points selected previously.
The machine learning community has long developed similar algorithms that
greedily optimize information-theoretic objectives in the context of Gaussian
process inference \cite{smola2000sparse, herbrich2002fast, seeger2003fast}.
Similar algorithms have also been developed in the context of
sensor placement \cite{krause2008nearoptimal, clark2018greedy}
and experimental design \cite{mutny2022experimental} where it is
assumed the target phenomena is modeled by a Gaussian process or
is otherwise linearly dependent on the selected measurements.
However, these works often focus on obtaining a global approximation of
the entire process, e.g. through sparse approximation of the likelihood
or covariance matrix (see \cite{liu2020when, chalupka2012framework,
quinonero-candela2005unifying} for a comprehensive overview).
In contrast \cite{wada2013gaussian} use \emph{directed} inference for
a point of interest, in which they use the kernel function itself as
the objective like the later work \cite{kang2021correlationbased}.
\cite{gramacy2014local} and the follow up work \cite{gramacy2015speeding}
use the active learning Cohn (ALC) objective first described in
\cite{cohn1996neural} for directed Gaussian process regression,
yielding an algorithm equivalent to our proposed algorithm in the
case of a single point of interest.

Our proposed selection method can also be viewed as the covariance
equivalent of orthogonal matching pursuit (OMP) \cite{tropp2007signal,
tropp2006algorithms}, a workhorse algorithm popular in signal processing and
compressive sensing which seeks to approximate in feature space a target vector
as the sparse linear combination from a given collection of vectors.

\paragraph{Main results}

\todo{Want to start a one-sentence descrption of our contributions, something
like ``We propose...''. We should first fully describe what we are doing
(greedy selection, integration into the KL framework etc.) We want to start
talking about technical things like the computational costs and the improvement
only afterward, once we have established what it is that we are accelerating.}
Our main contribution is a selection algorithm that greedily maximizes
conditional mutual information with a point of interest, which we use to select
sparsity entries for sparse approximate Cholesky factors of precision matrices.
We compute Cholesky factors through the KL-minimization framework
of \cite{schafer2021sparse}, but use conditional selection
to form the sparsity pattern instead of nearest neighbors.
The final algorithm extends the kernel-based selection algorithms
\cite{wada2013gaussian, kang2021correlationbased} to account for conditioning
and extends directed Gaussian process regression \cite{gramacy2014local}
to multiple points as well as global approximation.
For a single point of interest, direct computation of the mutual information
criterion would have time complexity \( \BigO(N k^4) \) to select \( k \)
points out of \( N \), but by maintaining a partial Cholesky factor we reduce
the complexity to \( \BigO(N k^2) \).
We extend the algorithm to maximize mutual information
with \emph{multiple} points of interest, naturally taking
advantage of the ``two birds with one stone'' effect.
For \( m \) target points we achieve a time complexity of \(
\BigO(N k^2 + N m^2 + m^3) \), which for \( m \approx k \) is
essentially \( m \) times faster than the single-point algorithm.
In the setting of aggregated (or supernodal) Cholesky factorization where
the sparsity patterns of multiple columns are determined together, a
candidate entry may only condition a \emph{partial} subset of the targets.
By carefully applying rank-one downdating of Cholesky factors, we
capture this structure at the same time complexity for multiple points.
When applying our greedy selection method for sparsity selection
of Cholesky factors, we achieve more accurate factors for the same
number of nonzeros compared to selection by nearest neighbors.
Finally, we show how to adaptively determine the number of nonzeros per
column in order to reduce the global KL divergence by maintaining a global
priority queue shared between each local greedy selection algorithm.

\paragraph{Outline}

This paper is organized as follows.
In \cref{sec:chol}, we show how minimizing KL divergence to compute sparse
Cholesky factors reduces to solving independent regression problems and
extend this result to adjacent and nonadjacent aggregated factorization in
\cref{subsec:kl}.
In \cref{sec:select}, we develop greedy algorithms to select
the sparsity pattern independently for each regression problem.
In \cref{sec:chol_select}, we combine the greedy selection algorithms with
KL minimization to yield algorithms for sparse Cholesky factorization.
In \cref{sec:experiments}, we present numerical experiments applying our method
to image classification, recovery of \textit{a priori} sparse Cholesky factors,
Cholesky factorization, Gaussian process regression, and preconditioning with
the conjugate gradient.
In \cref{sec:conclusion}, we summarize our results.
Proofs and details on implementing our algorithms are
provided in the appendix and supplementary material.

\begin{figure}[t]
  \centering
  \input{figures/screening/uncond.tex}%
  \input{figures/screening/cond.tex}
  \caption{An illustration of the screening effect with the Mat{\'e}rn kernel
    with length scale \( \ell = 1 \) and smoothness \( \nu = \frac{5}{2} \).
    The first panel shows the unconditional covariance with the point at (0,
    0). The second panel shows the conditional covariance after conditioning
    on the four points in orange.}
  \label{fig:screening}
\end{figure}

\Ftodo{``specific'' and ``intuitively'' are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much
as possible. It's normal to add them out of reflex initially, so it
requires active postprocessing.}

\section{Sparse Cholesky factorization by KL-minimization}
\label{sec:chol}

Let \( \Theta \in \Reals^{N \times N} \) be a positive-definite matrix.
We view \( \Theta \) as the covariance matrix of a Gaussian process;
we say a function \( f(\vec{x}) \) is distributed according to a
Gaussian process prior with mean function \( \mean(\vec{x}) \) and
covariance function or kernel function \( \K(\vec{x}, \vec{x}')
\), if for any finite set of points \( X = \{ \vec{x}_i \}^N_{i =
1} \), \( f(X) \sim \N(\vec{\mean}, \CM) \), where \( \mean_i =
\mu(\vec{x}_i) \) and \( \CM_{ij} = \K(\vec{x}_i, \vec{x}_j) \).
\begin{align}
  f(\vec{x}) &\sim \GP(\mean(\vec{x}), \K(\vec{x}, \vec{x}'))
\end{align}
For any positive-definite matrix \( \CM \) there exists a set of points \( X
\) and kernel function \( \K \) such that \( \CM = \K(X, X) \), and for any \(
X \) the matrix \( \K(X, X) \) is positive-definite, so we freely move between
positive-definite matrices and point sets with kernel functions.

In many applications of Gaussian processes we wish
to infer unknown data given a training dataset.
Given the dataset \( \mathcal{D} = \{ (\vec{x}_i, y_i) \}^N_{i = 1} \)
where the inputs \( \vec{x}_i \in \Reals^D \) are collected in the matrix
\( X_\Train = [\vec{x}_1, \dotsc, \vec{x}_N]^{\top} \in \Reals^{N \times
D} \) and the measurements at those points are collected in the vector \(
\vec{y}_\Train = [y_1, \dotsc, y_N]^{\top} \in \Reals^N \), we wish to
predict the values at \( M \) new points \( X_\Pred \in \Reals^{m \times
D} \) for which \( \vec{y}_\Pred \in \Reals^m \) is unknown.
We assume the function \( f(\vec{x}) \) that maps input points to their
outputs is distributed according to a Gaussian process with zero mean
function, \( f(\vec{x}) \sim \GP(\vec{0}, \K(\vec{x}, \vec{x}')) \).
From the distribution of \( f(\vec{x}) \), the joint distribution
of training and testing data \( \vec{y} \) has covariance
\(
  \CM =
  \begin{pmatrix}
    \CM_{\Train, \Train} & \CM_{\Train, \Pred} \\
    \CM_{\Pred, \Train} & \CM_{\Pred, \Pred}
  \end{pmatrix}
\)
where \( \CM_{\I, \J} \defeq \K(X_\I, X_\J) \) for index sets \( \I, \J \).
In order to make predictions at \( X_\Pred \), we condition the desired
prediction \( \vec{y}_\Pred \) on the known data \( \vec{y}_\Train \).
For Gaussian processes, the closed-form posterior distribution is
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \vec{\mean}_\Pred +
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    (\vec{y}_\Train - \vec{\mean}_\Train) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \CM_{\Pred, \Pred} -
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    \CM_{\Train, \Pred}
  \shortintertext{For brevity of notation, we
    denote the conditional covariance matrix as}
  \label{eq:cond_cov_notation}
  \CM_{\I, \J \mid \V} &\defeq
    \CM_{\I, \J} - \CM_{\I, \V} \CM_{\V, \V}^{-1} \CM_{\V, \J}
  \shortintertext{where \( \CM_{\V, \V}^{-1} \defeq (\CM_{\V, \V})^{-1} \).
    To denote the covariance between the variables in index sets \( \I \) and
    \( \J \), conditional on the variables in \( \V_1, \V_2, \dots, \V_n \) we
    write}
  \CM_{I, J \mid \V_1, \V_2, \dotsc, \V_n} &\defeq
    \Cov[\vec{y}_\I, \vec{y}_\J \mid
         \vec{y}_{\V_1 \cup \V_2 \cup \dotsb \cup \V_n}]
  \shortintertext{The sets \( \V_1, \dotsc, \V_n \) are written in order
    of computation. Although the final covariance matrix is the same, a
    different order of conditioning means different intermediate results
    in repeated application of \cref{eq:cond_cov}. By the quotient rule
    of Schur complementation:}
  \label{eq:quotient_rule}
  \CM_{\I,   \J   \mid \V_{1 \dots n}} &=
  \CM_{\I,   \J   \mid \V_{1 \dots n - 1}} -
  \CM_{\I,   \V_n \mid \V_{1 \dots n - 1}}
  \CM_{\V_n, \V_n \mid \V_{1 \dots n - 1}}^{-1}
  \CM_{\V_n, \J   \mid \V_{1 \dots n - 1}}
\end{align}
Calculating the posterior mean \cref{eq:cond_mean} and covariance
\cref{eq:cond_cov} requires inverting the training covariance matrix.
Often the inverse Cholesky factor \( L = \chol(\CM_{\Train,
\Train})^{-1} \) is used instead of the direct inverse \( \CM_{\Train,
\Train}^{-1} \) for improved numerical stability and performance.
However, the time complexity of computing the Cholesky factorization
is \( \BigO(N^3) \), which is prohibitive for large \( N \).
For computational efficiency, we will enforce that \(
L \) is \emph{sparse}, yielding an approximate factor.
A significant factor affecting the approximation quality is whether \( L
\) is computed as an approximate inverse factor of the covariance (\( L
\chol(\Theta) \approx \Id \)) or as an approximate factor of the precision
(\( L L^{\top} \approx \Theta^{-1} \)).

\subsection{Vecchia approximation}
\label{subsec:vecchia}

Both sparse inverse factors of the covariance and sparse
factors of the precision can be computed from the Vecchia
approximation for Gaussian processes \cite{vecchia1988estimation}.
We start by decomposing the joint likelihood \( \pi \).
\begin{align}
  \label{eq:joint}
  \p(\vec{x}) &= \p(x_1) \p(x_2 \mid x_1) \p(x_3 \mid x_1, x_2) \dots
    \p(x_N \mid x_1, x_2, \dotsc, x_{N - 1})
  \shortintertext{The key assumption is that many of the points are
    redundant after conditioning on a carefully chosen subset of the
    points. Letting \( i_1, \dotsc, i_N \) denote an ordering of the
    points and \( s_k \) the indices of points that condition the \(
    k \)th point in the ordering, the Vecchia approximation proposes
    to approximate \cref{eq:joint} by the sparse approximation}
  \label{eq:vecchia}
  \p(\vec{x}) &\approx \p(x_{i_1}) \p(x_{i_2} \mid x_{s_2})
    \p(x_{i_3} \mid x_{s_3}) \dots \p(x_{i_N} \mid x_{s_N})
\end{align}
Following \cref{eq:vecchia} if an elimination ordering \( \prec \) is fixed
(a permutation of \( \{ 1, \dotsc, N \} \)) and a lower triangular sparsity
pattern \( S \defeq \{ (i, j) : i \in s_j, i \succeq j \} \) is specified
then all is needed is a functional criterion \( \Loss: \Reals^{N \times N}
\to \Reals \) to specify the optimization problem
\begin{align}
  \label{eq:generic_obj}
  L &\defeq \argmin_{\hat{L} \in \SpSet} \Loss(\hat{L})
\end{align}
where \( \SpSet \defeq \{ M \in \Reals^{N \times N} :
M_{ij} \neq 0 \Rightarrow (i, j) \in S \} \) is the space
of matrices satisfying the sparsity pattern \( S \).
Proposed functionals which compute inverse Cholesky factors of the
covariance, \( L \chol(\CM) \approx \Id \), include the Kaporin
condition number \( (\trace(L \CM L^{\top})/N)^N/\det(L \CM L^{\top})
\) \cite{kaporin1990alternative} and the Frobenius norm \( \norm{\Id
- L \chol(\CM)}_{\FRO} \) additionally subject to the constraint \(
\diag(L \CM L^{\top}) = 1 \) \cite{yeremin2000factorized}, while the KL
divergence \( \KL{\N(\vec{0}, \CM)} {\N(\vec{0}, (L L^{\top})^{-1})}
\) \cite{schafer2021sparse} computes factors of the precision, \( L
L^{\top} \approx \CM^{-1} \).

As observed in \cite{schafer2021sparse}, factors of the precision are often
much sparser than factors of the covariance, because the precision encodes
conditional independence while the covariance encodes marginal independence.
The same phenomenon is observed by \cite{spantini2018inference}
working with the more general transport maps.
Covariance matrices arising from kernel functions are often fully
dense, but the approximate factors of their precision can be
sparse if the ordering and sparsity pattern are chosen carefully.

\subsection{Ordering and sparsity pattern}
\label{subsec:ordering}

Although in this work we focus on constructing sparsity
patterns, a good ordering is critical since the sparsity for
a point can only include points after it in the ordering.
Vecchia originally proposed ordering points lexicographically, which is
most natural in a one-dimensional setting \cite{vecchia1988estimation}.
More recent work finds that in higher dimensions, exploiting
space-covering orderings leads to significantly better
approximation quality \cite{guinness2018permutation}.
We specifically use the maximum-minimum (maximin) ordering
\cite{guinness2018permutation}, which has become popular for the
Vecchia approximation \cite{katzfuss2021general} and Cholesky
factorization \cite{schafer2020compression, schafer2021sparse,
kang2021correlationbased, katzfuss2022scalable}.
The reverse-maximin ordering \( i_1, \dotsc, i_N \) on a set of \( N \) points
\( \{ \vec{x}_i \}^N_{i = 1} \) is defined by first selecting the last index \(
i_N \) arbitrarily and then choosing for \( k = N - 1, N - 2, \dotsc, 1 \) the
index
\begin{align}
  i_k = \argmax_{i \in -\Order_{k + 1}} \; \min_{j \in \Order_{k + 1}}
    \norm{\vec{x}_i - \vec{x}_j}
\end{align}
where \( -\Order \defeq \{ 1, \dotsc, N \} \setminus \Order \)
and \( \Order_n \defeq \{ i_n, i_{n + 1}, \dotsc, i_N \} \),
i.e. select the point farthest from previously selected points.
The ordering is reversed for factorizing the precision.

Vecchia also originally proposed to select the sparsity set by Euclidean
distance \cite{vecchia1988estimation}, which, unlike the lexicographic
ordering, still remains widely used \cite{schafer2020compression,
schafer2021sparse, katzfuss2022scalable}.
Instead, we show how to select the sparsity pattern to directly
optimize the objective \( \Loss \) \cref{eq:generic_obj} in an
end-to-end manner by decomposing the KL divergence along each column.

\subsection{Review of KL-minimization}
\label{subsec:kl}

The Kullback-Leibler (KL) divergence between two probability distributions \( P
\) and \( Q \) is defined as \( \KL*{P}{Q} \defeq \E_P[\log(\frac{P}{Q})] \).
As the expected difference between true and approximate
log-densities, the KL divergence is a natural way to
judge the quality of an approximating distribution.
In line with the Gaussian process regression problem in \cref{sec:chol}, we
identify the positive-definite matrix \( \CM \in \Reals^{N \times N} \) as
the covariance of a centered Gaussian process \( \N(\vec{0}, \CM) \) which we
seek to approximate by a sparse approximate Cholesky factor \( L \in \SpSet
\) of its precision, \( \N(\vec{0}, (L L^{\top})^{-1}) \).
In order to compare the resulting distributions, we specialize
the generic optimization problem \cref{eq:generic_obj}
to the KL divergence as \cite{schafer2021sparse} does.
\begin{align}
  \label{eq:L_obj}
  L \defeq \argmin_{\hat{L} \in S} \,
    \KL*{\N(\vec{0}, \CM)}
        {\N(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})}
\end{align}
For multivariate Gaussians, the KL divergence
has a closed-form expression given by
\begin{align}
  \label{eq:kl}
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}
where \( \CM_1, \CM_2 \in \Reals^{N \times N} \).
Using this expression for the KL divergence and optimizing for \( L \)
yields the following closed-form expression for the \( i \)th column of
\( L \) with sparsity pattern \( s_i \), reproduced from Theorem 2.1 of
\cite{schafer2021sparse}:
\begin{align}
  \label{eq:L_col}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
\end{align}
where the notation for \( \CM_{s_i, s_i}^{-1} \) is from
\cref{eq:cond_cov_notation} and \( \vec{e}_1 \in \Reals^{\card{s_i}
\times 1} \) denotes the vector with first entry one and the rest zero.
We enforce the convention that \( i \) is the first entry
of \( s_i \), also implying that \( L \) is of full rank.
Plugging the optimal \( L \) \cref{eq:L_col} back into the KL divergence
\cref{eq:kl}, we obtain the objective as a function of the sparsity pattern.
\begin{align}
  \label{eq:obj_chol}
  2 \KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} &=
    \sum_{i = 1}^N
      \left [
        \log \left (
          (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
        \right )
      \right ]
      - \logdet(\CM)
\end{align}
See \cref{app:kl_L} for details. In particular, it is important
which direction the KL divergence is taken or cancellation
of the \( \trace(\CM_2^{-1} \CM_1) \) term may not occur.

In order to pick a sparsity pattern that minimizes the KL divergence
\cref{eq:obj_chol}, we can ignore the constant \( \logdet(\CM) \) and minimize
over each column independently, as each term \( (\vec{e}_1^{\top} \CM_{s_i,
s_i}^{-1} \vec{e}_1)^{-1} \) in the sum depends only on \( s_i \).
To do so, we recall that conditioning in
covariance is marginalization in precision.
For
\( \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\),
\begin{align}
  \label{eq:inverse_cond}
  \CM_{1, 1 \mid 2} &= \left ( \CM^{-1} \right )_{1, 1}^{-1} \\
  \shortintertext{Viewing \( (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
    \vec{e}_1)^{-1} = \left (\CM_{s_i, s_i}^{-1} \right )_{1, 1}^{-1}
    \) as a marginalization of \( \CM_{s_i, s_i}^{-1} \) to apply
    \cref{eq:inverse_cond},}
  \label{eq:L_cond_var}
  (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= \CM_{i, i \mid s_i \setminus \{ i \}}
\end{align}
So minimizing the KL divergence of a sparse Cholesky factor is equivalent
to independently minimizing over each column the variance of the \( i
\)th variable, conditional on the entries \( s_i \) (excluding itself).
Decomposition of the KL divergence into independent
regression problems was also observed in
\cite{katzfuss2022scalable} for lower triangular transport maps.

\subsubsection{Aggregated sparsity pattern}
\label{subsubsec:aggregated}

We can derive a similar decomposition of the KL divergence
if the same sparsity pattern is reused for multiple columns,
known as aggregated or supernodal Cholesky factorization.
Aggregation can lead to substantial asymptotic
time and space savings \cite{schafer2021sparse}.
Throughout the following discussion we will focus on a single group
\( \tilde{i} = \{i_1, \dots, i_m \} \) composed from aggregating
the column indices \( i_1 \succ i_2 \succ \dotsb \succ i_m \).
Let \( \tilde{i} \) have aggregated sparsity pattern \(
s_{\tilde{i}} \) satisfying \( s_{\tilde{i}} \supseteq \tilde{i}
\) to guarantee that the Cholesky factor has full rank.
Let \( \tilde{s} \defeq s_{\tilde{i}} \setminus \tilde{i} \) be the aggregated
sparsity pattern excluding the diagonal entries of the columns in the group.
The sparsity pattern of the \( k \)th column in the group is then
the aggregated sparsity pattern excluding the entries that violate lower
triangularity, \( s_k \defeq \{ j \in s_{\tilde{i}} : j \succeq k \} \).
Assuming every entry of \( \tilde{s} \) is after every index in \( \tilde{i}
\), then \( s_k = \tilde{s} \cup \{ j \in \tilde{i} : j \succeq k \} \).
This condition is guaranteed if the aggregated
columns are adjacent in the ordering, for example.
We defer handling the nonadjacent case to \cref{subsubsec:partial}.
We now simplify the KL minimization objective on aggregated
indices with the conditional chain rule of log determinants.
Using the same blocking as \cref{eq:inverse_cond},
\begin{align}
  \label{eq:det_chain}
  \logdet(\CM) &= \logdet(\CM_{1, 1}) + \logdet(\CM_{2, 2 \mid 1})
\end{align}
The KL divergence objective \cref{eq:obj_chol} restricted
to the contribution from the group \( \tilde{i} \) is
\begin{align}
  \sum_{i \in \tilde{i}} \log(\CM_{i \mid s_i \setminus \{ i \} }) &=
    \log(\CM_{i_1 \mid \tilde{s}}) +
    \log(\CM_{i_2 \mid \tilde{s} \cup \{ i_1 \}}) + \dotsb +
    \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{where we write \( \CM_j \defeq \CM_{j, j} \).
    Combining the first two terms by the chain rule \cref{eq:det_chain},}
  &= \logdet(\CM_{\{ i_1, i_2 \} \mid \tilde{s}}) +
     \log(\CM_{i_3 \mid \tilde{s} \cup \{ i_1, i_2 \}}) + \dotsb +
     \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{Proceeding by induction, we are
    able to reduce the entire sum to the single term}
  \label{eq:obj_mult}
  &= \logdet(\CM_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}
So the suitable generalization of the conditional variance
in \cref{eq:obj_chol} to aggregated columns is the log
determinant of the covariance matrix of the columns,
conditional on (well-behaved) selected entries.
We briefly discuss what happens in the general case.

\subsubsection{Nonadjacent or partial aggregation}
\label{subsubsec:partial}

\begin{figure}[t]
  \centering
  \input{figures/partial_factor.tex}
  \caption{
    Illustration of the Cholesky factorization of
    a partially conditioned covariance matrix.
    Here \textcolor{darksilver}{grey} denotes fully unconditional,
    \textcolor{darklightblue}{blue} denotes fully conditional, and the
    \textcolor{silver!50!lightblue}{mixed color} denotes interaction
    between the two.
    Surprisingly, such a matrix factors into a ``pure'' Cholesky factor.
  }
  \label{fig:partial_factor}
\end{figure}

Let the random variables corresponding to the indices \( i_1, \dotsc, i_m
\) be collected in a vector \( \vec{y} = [y_1, \dotsc, y_m]^{\top} \) with
joint density multivariate Gaussian with covariance matrix \( \CM \).
We select an index \( k \) that conditions the variables, but ignores the first
\( p \) variables (recall that the indices are sorted in decreasing order,
so if \( k \) conditions a variable, it conditions everything afterwards).
After this partial conditioning the variables become \( \vec{y}_{\parallel
k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k}, \dotsc, y_{m \mid k} \)
and the covariance matrix \( \Cov[\vec{y}_{\parallel k}] \) becomes
\begin{align}
  \label{eq:chol_partial}
  \CM_{\tilde{i}, \tilde{i} \parallel k} &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}
where \( L = \chol(\CM) \) and \( L' = \chol(\CM_{\mid
k}) \) (see \cref{app:partial} for details).
As depicted in \cref{fig:partial_factor}, the Cholesky factor of \(
\CM_{\parallel k} \) is composed of ``gluing'' the prefix of the unconditional
factor \( L \) with the suffix of the conditional factor \( L' \).
Armed with this representation, we show the equivalence between minimizing \(
\logdet(\CM_{\parallel k}) \) and minimizing KL divergence \cref{eq:obj_chol}.
We compute \( \logdet(\CM_{\parallel k}) \) by its Cholesky
factor \cref{eq:chol_partial} and recall the determinant of
a triangular matrix is the product of its diagonal entries.
\begin{align}
  \label{eq:partial_logdet}
  \frac{1}{2} \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
  \underbrace{\log(L_{1, 1}) + \dotsb + \log(L_{p, p})}_{\text{the same}} +
  \underbrace{
    \log({L'}_{p + 1, p + 1}) + \dotsb + \log({L'}_{m, m})
  }_{\text{conditioned}}
  \shortintertext{Comparing to the KL divergence \cref{eq:obj_chol}
    and recalling that \( k \) is added to \( s_i \) if \( i > p \),}
  \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
  \label{eq:partial_kl}
  &= \underbrace{
      \log \left ( \CM_{1, 1 \mid s_1 \setminus \{ 1 \}} \right ) + \dotsb +
      \log \left ( \CM_{p, p \mid s_p \setminus \{ p \}} \right )
     }_\text{the same} + \\
  \nonumber
  % disgusting hack, do properly later --- looks good enough to me, though
  & \hphantom{=} \: \,
  \underbrace{
    \log \left (
      \CM_{p + 1, p + 1 \mid s_{p + 1} \setminus \{ p + 1 \}}
    \right ) + \dotsb +
    \log \left ( \CM_{m, m \mid s_m \setminus \{ m \}} \right )
  }_\text{conditioned}
  \shortintertext{Since \( L_{i, i} \) (and \( {L'}_{i, i} \)) is the
    square root of the conditional variance of the \( i \)th variable from
    the staistical perspective in \cref{eq:chol}, we have \( 2 \log(L_{i,
    i}) = \log(\CM_{i, i \mid s_i \setminus \{ i \}}) \) and so}
  \nonumber
  \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
    \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
\end{align}
Like the aggregated case in \cref{subsubsec:aggregated},
minimizing the log determinant of the \emph{partially} conditioned
covariance matrix is the same as minimizing the KL divergence.

In every case we have considered, picking the right sparsity pattern
to minimize KL divergence reduces to selecting the points \( s_i \)
out of the possible candidates \( i + 1, \dotsc, N \) that most reduce
predictive error at point(s) of interest.
In the next section, we develop such a selection
algorithm for directed inference in Gaussian processes.
We apply this algorithm for sparsity selection
of Cholesky factors in \cref{sec:chol_select}.

\section{Greedy selection for directed inference}
\label{sec:select}

\begin{figure}[t]
  \centering
  \input{figures/selection/knn.tex}%
  \input{figures/selection/cknn.tex}
  \caption{Here, the \textcolor{lightblue}{blue} points are the
    \textcolor{lightblue}{candidates}, the \textcolor{orange}{orange}
    point is the \textcolor{orange}{unknown} point to predict at, and the
    \textcolor{seagreen}{green} points are the \textcolor{seagreen}{selected}
    points. The \textcolor{rust}{red} line is the \textcolor{rust}{conditional
    mean} \( \mu \), conditional on the selected points, and the \( \pm 2
    \sigma \) confidence interval is shaded for the conditional variance
    \( \var \). On the left is selection by Euclidean distance and on the
    right is selection by conditional variance.}
  \label{fig:selection}
\end{figure}

In Gaussian process regression, we are given \( N \) points of training
data \( X_\Train \in \Reals^{N \times D} \) with associated measurements
\( \vec{y}_\Pred \in \Reals^N \) and wish to predict at \( m \) points
\( X_\Pred \in \Reals^{m \times D} \) whose values \( \vec{y}_\Pred \in
\Reals^m \) are unknown.
Unfortunately, computing the conditional distribution of \( \vec{y}_\Pred
\) by the equations \cref{eq:cond_mean} and \cref{eq:cond_cov}
has computational time complexity \( \mathcal{O}(N^3) \).
Instead, we carefully select a subset of \( s \) points out of the
\( N \), \( s \ll N \), and pay a much smaller \( \mathcal{O}(s^3)
\) cost by using only this subset to make predictions.
Of course, throwing away \( N - s \) points worsens the predictive accuracy.
Maintaining reasonable accuracy at a reduced computational
cost requires a criterion to choose the most effective points.

From minimizing the KL divergence, the criteria should
be to minimize the variance of the target point
conditional on the selected points \cref{eq:L_cond_var}.
The variance objective was first described by \cite{cohn1996neural} for optimal
experimental design and later applied to directed Gaussian process inference
by \cite{gramacy2014local} who refer to it as the active learning Cohn (ALC)
objective in honor of \cite{cohn1996neural}.
In addition, the variance objective is equivalent to maximizing
the \emph{mutual information} or \emph{information gain} with the
target point as well as minimizing the expected mean squared error
(see \cref{app:mutual_info}).
The mutual information (in a slightly different context) is
also used by \cite{krause2008nearoptimal} for sensor placement.

In contrast to using Euclidean distance \cite{vecchia1988estimation}
or the unconditional kernel function \cite{wada2013gaussian,
kang2021correlationbased}, conditional variance incentives picking points near
the target point, but also away from previously selected points.
This spreading effect is illustrated in \cref{fig:selection},
where each method is given a budget of two points.
Euclidean distance prefers the two points right of the target point.
However, a more balanced view of the situation is obtained when picking the
further away but ultimately more informative point on the left, reducing
variance at the target point and thereby reducing predictive error.

\subsection{A greedy approach}
\label{subsec:greedy_select}

Maximizing the conditional variance over all possible \( \binom{N}{s} \)
subsets is intractable, so we greedily select the next point which most
reduces the conditional variance, conditional on previously selected points.
Let \( \I = \{ i_j \}^s_{j = 1} \subseteq \Train \) be the
set of indices of previously selected training points.
For a candidate index \( k \), we condition the current
covariance matrix on \( y_k \) according to \cref{eq:cond_cov}:
\begin{align}
  \CM_{:, : \mid \I, k} &= \CM_{:, : \mid \I} -
    \CM_{:, k \mid \I} \CM_{k, k \mid \I}^{-1} \CM_{k, : \mid \I} \\
  %                    &= \CM_{:, : \mid \I} -
  % \frac{\CM_{:, k \mid \I} \CM_{:, k \mid \I}^{\top}}{\CM_{k, k \mid \I}} \\
  \label{eq:cond_select}
                       &= \CM_{:, : \mid \I} - \vec{u} \vec{u}^{\top} \\
  \label{eq:cond_cov_vec}
  \vec{u} &= \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k, k \mid \I}}}
\end{align}
From \cref{eq:cond_select}, adding a new index \( k \)
is a rank-one downdate on the current covariance matrix.
The decrease in the variance of \( y_\Pred \) after
selecting \( k \) is given by \( u_\Pred^2 \), or
\begin{align}
  \label{eq:obj_gp}
  u_\Pred^2 = \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
  = \frac{\Cov[y_\Pred, y_k \mid \I]^2}{\Var[y_k \mid \I]}
  = \Var[y_\Pred \mid \I] \Corr[y_\Pred, y_k \mid \I]^2
\end{align}
After adding the index \( k \), we need to keep track of each
candidate \( j \)'s updated variance and covariance with the
prediction point to compute the objective \cref{eq:obj_gp}.
We start with the unconditional values given by \( \CM_{j, j} \) and
\( \CM_{\Pred, j} \) and update after selecting each index \( k \).
We directly compute \( \vec{u} \) for \( k \) according to \cref{eq:cond_cov}
and update \( j \)'s conditional variance by subtracting \( u_j^2 \)
and update its conditional covariance by subtracting \( u_j u_\Pred \).

We have two efficient strategies to compute \( \vec{u} \).
The direct method is to keep track of \( \CM_{\I, \I}^{-1}
\), or the precision of the selected entries, and update
the precision every time a new index is added to \( \I \).
This can be done efficiently in computational time
complexity \( \BigO(s^2) \), see \cref{app:prec_insert}.
Once \( \CM_{\I, \I}^{-1} \) has been computed, \( \vec{u}
\) is computed directly according to \cref{eq:cond_cov}.
For each of the \( s \) rounds of selection, it takes \( \BigO(s^2)
\) to update the precision and \( \BigO(Ns) \) to compute \( \vec{u}
\), costing \( \BigO(N s^2 + s^3) \equiv \BigO(N s^2) \) overall.

The second strategy is to take advantage of the quotient
rule of Schur complementation \cref{eq:quotient_rule}.
From a statistical perspective, the quotient rule states
that conditioning on \( \I \) and then conditioning on \(
\J \) is the same as conditioning on \( \I \cup \J \).
We then remind ourselves that Cholesky factorization
can be viewed as iterative conditioning.
Re-writing the joint covariance matrix by
two steps of block Gaussisan elimination,
\begin{align}
  \label{eq:chol_schur}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{so we see that the Cholesky factorization of the joint
    covariance \( \CM \) is}
  \label{eq:chol}
  \chol(\CM) &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \chol(\CM_{1, 1}) & 0 \\
    0 & \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix} \\
  \nonumber
  &=
  \begin{pmatrix}
    \textcolor{darkorange}{\chol(\CM_{1, 1})} & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \chol(\CM_{1, 1})^{-\top}} &
    \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix}
\end{align}
Here the conditional expectation in \cref{eq:cond_mean} corresponds to
\( \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} \)
and the conditional covariance in \cref{eq:cond_cov} corresponds to
\(
\textcolor{lightblue}{
  \CM_{2, 2 \mid 1} = \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
} \).
Thus, we see that Cholesky factorization is
iteratively conditioning the Gaussian process.
From the iterative conditioning perspective, the \( k \)th column of the
Cholesky factor corresponds precisely to the corresponding \( \vec{u} \) for
\( k \) in \cref{eq:cond_cov_vec} since a iterative sequence of conditioning
on \( i_1, i_2 \dotsc, i_{k - 1} \) is equivalent to conditioning on \( \I
\) by the quotient rule \cref{eq:quotient_rule}.

Adding a column to the current Cholesky factor can be efficiently
computed without excess dependence on \( N \) with left-looking (see
\cref{alg:chol_update}), so the conditioning only happens when we need it.
For each of the \( s \) rounds of selection, it costs \( \BigO(N s) \)
to compute the next column of the Cholesky factorization, for a total
time complexity of \( \BigO(N s^2) \), matching the time complexity of
the explicit precision approach.

However, the precision algorithm uses \( \BigO(s^2) \) space to store the
precision \( \Theta_{\I, \I}^{-1} \) while the Cholesky algorithm uses \(
\BigO(N s) \) space to store the first \( s \) columns of the Cholesky factor
of \( \CM \), always more memory than the precision (\( N > s \)).
Both algorithms use an additional \( \BigO(N) \) space
to store the conditional variances and covariances.
Although the precision algorithm uses slightly less memory
than the Cholesky algorithm, the Cholesky algorithm is
preferred for better performance and ease of implementation.

\Stodo{this is a bit useless}
Once the indices have been computed by \cref{alg:select_prec} or
\cref{alg:select_chol}, inferring the conditional mean and covariance
of unknown data can be done directly according to \cref{eq:cond_mean}
and \cref{eq:cond_cov} in time \( \BigO(s^3) \) using \cref{alg:infer_select}.

\subsection{Supernodes and blocked selection}
\label{subsec:mult_select}

We now consider efficiently dealing with multiple prediction points.
The first problem is generalizing the objective for
a single point \cref{eq:obj_gp} to multiple points.
Following the KL minimization reasoning in \cref{subsubsec:aggregated},
the criterion should be to minimize the log determinant of the prediction
points' covariance matrix after conditioning on the selected points, \(
\logdet(\CM_{\Pred, \Pred \mid I}) \).
This objective, known as D-optimal design in the literature
\cite{krause2008nearoptimal}, has many intuitive interpretations, for
example, as a volume of the region of uncertainty or as the scaling factor
in the probability density function of multivariate Gaussians.
In addition, it is equivalent to maximizing mutual information
since the differential entropy of a Gaussian is monotonically
increasing with its log determinant (see \cref{app:mutual_info}).

We want to quickly compute how selecting an
index \( k \) affects the log determinant.
From \cref{eq:cond_select}, selecting an index
is a rank-one downdate on the covariance matrix:
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{By application of the matrix determinant lemma
    (the details are in \cref{app:logdet_downdate}),}
  \label{eq:greedy_mult}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}
Equation \cref{eq:greedy_mult} shows that to compute
the updated log determinant, it suffices to only
compute conditional variances of the candidate point.
Intuitively this corresponds to \emph{backwards}
regression, where we imagine measuring the values of the
\emph{prediction points} instead of at the \emph{candidates}.
We then infer the posterior variance at a candidate, and pick the candidate
whose conditional variance decreases the most (relative to its starting value).
These candidates are likely to give information about the prediction
points, because the prediction points give information about the candidate.

\Stodo{image for ``backwards'' sensor placement}

Re-writing the objective in this way motivates
an efficient algorithm to compute the objective.
We condition on a newly added point essentially the same as in
\cref{subsec:greedy_select}, but now maintaining two data structures
instead of one: one for the variance after conditioning on the
previously selected points, and the other for the variance after
also conditioning on the prediction points.
By the quotient rule \cref{eq:quotient_rule}, the order of
conditioning does not matter as long as the order is consistent.
For the second data structure, we therefore condition on the
prediction points \emph{first} before any points have been selected.
We again have two strategies, one which explicitly maintains precisions
and the other which relies on maintaining partial Cholesky factors.

For the precision algorithm, using \cref{eq:cond_cov} directly, for \( m
\) prediction points it costs \( \BigO(m^3) \) to compute \( \CM_{\Pred,
\Pred}^{-1} \) and then \( \BigO(N m^2) \) to compute the initial conditional
variances \( \CM_{k, k \mid \Pred} \) for the \( N \) candidates.
For each of the \( s \) rounds of selecting candidates, it costs \(
\BigO(s^2) \) and \( \BigO(m^2) \) to update the precisions \( \CM_{\I,
\I}^{-1} \) and \( \CM_{\Pred, \Pred}^{-1} \) respectively, where the details
of efficiently updating \( \CM_{\Pred, \Pred}^{-1} \) after the rank-one
update in \cref{eq:obj_gp_mult} are given in \cref{app:prec_cond}.
Given the precisions, \( \vec{u} = \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k,
k \mid \I}}} \) and \( \vec{u}_\Pred = \frac{\CM_{:, k \mid \I,
\Pred}}{\sqrt{\CM_{k, k \mid \I, \Pred}}} \) are computed as usual according
to \cref{eq:cond_cov} in time \( \BigO(Ns) \) and \( \BigO(Nm) \).
Finally, for each candidate \( j \), the conditional variance \( \CM_{j, j
\mid \I} \) is updated by subtracting \( u_j^2 \), the conditional covariance
\( \CM_{\Pred, k \mid \I} \) is updated for each index \( c \) of a prediction
point by subtracting \( u_j u_c \), and the conditional variance \( \CM_{j, j
\mid I, \Pred} \) is updated by subtracting \( {u_\Pred}_j^2 \).
The total time complexity after simplification
is \( \BigO(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two partial Cholesky factors are stored.
We first compute the Cholesky factorization after selecting each prediction
point, for a cost of \( \BigO((N + m) m) \) for each of the \( m \) columns.
We then begin selecting candidates, which requires updating
both Cholesky factors in time \( \BigO((N + m)(m + s)) \)
dominated by updating the preconditioned Cholesky factor.
The columns of the Cholesky factors correspond precisely to \( \vec{u}
\) and \( \vec{u}_\Pred \) and both conditional variances \( \CM_{j, j
\mid I} \) and \( \CM_{j, j \mid I, \Pred} \) can be computed as above.
The conditional covariances do not need to be computed.
Over \( s \) rounds the total time complexity is \( \BigO((N + m) m^2 +
s(N + m)(m + s)) \) which simplifies to \( \BigO(N s^2 + N m^2 + m^3) \).

Like the single-point case, both approaches have the
same time complexity but differ in space complexity.
The precision algorithm requires \( \BigO(s^2 + m^2) \)
memory to store both precisions, as well as \( \BigO(N
m) \) memory to store the conditional covariances.
The Cholesky algorithm requires \( \BigO((N + m)(m + s)) \) memory
to store the first \( m + s \) columns of the Cholesky factor for
the joint covariance matrix between training and prediction points,
which simplifies to \( \BigO(N s + N m + m^2) \).
The memory usages are identical except for \( \BigO(s^2)
\) versus \( \BigO(N s) \), so the Cholesky algorithm
again uses more memory than the precision algorithm.

\subsection{Partial selection}
\label{subsec:partial_select}

We can now generalize the multiple-point algorithm to track the log
determinant of the target points' covariance matrix if a selected
index \( k \) only conditions \emph{suffixes} of the target points,
skipping the first \( p \) targets, say.

The form of the Cholesky factor of the target covariance
matrix \cref{eq:chol_partial} suggests to keep track of
the \emph{insertion position} of the new index \( k \).
To be precise, we maintain a partial Cholesky factor \( L \) of the joint
covariance matrix between the target points and all training points, whose
columns consist of the target points and previously selected points sorted
in reverse order with respect to \( \prec \).
Adding a new entry adds a column to \( L \).
Inserting the index \( k \) into its proper
spot in the Cholesky factor \( L \),
\begin{align}
  \label{eq:chol_partial_update}
  L &\gets
  \begin{pmatrix}
    L_{:, :j} & \vec{u} & {L'}_{:, j + 1:}
  \end{pmatrix} \\
  \vec{u} &= \frac{\CM_{:, k \mid :j}}{\sqrt{\CM_{k, k \mid :j}}}
  \shortintertext{where unlike \cref{eq:chol_partial}, we are using a
    lower-triangular factor \( L^{\top} L = \CM \) for simplicity: if}
  \label{eq:U_chol}
  U &= P^{\Reverse} \chol(P^{\Reverse} \CM P^{\Reverse}) P^{\Reverse}
\end{align}
where \( P^{\Reverse} \) is the order-reversing permutation, then \( U \)
is an upper-triangular factor satisfying \( U U^{\top} = \CM \) so \( L =
U^{\top} \) is a lower-triangular factor satisfying \( L^{\top} L = \CM \).

To account for this insertion, the first \( j \) columns \( L_{:, :j} \)
are unchanged, the newly inserted column \( \vec{u} \) as a variance-scaled
conditional covariance of the form \cref{eq:cond_cov_vec} can be computed
as usual with left-looking (see \cref{alg:chol_update}) and the remaining
columns \( {L'}_{:, j + 1:} \) are the Cholesky factor of the covariance
matrix conditional on the point \( k \).

From \cref{eq:cond_select} conditioning on an additional point is a rank-one
downdate of the covariance matrix by the vector \( \vec{u} \), allowing the
updated factor \( {L'}_{:, j + 1:} \) to be efficiently computed from the
original factor \( L_{:, j + 1:} \).
Specifically we adapt Lemma 1 of \cite{krause2015more} to make no
assumption on the row ordering of the Cholesky factor, allowing
the downdate to be implemented with in-place BLAS \texttt{daxpy}
operations (see \cref{alg:chol_insert}).
For a Cholesky factor with \( R \) rows and \( C \) columns, the
time complexity of the downdate is \( \BigO(R C) \) compared to
\( \BigO(R C^2) \) if the factor was recomputed from scratch.

In the context of the overall update for \( N \) training points,
\( m \) target points, and a budget of \( s \) selected points, the
Cholesky factor \( L \) is of size \( (N + m) \times (s + m) \).
When adding a new index \( k \) that ignores the first \( j \) targets, the
first \( j \) columns of \( L \) are ignored, the new column \( \vec{u} \)
must be computed with left-looking in time \( \BigO((N + m) j) \), and the
columns past \( j \) are computed with downdating in time \( \BigO((N + m)
(s + m - j)) \).
The cost for a single insertion is \( \BigO((N + m)(s + m)) \) for a total time
complexity of \( \BigO(s(N + m)(s + m)) \) over \( s \) rounds of selection.
Given the factor \( L \), we now discuss actually computing the log
determinant objective \cref{eq:partial_logdet} after adding a candidate.

We have a candidate index \( j \) and wish to compute
its effect on the log determinant of the target points
(or KL divergence) \cref{eq:partial_kl} if added.
To do so, we will directly compute each individual target point's
variance, conditional on \( j \) as well as previously selected
points, and simply add these variances to get the overall objective.

Recall from \cref{eq:chol} that the \( j \)th row of the \( i \)th column
of a Cholesky factor \( L \) is \( L_{j, i} = \frac{\CM_{j, i \mid :i -
1}}{\sqrt{\CM_{i, i \mid :i - 1}}} \), i.e. the covariance between the \( j
\)th point and the \( i \)th point, conditional on all points prior to \(
i \) in the \emph{columns} of \( L \) (not with respect to \( \prec \)).
A special case is that the ``diagonal'' entry \( L_{i, i} = \sqrt{\CM_{i, i
\mid :i - 1}} \) is the square root of the conditional variance of the \( i
\)th point (the rows of \( L \) are not necessarily ordered with respect to \(
\prec \), so \( L \) is not necessarily literally lower triangular in memory).

If we have the variance of the \( j \)th point conditional on all points
prior to the \( i \)th column, \( \CM_{j, j \mid :i - 1} \), then by reading
off the conditional covariance \( L_{j, i} \) we can use conditioning
\cref{eq:quotient_rule} to compute the desired effect on the \( i \)th point
\( \CM_{i, i \mid :i - 1, j} \), and by using the conditional variance \(
L_{i, i} \) we can condition the \( j \)th point \( \CM_{j, j \mid :i - 1,
i} = \CM_{j, j \mid :i} \) to bring it to the next column.
These series of updates imply an inductive algorithm starting with the
first column, where \( \CM_{j, j \mid :i - 1} \) is just \( \CM_{j,
j} = K(\vec{x}_j, \vec{x}_j) \), the variance of the \( j \)th point.
For the \( i \)th column, the updates are as follows:
\begin{align}
  \label{eq:partial_diag}
  \CM_{i, i \mid :i - 1} &= L_{i, i}^2 \\
  \CM_{j, i \mid :i - 1} &= L_{j, i} \cdot L_{i, i} \\
  % \CM_{i, i \mid :i - 1, j} &= \CM_{i, i \mid :i - 1} -
  %   \frac{\CM_{j, j \mid :i - 1}^2}{\CM_{j, j \mid :i - 1}} \\
  \label{eq:obj_partial}
  \CM_{i, i \mid :i - 1, j} &= \CM_{i, i \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{j, j \mid :i - 1} \\
  % \CM_{j, j \mid :i - 1, i} &= \CM_{j, j \mid :i - 1} -
  %   \frac{\CM_{j, i \mid :i - 1}^2}{\CM_{i, i \mid :i - 1}} \\
  \label{eq:partial_induct}
  \CM_{j, j \mid :i - 1, i} &= \CM_{j, j \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{i, i \mid :i - 1} \\
                            &= \CM_{j, j \mid :i}
\end{align}
where \cref{eq:partial_induct} satisfies the conditions of the inductive
hypothesis for the next column and \cref{eq:obj_partial} is the desired
conditional variance when the \( i \)th point is a target (recall the
columns of \( L \) include previously selected training points, whose
variances are ignored in the objective but whose columns still need to
be processed to compute \cref{eq:partial_induct}).

For each of the \( N \) candidates, it requires \( \BigO(1) \) work per column
for \( s + m \) columns. Over \( s \) selections, the total time complexity
is \( \BigO(s N (s + m)) \) which is dominated by the time to downdate the
Cholesky factor, meaning the partial selection algorithm matches the asymptotic
time complexity of the multiple-point algorithm.
However, the downdate of the Cholesky factor is implemented with
repeated BLAS level-one \texttt{daxpy} operations while the bulk of
left-looking takes place in a BLAS level-two \texttt{dgemv} operation.
Higher level operations often have better constant-factor
performance for the same asymptotic time complexity.
In addition, the objective for the partial selection algorithm is
more complicated to compute than its multiple-point counterpart.

\section{Greedy selection for \emph{global} approximation by KL-minimization}
\label{sec:chol_select}

\begin{figure}[t]
  \centering
  \input{figures/cholesky_factor.tex}%
  % TODO: matching GP image with 13 candidates, 3 selected, 1 training point
  \input{figures/selection/cknn_factor.tex}
  \caption{For a column in isolation, the \textcolor{orange}{unknown} point
    is the diagonal entry, below it are \textcolor{lightblue}{candidates},
    and the \textcolor{seagreen}{selected} entries are added to the sparsity
    pattern \( s_i \). Thus, sparsity selection in Cholesky factorization
    is analogous to point selection in Gaussian processes.}
  \label{fig:select_chol}
\end{figure}

It is remarkably straightforward to apply the suite of selection
algorithms we developed to sparse Cholesky factorization.
As depicted in \cref{fig:select_chol}, for computing
the \( i \)th column of the Cholesky factor, we use the
\( i \)th point in the ordering as the target point.
For the training points, we use all the points satisfying
lower-triangularity, those after the target in the ordering.
Running the selection algorithm for the desired number of nonzeros
picks out indices which we add to the sparsity set \( s_i \).
Finally, once the sparsity pattern is determined, we compute the
entries of the factor column-by-column according to \cref{eq:L_col}.
For \( s \) nonzero entries per column, each column costs \( \BigO(s^3)
\) to compute \( \CM_{s_i, s_i}^{-1} \) for a total time complexity
cost of \( \BigO(N s^3) \) for the \( N \) columns of \( L \).
Using the single-point selection algorithm in \cref{subsec:greedy_select}
costs \( \BigO(C s^2) \) for selecting \( s \) points out of \(
C \) candidates, so \( \BigO(N C s^2) \) over \( N \) columns.
If \( C \) is chosen to be a constant factor bigger than \( s \),
then selection has the same complexity as computing the entries of
\( L \), suggesting the need to limit the candidates considered.
In practice we pick the candidate set as the nearest neighbors
to the point of interest and use the selection algorithm to
subsample within these candidates as \cite{gramacy2014local} does.
Specifically we use the framework of \cite{schafer2021sparse} which selects all
points within a radius of \( \rho \ell_i \) to the \( i \)th point, where \(
\rho \) is a tuning parameter and \( \ell_i \) is a length scale systematically
decreasing with decreasing position in the ordering.

\subsection{Aggregated sparsity pattern}

For an aggregated sparsity pattern as discussed in \cref{subsubsec:aggregated},
the target points become all the points in the group \( \tilde{i} \).
We use the framework of \cite{schafer2021sparse} which
determines the groups by aggregating the points that are
both close geometrically as well as close in the ordering.
The aggregated sparsity pattern is then the union of
the nearest neighbors for each point in the group.
If all the points in the group are adjacent in the ordering or only candidate
indices \( k \) that satisfy \( k \succ \max{\tilde{i}} \) are considered, then
each candidate conditions every point in the group and so the multiple-point
algorithm \cref{subsec:mult_select} can be directly applied.
However, we observe forcing these conditions irreparably damages the
approximation quality of the resulting factor: forcing groups to be
contiguous in the ordering removes the guarantee that points in the
group are geometrically close, and removing candidates between target
points filters a large number of candidates, obviating the benefit of
aggregation in the first place.
If these conditions are not met, then candidates
can condition partial subsets of the group.
Of course, the multiple-point algorithm can still be applied, but it
systematically overestimates the effect of selecting candidate points.
In practice, we also observe a significant decrease in approximation quality,
to the extent that it becomes worse than non-aggregated factorization.
The partial selection algorithm is necessary to be able to use the original
grouping and candidate sets while maintaining approximation quality.

Once the sparsity pattern has been determined, we again need to
compute each column of \( L \) according to \cref{eq:L_col}.
Because the sparsity pattern for a column is a subset of its group's
aggregated sparsity pattern, we can efficiently compute the group's
entries together in the time for a single column, \( \BigO(s^3) \).
See Algorithm 3.2 in \cite{schafer2021sparse} or
\cref{app:L_mult} for a statistical explanation.
If the average group size is \( m \) points, for both the multiple-point
or partial selection algorithms the time complexity of selecting \( s \)
points out of \( C \) candidates is \( \BigO(C s^2 + C m^2 + m^3) \).
Over \( \frac{N}{m} \) groups the total time complexity to do both sparsity
selection and entry computation is \( \BigO(\frac{N}{m} (C s^2 + C m^2 + m^3 +
s^3)) \) which simplifies to \( \BigO(\frac{N C s^2}{m}) \) assuming \( s > m
\), resulting in a \( m \) times improvement over non-aggregated factorization.
This allows the aggregated factorization to produce denser factors in the same
time as the non-aggregated factorization, often leading to better accuracy.
However the resulting factor will be less efficient at reducing the KL
divergence per sparsity entry, since the algorithm is forced to select
the same entry for nearly \emph{all} columns in the group rather than
tailoring the sparsity pattern to a particular column.

\subsection{Allocating nonzeros by global greedy selection}
\label{subsec:global_greedy}

Although columns are essentially independent from the perspective of
KL minimization, if there is a prescribed budget on the total number
of nonzeros, then one must distribute the nonzeros over the columns,
tying the fate of one column to another.
\cite{schafer2021sparse} implicitly determines both the total number of
zeros as well as their distribution over columns by selecting points
within a radius that monotonically decreases earlier in the ordering.
We observe in our numerical experiments as \cite{kang2021correlationbased}
does that this approach of \cite{schafer2021sparse} leads to significantly
worse results than simply allocating the same number of nonzeros per column.
Distributing nonzeros as evenly as possible also maximizes
computational efficiency since computing the entries for a
column scales cubically with the number of nonzeros, denser
columns have an outsized impact on computational time.
In addition, it is simple to implement and adds almost
no overhead, so we recommend allocating the number of
nonzeros divided by the number of columns for each column.

However, one principled way of distributing nonzeros might be to minimize KL
divergence in an end-to-end manner like we did for sparsity pattern selection.
The local greedy selection algorithms select the sparsity entry for a column
or group of columns that minimizes prediction error at those columns.
We generalize this to \emph{global} greedy selection by picking from
\emph{any} of the columns the candidate entry that minimizes the
overall KL divergence \cref{eq:obj_chol} when added to the sparsity.
We maintain a global priority queue over all columns that determines both the
next column to select from as well as the particular entry within the column
to add as a nonzero, keyed by the effect of the entry on the KL divergence.
This priority queue data structure must support popping off the largest
element currently in the queue as well as updating the value for an
element in the queue, given a unique identifier for the element.
Both operations can be done in \( \BigO(\log n) \) for \( n \) elements in the
queue if efficiently implemented as an array-backed binary heap, for example.

The greedy selection algorithms already compute the effect of an
entry on the KL divergence, at least up to monotonically increasing
transformation (which preserves the relative ranking of candidates).
But from the global perspective, if different columns
use different transformations, then the relative
ranking of candidates between columns is skewed.
Thus, we now describe the modifications necessary to compute exactly the
difference in KL divergence without any monotone transformations.

\subsubsection{Single column selection}
\label{subsubsec:single_column}

Selecting an entry \( k \) for a single target point
only affects its conditional variance, so exactly one
term in the KL divergence \cref{eq:obj_chol} changes.
Subtracting the updated variance and original variance,
\begin{align}
  \min \left [
    \log(\CM_{\Pred, \Pred \mid \I, k}) - \log(\CM_{\Pred, \Pred \mid \I})
  \right ]  &=
    \min \frac{\CM_{\Pred, \Pred \mid \I, k}}
              {\CM_{\Pred, \Pred \mid \I}} \\
  \shortintertext{From the original objective and update \cref{eq:obj_gp},}
            &= \min \frac{\CM_{\Pred, \Pred \mid \I}
                          - \CM_{k, \Pred \mid \I}^2/\CM_{k, k \mid \I}}
                         {\CM_{\Pred, \Pred \mid \I}} \\
            &= \min \left [ 1 -
              \frac{\CM_{k, \Pred \mid \I}^2}
                   {\CM_{k, k \mid \I} \CM_{\Pred, \Pred \mid \I}}
               \right ] \\
  \label{eq:global_obj}
            &= \max
              \frac{\CM_{k, \Pred \mid \I}^2}
                   {\CM_{k, k \mid \I} \CM_{\Pred, \Pred \mid \I}}
\end{align}
where the new objective \cref{eq:global_obj} can be interpreted
as the percentage of variance the decrease in variance takes up.
The new objective is easily computed as the original objective
\cref{eq:obj_gp} divided by the target point's conditional variance.

\subsubsection{Aggregated selection}
\label{subsubsec:mult_column}

Since the multiple-point algorithm already computes the difference in log
determinant after selecting the candidate, no modification needs to be made.
The partial selection algorithm computes exactly the log determinant after
selecting the candidate, not the difference, so the log determinant before
the selection needs to be subtracted, which is easily computed as the sum of
the squared ``diagonal'' entries of \( L \) corresponding to target points
in \cref{eq:partial_diag}.

One heuristical improvement is to account for ``bang-for-buck'', that is, to
consider that different candidates cost a different number of nonzero entries.
The number of nonzeros selecting a candidate can add is
between one and the number of columns in its aggregated
group, depending on its position in the ordering.
The KL divergence is a purely additive measure of a candidate's effect
on every point in its group, so candidates with larger groups tend to
decrease the KL divergence more, even if they are not as efficient per
target point as candidates with smaller groups.
In practice it is better to use the objective \( \frac{\Delta}{n} \) where
\( \Delta \) is the change in KL divergence from selecting the candidate
and \( n \) is the number of nonzero entries selecting the candidate adds.

For the multiple-point algorithm this modification makes no difference
within a group since every candidate conditions every target point, but can
make a difference in the global setting if groups have different sizes.
This modification can make a difference both within a column and
globally for the partial algorithm since different candidates
within the same group can condition a different number of targets.

\section{Numerical experiments}
\label{sec:experiments}

All experiments ran on the Partnership for an Advanced Computing Environment
(PACE) Phoenix cluster at the Georgia Institute of Technology, with 8
cores of a Intel Xeon Gold 6226 CPU @ 2.70GHz and 22 GB of RAM per core.
The code is written in Python using standard scientific libraries
\texttt{numpy} \cite{harris2020array}, \texttt{scipy} \cite{virtanen2020scipy},
\texttt{scikit-learn} \cite{pedregosa2011scikitlearn}, \texttt{matplotlib}
\cite{hunter2007matplotlib} as well as Cython \cite{behnel2011cython} which
provides direct transpilation of Python code into C.
Cython also allows Python code to access native C interfaces
to the \texttt{BLAS} and Intel \texttt{oneMKL} libraries.
Code for all numerical experiments can be found at
\href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\subsection{\textit{k}-nearest neighbors selection}
\label{subsec:knn_exp}

We directly compare \( k \)-nearest neighbors (\( k \)-NN) to our selection
algorithm, which we will call conditional \( k \)-nearest neighbors (C\( k
\)-NN), by comparing classification accuracy in the following toy example.
From the MNIST database of handwritten digits
\cite{lecun1998gradientbased}, we randomly select 1000 images
to form the training set and 100 to form the testing set.
For each image in the testing set, we select the \( k \) ``closest''
training images to it using either selection method and make a prediction
by the mode (most frequently occurring) label in the selected images.
For \( k \)-NN, we use the standard Euclidean distance and
for C\( k \)-NN, we use a Mat{\'e}rn kernel with smoothness
\( \nu = \frac{3}{2} \) and length scale \( \ell = 2^{10} \).
Accuracy is measured by the number (or
percentage) of test images classified correctly.

As seen in \cref{fig:mnist}, the accuracy of both
methods degrades linearly with increasing \( k \).
We hypothesize that nearby images are more likely
to have the same label as a given test image.
Forcing the algorithm to select more points therefore increases the likelihood
that the algorithm becomes confused by far away, differently labeled images.
However, C\( k \)-NN is more accurate than \( k \)-NN
for every \( k > 2 \), suggesting that conditional
selection consistently selects more informative points.
We emphasize that the difference in accuracy results solely from conditioning:
because the Mat{\'e}rn kernel degrades monotonically with distance,
sorting by unconditional covariance is identical to sorting by distance.
In addition, we use the mode to summarize the labels of the selected
points, rather than performing Gaussian process classification.
The difference in accuracy results exactly
from the methods selecting different points.

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed},
    {very thick, orange,    style=solid},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/mnist/accuracy_k.tex}%
  \input{figures/mnist/time_k.tex}
  \caption{
    We compare selection by Euclidean distance (\( k \)-NN) to conditional
    selection using a Mat{\'e}rn kernel (C\( k \)-NN) on image classification.
    From the MNIST database, \( N = 1000 \) training images and \( m = 100
    \) testing images are randomly chosen; each test image is classified by
    taking the mode label of the \( k \) selected training images, and this
    process is repeated 100 times for each value of \( k \).
    On the left, C\( k \)-NN is more accurate
    than \( k \)-NN for every \( k > 2 \).
    On the right, the time to select \( k \) points using C\( k \)-NN
    seems to scale linearly with \( k \) although its asymptotic
    complexity is quadratic; this is a result of performing highly
    optimized BLAS operations on relatively small matrices.
  }
  \label{fig:mnist}
\end{figure}

\subsection{Recovery of sparse Cholesky factors}
\label{subsec:recover_exp}

Motivated by the similarity of the selection algorithm to orthogonal
matching pursuit \cite{tropp2007signal}, we experiment with its sparse
recovery properties by attempting to recover a randomly generated
\textit{a priori} sparse Cholesky factor \( L \) solely from the inner
products \( \CM = L L^{\top} \).
For each column of \( L \), we uniformly randomly pick \( s \)
entries from those that satisfy lower triangularity to make nonzero.
We sample their values i.i.d. from the standard normal \( \N(0, 1) \).
Finally, we fill \( L \)'s diagonal with a ``large'' positive value
(10) to ensure both the resulting covariance matrix \( \CM \) and
precision matrix \( \CM^{-1} \) are reasonably well-conditioned.
The various selection algorithms are given the target number of nonzeros per
column \( s \) and either the covariance matrix \( \CM \) or the precision
matrix \( \CM^{-1} \) depending on which results in higher accuracy, and are
asked to reconstruct the sparse Cholesky factor \( L \).
Accuracy is measured by taking the cardinality of the intersection of
the recovered sparsity set with the ground truth sparsity set over the
cardinality of their union, which we call intersection over union (IOU).
Measuring accuracy by the KL divergence between \( \Theta \) and \(
(\hat{L} \hat{L}^{\top})^{-1} \) where \( \hat{L} \) is the optimal
inverse Cholesky factor following the recovered sparsity as computed
by \cref{eq:L_col} was found to be mostly equivalent to IOU.

As seen in \cref{fig:recover_acc}, C\( k \)-NN retains its near-perfect
recovery accuracy while the rest of the methods quickly degrade and
asymptote to their final accuracies with an increasing number of rows
and columns of \( L \) for a fixed \( s = 32 \) nonzeros per column.
However, if the number of columns is fixed at \( N = 256 \) and \( s
\) is increased instead, all methods drop in accuracy with increasing
density until a tipping point where the problem starts to become easier.
Accuracy then increases until the Cholesky factor
becomes fully dense, when perfect recovery is trivial.
The C\( k \)-NN strategy follows this pattern, but maintains
much higher accuracy than the rest of the strategies.

In the setting of noisy measurements, noise sampled i.i.d from \(
\N(0, \var) \) is added to each entry of \( \CM \) symmetrically
(i.e. \( \CM_{ij} \) receives the same noise as \( \CM_{ji} \)).
Accuracy slightly degrades with increasing noise for all
methods, but C\( k \)-NN seems to be the most sensitive
to noise as can be seen in \cref{fig:recover_noise}.
At high levels of noise \( \CM \) can lose positive-definiteness,
which causes C\( k \)-NN to break down entirely.

An important setting where \( \CM \) (and possibly \( L \)) are known
to be \textit{a priori} sparse is when solving Laplacian systems.
In exploratory numerical experiments we found that the factors
generated by our method were unreasonably dense compared the sparsity
of the Laplacian matrix, a manifestation of \emph{fill-in}.
We comment that our method is designed for \emph{dense} kernel matrices
resulting from a set of points and a ``well-behaved'' kernel function.
For solving sparse Laplacian systems, a method designed
to exploit the sparsity and graph structure of Laplacian
matrices like \cite{kyng2016approximate} is more appropriate.

\pgfplotsset{
  cycle list={
    {very thick, silver,    style=densely dotted},
    {very thick, lightblue, style=dashed},
    {very thick, seagreen,  style=dashdotted},
    {very thick, orange,    style=solid},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/recover/accuracy_n.tex}%
  \input{figures/recover/accuracy_s.tex}
  \caption{
    We attempt to recover sparse Cholesky factors \( L \)
    from their covariance matrix \( \Theta = L L^{\top} \).
    ``C\( k \)-NN'' minimizes the conditional variance of the target (diagonal)
    entry, ``\( k \)-NN'' maximizes covariance with the target entry,
    ``corr.'' maximizes correlation with the target \cref{eq:obj_gp} without
    conditioning, and ``rand.'' randomly samples entries uniformly.
    All methods achieve the highest accuracy when given
    the covariance matrix \( \CM \) except for C\( k \)-NN
    which is given the precision matrix \( \CM^{-1} \).
    The left panel shows accuracy with increasing size of \( L \) with a
    fixed number of nonzeros per column \( s = 32 \) while the right panel
    shows accuracy with increasing \( s \) and fixed size \( N = 256 \).
  }
  \label{fig:recover_acc}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/recover/accuracy_noise.tex}%
  \input{figures/recover/accuracy_noise_cknn.tex}
  \caption{
    We experiment with adding noise sampled i.i.d.
    from \( \N(0, \var) \) to \( \Theta \) entrywise.
    On the left is accuracy with increasing noise for \( N
    = 1024 \) columns and \( s = 32 \) nonzeros per column.
    On the right is accuracy for the C\( k \)-NN method at various
    noise levels for \( N = 256 \) and increasing \( s \).
  }
  \label{fig:recover_noise}
\end{figure}

\subsection{Cholesky factorization}
\label{subsec:chol_exp}

We empirically verify that greedy selection minimizes KL divergence in
Cholesky factorization compared to selection by Euclidean distance.
We sample \( N \) points uniformly randomly from the unit hypercube in \(
D = 3 \) dimensions and use a Mat{\'e}rn kernel with smoothness \( \nu =
\frac{5}{2} \) and length scale \( \ell = 1 \), implicitly determining the
covariance matrix \( \Theta \).
As a baseline for comparison, we use both the single-column and aggregated
variants of the KL-minimization framework of \cite{schafer2021sparse}, which
uses the reverse-maximin ordering and selects sparsity entries by selecting
points within a radius that decreases earlier in the ordering.
The density of the factor is tuned by the hyperparameter \( \rho \), increasing
\( \rho \) increases the radius and therefore the number of nonzeros.
We compare with a simpler \( k \)-nearest neighbors approach where
\( k \) is chosen to match the density of the baseline factor.
For our method, we run the baseline \cite{schafer2021sparse} with a larger
\( \rho' = \rho \cdot \rho_s \) to get an initial candidate set where \(
\rho_s \) is a hyperparameter controlling the number of candidates considered.
If not stated otherwise, \( \rho_s = 2 \) is used in the following experiments.
We then use both the single-column and aggregated variants of the
conditional selection algorithm described in \cref{sec:chol_select}
to subsample the actual sparsity entries from this candidate set,
in these experiments, each column gets the same number of nonzeros.
In exploratory numerical experiments, we found using the global
selection procedure in \cref{subsec:global_greedy} to determine
the number of nonzeros for each column led to little improvement
in accuracy at a significant performance penalty.
We use the same aggregation procedure for conditional selection as the
baseline aggregated factor and in all experiments we use an aggregation
parameter of \(\lambda = 1.5 \) as recommended by \cite{schafer2021sparse}.

As \cref{fig:chol_n} shows, the KL divergence and running time
increases linearly with the number of points for all methods.
Conditional methods are more accurate their unconditional counterparts
and aggregated variants are both more accurate and faster than their
non-aggregated counterparts; however the aggregated factors are denser,
which may impact their computational efficiency in downstream tasks.

As \cref{fig:chol_rho} shows, the conditional selection methods achieve
significantly better KL divergence for the same number of sparsity entries.
Part of that improvement is from spreading the number of nonzeros
evenly over the columns as shown by the performance of \( k \)-NN.
The conditional methods achieve better accuracy per computational cost than
the baseline, but the simple method of \( k \)-NN remains hard to beat.

Finally, we experiment with increasing the size of the candidate set \(
\rho_s \) but \emph{without} increasing the density of the factor \( \rho \).
Since the conditional selection methods scale linearly with the number of
candidates, increasing \( \rho_s \) is much cheaper than increasing \( \rho \).
As shown in \cref{fig:chol_s}, increasing \( \rho_s \)
has diminishing returns past \( \rho_s \approx 4 \).

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed,         mark=*},
    {very thick, seagreen,  style=dashdotted,     mark=o},
    {very thick, silver,    style=densely dotted, mark=triangle*},
    {very thick, orange,    style=solid,          mark=square*},
    {very thick, rust,      style=dotted,         mark=square},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/n_kl_div.tex}%
  \input{figures/cholesky/n_time.tex}
  \caption{
    Performance of various Cholesky factorization
    methods with increasing number of points.
    ``KL'': baseline from \cite{schafer2021sparse}, ``(agg.)'' denotes
    aggregated version, ``select (\( k \)-NN)'' is selection by \(
    k \)-nearest neighbors, ``select'' is conditional selection.
    The density is fixed at \( \rho = 2 \).
    The left panel shows KL divergence and the right
    panel shows time with an increasing number of points.
  }
  \label{fig:chol_n}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/rho_kl_div.tex}%
  \input{figures/cholesky/rho_time.tex}
  \caption{
    The left panel shows the KL divergence with increasing
    density \( \rho \) and the right panel shows the time to
    accuracy trade-off over different values of \( \rho \).
    The number of points is \( N = 65536 \).
  }
  \label{fig:chol_rho}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/s_kl_div.tex}%
  \input{figures/cholesky/s_time.tex}
  \caption{
    We increase the number of candidates considered, \(
    \rho_s \), with the density of the factor fixed at \(
    \rho = 4 \) and the number of points at \( N = 65536 \).
    The left panel shows KL divergence with an increasing number of candidates
    considered, the right shows the corresponding increases in time.
  }
  \label{fig:chol_s}
\end{figure}

\subsection{Gaussian process regression}
\label{gp_exp}

For Gaussian process regression we use the ``predictions points first'' method
of \cite{schafer2021sparse} which seeks to compute a sparse Cholesky factor
of the joint covariance matrix between the training and prediction points.
In this Cholesky factor, prediction points are placed before training points
in the reverse ordering (afterwards in the forward ordering), conditioning
them in the factorization from the statistical perspective of \cref{eq:chol}.
For a covariance matrix blocked as
\( \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\)
and a Cholesky factor of its precision
\( L =
    \begin{pmatrix}
      L_{1, 1} & 0 \\
      L_{2, 1} & L_{2, 2}
    \end{pmatrix}
\),
\( L L^{\top} = \CM^{-1} \), then the posterior distribution can be read off as
\( \E[\vec{y}_1 \mid \vec{y}_2] = -L_{1, 1}^{-\top} L_{2, 1}^{\top} \vec{y}_2
\) and \( \Cov[\vec{y}_1 \mid \vec{y}_2] = L_{1, 1}^{-\top} L_{1, 1} \).
These quantities can be computed efficiently since
they involve only the sparse submatrices of \( L \).

In order to empirically test the performance of the method
we sample 65536 points uniformly randomly from the unit
hypercube in \( D = 3 \) dimensions and split these points into a
train-test-split of 90\% training points and 10\% testing points.
We use a Mat{\'e}rn kernel with smoothness \( \nu =
\frac{5}{2} \) and length scale \( \ell = 1 \) and draw
1000 realizations from the resulting Gaussian process.
We consider three accuracy metrics for the Gaussian process regression
problem: the log determinant of the testing points' posterior covariance
matrix, the empirical 90\% coverage computed from the posterior mean and
variance averaged over all realizations, and the root mean square error
(RMSE) of the prediction averaged over all realizations.
The log determinant is equivalent to the KL divergence by the
discussion in \cref{subsubsec:aggregated}, so the results are
nearly identical to the ones shown in \cref{fig:chol_rho}.
We find that for our particular geometry and kernel function coverage is
extremely accurate for all methods (within \( 0.1\% \) for \( \rho \geq 2 \)).
Finally, the RMSE is shown in \cref{fig:gp_rho}.
Unlike the KL divergence, aggregated variants are indistinguishable
in accuracy from their non-aggregated counterparts.
Despite an increased cost to select points, the conditional methods have
better accuracy per unit computational cost than their unconditional
counterparts as a result of their superior accuracy at the same sparsity.

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed,         mark=*},
    {very thick, seagreen,  style=dashdotted,     mark=o},
    {very thick, orange,    style=solid,          mark=square*},
    {very thick, rust,      style=dotted,         mark=square},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/gp/rho_loss.tex}%
  \input{figures/gp/rho_time_loss.tex}
  \caption{
    We perform Gaussian process regression by sparse
    Cholesky factorization of the joint covariance matrix.
    Like Cholesky factorization in \cref{subsec:chol_exp},
    we use a candidate size scaling factor of \( \rho_s = 2
    \) and an aggregation parameter of \( \lambda = 1.5 \).
    The left panel shows the difference in RMSE from
    exact Gaussian process regression using the same
    training points with increasing density \( \rho \).
    The right panel shows the time to accuracy
    trade-off over different values of \( \rho \).
  }
  \label{fig:gp_rho}
\end{figure}

\subsection{Preconditioning for conjugate gradient}
\label{subsec:cg_exp}

Motivated by the equivalence of functionals like the Kaporin condition number
to the KL divergence, we investigate solving symmetric positive-definite
systems \( \CM \vec{x} = \vec{y} \) using the conjugate gradient and a sparse
Cholesky factor \( L \) as a preconditioner.
We note that from \cref{eq:kl} the KL divergence strongly
penalizes zero eigenvalues of the preconditioned matrix
\( \CM L L^{\top} \), improving its condition number.
In order to generate the covariance matrix \( \CM \) we sample up to \( N
= 32768 \) points from the unit hypercube in \( D = 3 \) dimensions with a
Mat{\'e}rn kernel with smoothness \( \nu = \frac{1}{2} \) and length scale
\( \ell = 1 \).
In exploratory numerical experiments, we found higher smoothnesses
like \( \nu = \frac{5}{2} \) led to high numerical instability and
extremely poor condition (thousands of iterations to converge).
Increasing length scale also worsens the
condition but to a less extreme extent.
Rather than generate a right hand side \( \vec{y} \) directly, we first
sample a solution \( \vec{x} \sim \N(\vec{0}, \Id_N) \) and then compute
\( \vec{y} = \CM \vec{x} \) so \( \vec{y} \) is realistically smooth.
When computing the preconditioner \( L \) by sparse Cholesky factorization,
unless stated otherwise, we use a candidate size scaling factor of \(
\rho_s = 2 \) and an aggregation parameter of \( \lambda = 1.5 \).
We then run conjugate gradient iterations with \( L \) as a
preconditioner until a relative tolerance of \( 10^{-12} \) is reached.

As shown in \cref{fig:cg_iter}, the conditional methods converge
in half the iterations of their unconditional counterparts.
Aggregation seems not to reduce the number of iterations,
but does make each iteration slightly slower since denser
aggregated factors result in slower matrix-vector products.
The minimum number of nonzeros per column for the
conjugate gradient to converge within 50 iterations
seems to grow logarithmically with the number of points.
Although this logarithmic growth likely holds for the conditional
methods, it appears near constant due to the slow growth.

We observe a characteristic ``U'' shape for the total wall-clock
time (the total time to form the preconditioner and for
the conjugate gradient to converge) in \cref{fig:cg_rho}.
If the preconditioner is too sparse, the conjugate gradient dominates
the running time, and if the preconditioner is too dense, it can be
expensive to form without significantly reducing the number of iterations.
We note there always exists a situation where more accurate methods for the
same sparsity have better wall-clock time by simply increasing the number of
iterations, e.g. by demanding better tolerance or using matrices with worse
condition (e.g. Mat{\'e}rn kernels with higher smoothness or length scale).
The only way to have better wall-clock time in \emph{every} situation is to be
both more accurate and faster, which is not achieved by conditional selection,
so it is hard to compare the conditional methods to the unconditional methods.
We do observe that aggregated variants perform worse than their non-aggregated
counterparts due to barely reducing the number of iterations while resulting
in significantly denser factors, leading to slower matrix-vector products.
The optimal density for conditional methods is sparser than their
unconditional counterparts due to less iterations at the same
sparsity and selecting nonzeros being more expensive to compute.

\begin{figure}[t]
  \centering
  \input{figures/cg/n_iter-res.tex}%
  \input{figures/cg/nnz_nnz.tex}
  \caption{
    We use the conjugate gradient preconditioned with
    sparse Cholesky factors to solve the symmetric
    positive-definite system \( \CM \vec{x} = \vec{y} \).
    The left panel shows iteration progress for \( N =
    32768 \) points and a factor density of \( \rho = 4 \).
    The right panel shows the minimum number of nonzeros
    per column for conjugate gradient to converge within
    50 iterations with an increasing number of points.
  }
  \label{fig:cg_iter}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cg/rho_iter.tex}%
  \input{figures/cg/rho_time.tex}
  \caption{
    The left panel shows how iterations of the conjugate gradient decrease with
    increasing preconditioner density \( \rho \) for \( N = 32768 \) points.
    The right panel shows the corresponding total wall-clock time (both to
    compute the preconditioner and to converge) with increasing density.
  }
  \label{fig:cg_rho}
\end{figure}

\subsection{Comparison to other methods}
\label{subsec:compare}

In this section we compare our selection algorithm and
resulting Cholesky factorization procedure to existing work.

\paragraph{Local approximate Gaussian process (laGP)}

Our conditional variance objective for point selection \cref{eq:obj_gp} was
first described in \cite{cohn1996neural} for optimal experimental design,
which has subsequently been named the active learning Cohn (ALC) objective.
\cite{gramacy2014local} and the follow up work \cite{gramacy2015speeding}
apply the ALC objective to directed Gaussian process regression, yielding
an algorithm equivalent to ours in the case of a single point of interest.
We note that their definition of conditional variance in equation 5 is
analogous to our equation \cref{eq:cond_cov}, equating their conditional
variance objective in equation 8 to our equation \cref{eq:obj_gp} (the
connection is explicitly stated in SM\S4's equation SM.22).
We also note that they update the precision after a new
point is selected through the blocked equations in SM\S1
like our \cref{app:prec_insert} and \cref{alg:select_prec}.

For inference at multiple points of interest, \cite{gramacy2014local}
seems to only consider direct parallelization of the
single-point algorithm to each of the target points.
They mention that the \texttt{laGP} function in their \texttt{R}
package jointly considers target points if multiple input values
are provided, although without providing details on this procedure.
We generalize the ALC objective to multiple prediction points through
the log determinant of the covariance matrix \cref{eq:greedy_mult}
and provide an explicit algorithm \cref{alg:select_mult_chol}.
In addition, integration of the selection algorithm into
Cholesky factorization provides a global decomposition
of the joint training and prediction covariance matrix.

\paragraph{Orthogonal matching pursuit (OMP)}

Our selection algorithm can be viewed as the covariance equivalent
of the sparse signal recovery algorithm orthogonal matching
pursuit (OMP) \cite{tropp2007signal, tropp2006algorithms}.
OMP measures the approximation of a target signal by its residual after
orthogonal projection onto the subspace of chosen training signals,
which is efficiently calculated by maintaining a QR factorization.
In contrast, our selection algorithm uses a kernel function to evaluate inner
products, so orthogonalization becomes conditioning, the residual norm
becomes variance, and the QR factorization becomes a Cholesky factorization.
A major difference from OMP is that the computational time of conditioning
dominates that of evaluating the kernel function since the feature space
is relatively low-dimensional (often 2 or 3 in spatial statistics).

\paragraph{Sparse Cholesky factorization}

Our proposed sparse Cholesky factorization algorithm is most similar
to \cite{schafer2021sparse} and \cite{kang2021correlationbased}.
We note using \( k \)-nearest neighbors to select the sparsity set
instead of our proposed conditional selection algorithm essentially
reduces to \cite{schafer2021sparse} and that using the correlation
objective \cref{eq:obj_gp} but without conditioning on previously
selected points reduces to \cite{kang2021correlationbased}.

\section{Conclusions}
\label{sec:conclusion}

We have shown that accounting for conditioning while selecting sparsity
entries for sparse Cholesky factors significantly improves accuracy and
performance in downstream tasks compared to selection by Euclidean distance.
Not only is conditional selection computationally efficient,
it can be extended to the settings of multiple-point and
partial conditioning at no additional asymptotic time cost.
In addition, conditional selection gives a principled
way of deciding the number of nonzeros per column of the
Cholesky factor by greedy selection over all columns.
We support these claims by extensive numerical
experimentation for a variety of problems.

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{cholesky}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Derivations in KL-minimization}

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL divergence between \( \CM \) and the optimal
sparse approximate Cholesky factor \( L \) computed from \cref{eq:L_col}.
From \cref{eq:kl} and letting \( \Delta = 2
\KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} \),
\begin{align}
  \label{eq:kl_L}
  \Delta &= \trace(L L^{\top} \CM) - \logdet(L L^{\top}) - \logdet(\CM) - N
  \shortintertext{
    Focusing on the term \( \trace(L L^{\top} \CM) = \trace(L^{\top} \CM L)
    \) by the cyclic property of trace and using the sparsity of \( L \) by
    plugging in the definition \cref{eq:L_col} for each column \( L_{s_i, i}
    \),
  }
  \trace(L^{\top} \CM L) &= \sum_{i = 1}^N
    L_{s_i, i}^{\top} \CM_{s_i, s_i} L_{s_i, i} \\
  &= \sum_{i = 1}^N
    \left (
      \frac{\left ( \CM_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \CM_{s_i, s_i}
    \left (
      \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right ) \\
  &= \sum_{i = 1}^N
    \frac{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
          \CM_{s_i, s_i} \CM_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}
  = \sum_{i = 1}^N 1 = N
  \shortintertext{
    exactly the Jacobi scaling constraint \( \diag(L^{\top} \CM L) = 1 \).
    Substituting back into \cref{eq:kl_L},
  }
  \Delta &= -\logdet(L L^{\top}) - \logdet(\CM)
  \shortintertext{
    Computing the determinant of a triangular matrix as the
    product of its diagonal entries and plugging in the
    definition \cref{eq:L_col} for the diagonal entries,
  }
  &= -2 \sum_{i = 1}^N \left [ \log(L_{i, i}) \right ] - \logdet(\CM) \\
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\CM) \\
  &= \sum_{i = 1}^N
    \left [
      \log
      \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
  \shortintertext{
    Applying \cref{eq:L_cond_var} to turn each term \(
    (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1} \)
    into a variance \( \CM_{i, i \mid s_i \setminus \{ i \}} \),
  }
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
    \right ]
    - \logdet(\CM) \\
  \shortintertext{
    Expanding the log determinant by the chain rule \cref{eq:det_chain},
  }
  &= \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right ) -
    \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid i + 1:} \right ) \\
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right ) -
      \log \left ( \CM_{i, i \mid i + 1:} \right )
    \right ]
\end{align}
We can interpret this sum as the accumulated \emph{difference} in error for
a series of independent prediction problems: each to predict the \( i \)th
variable given a subset of the variables after it in the ordering, \( i + 1,
i + 2, \dotsc, N \).
The left term \( \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
\) is restricted to the variables in the sparsity pattern \( s_i \), while the
right term \( \log \left ( \CM_{i, i \mid i + 1:} \right ) \) is unconstrained.
As a result, the left term will necessarily be greater than the
right, and the goal is to minimize the accumulated deviation.
Thus, the KL divergence measures the maintenance of predictive accuracy after
forcing conditional independence (sparsity) on the original distribution.
This independent regression problem interpretation of the KL
divergence is also given in equation 5 of \cite{katzfuss2022scalable}.

The asymmetry of KL divergence implies the order of the matrices matters as
well as whether both matrices have been inverted or not. This seems to imply
that there are four possible ways to compare two covariance matrices. However,
note that
\begin{align}
  \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \KL*{\N(\vec{0}, \CM_2^{-1})}{\N(\vec{0}, \CM_1^{-1})}
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL divergence.
There are therefore only two possible ways to
compare the two matrices, depending on their order.
From a statistical perspective the KL divergence can interpreted as the
likelihood-ratio test, so the asymmetry in order corresponds to the asymmetry
between the null and alternative hypotheses \cite{eguchi2006interpreting}.

\subsection{Partial updates in the selection algorithm}
\label{app:partial}

We have the collection of Gaussian random variables \( \vec{y} = [y_1, \dotsc,
y_m]^{\top} \) corresponding to the ordered column indices \( i_1 \succ
\dotsb \succ i_m \) in the context of aggregated Cholesky factorization.
We then select an index \( k \) that conditions the variables, but ignores
the first \( p \) variables (the indices are sorted in decreasing order,
so if \( k \) conditions a variable, it conditions everything afterwards).
After this partial conditioning we denote the updated variables as
\( \vec{y}_{\parallel k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k},
\dotsc, y_{m \mid k} \) and wish to compute the covariance matrix \(
\Cov[\vec{y}_{\parallel k}] \).
We know the original variables \( \vec{y} \) have joint density multivariate
Gaussian according to some covariance matrix \( \CM \), \( \vec{y} \sim
\N(\vec{0}, \CM) \), and the fully conditional variables \( \vec{y}_{\mid k}
\) have closed-form posterior distribution according to \cref{eq:cond_cov},
\( \vec{y}_{\mid k} \sim \N(\vec{\mean}, \CM - \CM_{:, k} \CM_{k, k}^{-1}
\CM_{k, :}) \) for some posterior mean \( \vec{\mean} \).
For unconditioned \( y_i \) and \( y_j \),
their covariance is simply \( \CM_{i, j} \).
Similarly, for conditioned \( y_{i \mid k} \) and \( y_{j
\mid k} \), their covariance is \( \CM_{i, j \mid k} \).
The only unknown is the covariance between unconditioned
\( y_i \) and conditioned \( y_{j \mid k} \).
Taking the Cholesky factorization of both covariance matrices,
let \( L = \chol(\CM) \) and \( L' = \chol(\CM_{:, : \mid k}) \).
We can then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z}
\) is distributed according to standard normal, \( \N(\vec{0}, \Id) \).
Similarly, \( \vec{y}_{\mid k} = L' \vec{z} + \vec{\mean} \).
By definition,
\begin{align}
  \Cov[y_i, y_{j \mid k}] &=
    \E[(y_i - \E[y_i])(y_{j \mid k} - \E[y_{j \mid k}])] \\
  &= \E[(L_i \vec{z}) ({L'}_j \vec{z} + \mu_j - \mu_j)] \\
  &= \E[(L_{i, 1} z_1 + \dotsb + L_{i, m} z_m)
        ({L'}_{j, 1} z_1 + \dotsb + {L'}_{j, m} z_m)]
  \shortintertext{
    For \( i \neq j \), \( \E[z_i z_j] = \E[z_i] \E[z_j] = 0 \)
    since \( z_i \) is independent of \( z_j \) and has mean 0.
  }
  &= \E[L_{i, 1} {L'}_{j, 1} z_1^2 + \dotsb +
        L_{i, m} {L'}_{j, m} z_m^2] \\
  &= L_{i, 1} {L'}_{j, 1} \E[z_1^2] + \dotsb +
     L_{i, m} {L'}_{j, m} \E[z_m^2]
  \shortintertext{
    For any \( i \), \( \E[z_i^2] = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \).
  }
  &= L_{i, 1} {L'}_{j, 1} + \dotsb + L_{i, m} {L'}_{j, m} \\
  &= L_i \cdot {L'}_j
  \shortintertext{Thus, the new covariance matrix can be written as}
  \Cov[\vec{y}_{\parallel k}] &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}

\subsection{Aggregated computation of sparsity entries}
\label{app:L_mult}

We wish to compute the entries of the sparse Cholesky factor
\( L_{s_i, i} \) according to \cref{eq:L_col} in the aggregated
sparsity setting of \cref{subsubsec:aggregated}.
Recall that we have aggregated the column indices \( i_1 \succ
\dots \succ i_m \) into \( \tilde{i} = \{ i_1, \dotsc, i_m \} \)
and \( s_{\tilde{i}} \) denotes the aggregated sparsity pattern.
The sparsity pattern for the \( i \)th column of the group is all
those entries that satisfy lower-triangularity, \( s_i \defeq \{
j \in s_{\tilde{i}} : j \succeq i \} \subseteq s_{\tilde{i}} \).
Because each column's sparsity pattern is a subset of the
overall sparsity pattern, it is possible to compute an outer
approximation and specialize to each column efficiently.
Assuming the sparsity entries \( s_{\tilde{i}} \) are sorted according to
\( \prec \), let \( Q \defeq \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) be
the precision of the aggregated sparsity pattern and let \( k \) be the \(
i \)th column's index in \( s_{\tilde{i}} \).
Because \( s_{\tilde{i}} \) is sorted according to \(
\prec \), the sparsity pattern for the \( i \)th column
\( s_i \) is exactly the entries \( k \) and after.
\begin{align}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}} \\
  \shortintertext{
    Since \( Q^{-1} = \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) and \(
    s_i \) is the \( k \)th index of \( s_{\tilde{i}} \) and after,
  }
  &= \frac{\left (Q^{-1} \right )_{k:, k:}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \left (Q^{-1} \right )_{k:, k:} \vec{e}_1}} \\
  \shortintertext{
    By \cref{eq:inverse_cond}, we can turn marginalization
    in precision into conditioning in covariance.
  }
  \label{eq:L_precision}
             &= \frac{Q_{k:, k: \mid :k - 1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} Q_{k:, k: \mid :k - 1} \vec{e}_1}}
\end{align}
So we want the \( k \)th column of \( Q
\), conditional on all columns before it.
From \cref{eq:chol}, this can be directly read off the \(
k \)th column of the Cholesky factor \( L = \chol(Q) \) to
compute \cref{eq:L_col} for each \( i \in \tilde{i} \).
However, computing \( Q = \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) by
inverting \( \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) and then additionally
computing its Cholesky factor \( L = \chol(Q) \) is a bit wasteful.

Instead of computing a lower-triangular factor for the precision, we can
compute an \emph{upper}-triangular factor for the covariance whose inverse
transpose will be a \emph{lower}-triangular factor for the precision.
Using the same flipping trick as \cref{eq:U_chol}, let \( U = P^{\Reverse}
\chol(P^{\Reverse} \CM_{s_{\tilde{i}}, s_{\tilde{i}}} P^{\Reverse})
P^{\Reverse} \) where \( P^{\Reverse} \) is the order-reversing permutation;
\( U \) is upper-triangular and satisfies \( U U^{\top} = \CM_{s_{\tilde{i}},
s_{\tilde{i}}} \) so \( U^{-\top} U^{-1} = \CM_{s_{\tilde{i}},
s_{\tilde{i}}}^{-1} = Q \) and we see that \( L = U^{-\top} \) is a
lower-triangular Cholesky factor satisfying \( L L^{\top} = Q \).
Thus we can compute \( U \) as a Cholesky factor of the covariance and
dynamically form its inverse transpose by solving the triangular system
\( L_{:, k} = U^{-T} \vec{e}_k \) for the desired column \( k \) of \(
L \) (like \cref{eq:obj_chol}, \( \vec{e}_k \) is the vector with \( k
\)th entry one and rest zero).

\section{Computation in sparse Gaussian process selection}

\subsection{Mutual information objective}
\label{app:mutual_info}

The mutual information or information gain between two collections of
random variables \( \vec{y}_\Pred \) and \( \vec{y}_\Train \) is defined as
\begin{align}
  \label{eq:info}
  \MI[\vec{y}_\Pred;\vec{y}_\Train] &= \entropy[\vec{y}_\Pred] -
    \entropy[\vec{y}_\Pred \mid \vec{y}_\Train]
\end{align}
Maximizing the mutual information is equivalent to minimizing the
conditional entropy since the entropy of \( \vec{y}_\Pred \) is constant.
Because the differential entropy of a multivariate Gaussian is monotonically
increasing with the log determinant of its covariance matrix, minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix.
For a single predictive point, its log determinant is simply its variance.
Thus, maximizing mutual information minimizes the
\emph{conditional variance} of the target point.
In particular, because our estimator is the conditional
expectation \cref{eq:cond_mean}, it is unbiased because \(
\E[\E[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[\vec{y}_\Pred] \).
Because it is unbiased, its expected mean squared error is simply the
conditional variance since \( \E[(\vec{y}_\Pred - \E[\vec{y}_\Pred
\mid \vec{y}_\Train])^2 \mid \vec{y}_\Train] = \Var[\vec{y}_\Pred
\mid \vec{y}_\Train] \) where the outer expectation is taken under
conditioning because of the assumption that \( \vec{y}_\Pred \) is
distributed according to the Gaussian process.
So maximizing the mutual information is equivalent to minimizing
the conditional variance which is in turn equivalent to
minimizing the expected mean squared error of the prediction.
Another perspective arises from comparing the definition of mutual
information \cref{eq:info} to the EV-VE identity \cref{eq:eve},
\begin{align}
  \label{eq:info_eve}
  \textcolor{darkorange}{\entropy[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\Pred \mid \vec{y}_\Train]} +
    \textcolor{rust}{\MI[\vec{y}_\Pred;\vec{y}_\Train]} \\
  \label{eq:eve}
  \textcolor{darkorange}{\Var[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\Pred \mid \vec{y}_\Train]]}
\end{align}
On the left hand side, entropy is monotone with variance.
On the right hand side, the expectation of conditional
variance is monotone with conditional entropy.
Because the sum of the two terms on the right hand side is constant,
minimizing the expectation of conditional variance is equivalent to
maximizing the variance of conditional expectation, which corresponds
to the mutual information.

Supposing \( \vec{y}_\Pred \) was independent of \( \vec{y}_\Train
\), then the predictor \( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)
reduces to the constant \( \E[\vec{y}_\Pred] \) whose variance is 0.
Meanwhile, the error \( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]
\) becomes \( \E[\Var[\vec{y}_\Pred]] = \Var[\vec{y}_\Pred] \).
On the other extreme, supposing \( \vec{y}_\Pred \) was a deterministic
function of \( \vec{y}_\Train \), then \( \Var[\E[\vec{y}_\Pred \mid
\vec{y}_\Train]] = \Var[\vec{y}_\Pred] \) while the error term becomes
\( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[0] = 0 \).
Thus, the variance of conditional expectation is the information shared
between \( \vec{y}_\Pred \) and \( \vec{y}_\Train \), as it increases
when the predictor for \( \vec{y}_\Pred \) (the conditional expectation
\( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)) depends on the observed
results of \( \vec{y}_\Train \).

\subsection{Updating precision after insertion}
\label{app:prec_insert}

Assuming we have the precision matrix \( \CM_{1, 1}^{-1} \), we wish
to compute the precision of the covariance \( \CM_{1, 1} \) with a
new row and column added to it, that is, compute \( \CM^{-1} \) for
\(
  \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\)
where \( \CM_{1, 2} = \CM_{2, 1}^{\top} \) is a
column vector and \( \CM_{2, 2} \) is a scalar.
Using the same block \( L D L^{\top} \) factorization as \cref{eq:chol}
where the Schur complement is denoted \textcolor{lightblue}{\(
\CM_{2, 2 \mid 1} \)} from \cref{eq:cond_cov_notation},
\begin{align}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{Inverting both sides of the equation,}
  \CM^{-1} &=
  \begin{pmatrix}
    \Id & -\textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & 0 \\
    -\textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \CM_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry \( k \) to the matrix,
    \( \CM_{1, 1} = \CM_{\I, \I} \), \( \CM_{1, 2} = \CM_{\I, k} \), and \(
    \CM_{2, 2} = \CM_{k, k} \). Also note that \( \textcolor{lightblue}{\CM_{k,
    k \mid \I}^{-1}} \) is the precision of \( k \) conditional on the entries
    in \( \I \), which has already been computed in \cref{alg:select_prec}. If
    we let \( \vec{v} = \textcolor{darkorange}{\CM_{\I, \I}^{-1} \CM_{\I, k}}
    \), then}
  &=
  \begin{pmatrix}
    \CM_{\I, \I}^{-1} + \CM_{k, k \mid \I}^{-1} \vec{v} \vec{v}^T &
    -\CM_{k, k \mid \I}^{-1} \vec{v} \\
    -\CM_{k, k \mid \I}^{-1} \vec{v}^{\top} & \CM_{k, k \mid \I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:select_prec}.
Note that the bulk of the update is a rank-one update to \( \CM_{1,
1}^{-1} \), which can be computed in \( \BigO(\card{\I}) = \BigO(s^2) \).

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

Assuming we have the precision matrix of the prediction points conditional on
the selected entries, \( \CM_{\Pred, \Pred \mid \I}^{-1} \), we want to take
into account selecting an index \( k \), or to compute \( \CM_{\Pred, \Pred
\mid \I \cup \{ k \}}^{-1} \), which is a rank-one update to the covariance
(but not necessarily the precision) from \cref{eq:obj_gp_mult}.
We can directly apply the ShermanMorrisonWoodbury
formula which states that:
\begin{align}
  \CM_{1, 1 \mid 2}^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning from \cref{eq:cond_cov},}
  \left (
    \CM_{1, 1} - \CM_{1, 2} \CM_{2, 2}^{-1} \CM_{2, 1}
  \right )^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{
    For brevity of notation, letting \( \vec{u} = \CM_{1, 2} \) and \(
    \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} = \CM_{1, 1}^{-1} \vec{u} \),
  }
  (\CM_{1, 1} - \CM_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \CM_{1, 1}^{-1} + \CM_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{
    So we see that a rank-one update to \( \CM_{1, 1} \) then
    inverting is a rank-one update to \( \CM_{1, 1}^{-1} \).
    In our context, \( \CM_{1, 1} = \CM_{\Pred, \Pred \mid \I}, \vec{u}
    = \CM_{\Pred, k \mid I}, \CM_{2, 2} = \CM_{k, k \mid \I} \) so \(
    \CM_{2, 2 \mid 1}^{-1} = \CM_{k, k \mid \Pred, I}^{-1} \) (from the
    quotient rule \cref{eq:quotient_rule}).
    \( \vec{v} \) can be computed according to definition
    as \( \CM_{\Pred, \Pred \mid \I}^{-1} \vec{u} \).
    Thus, we can write the update as
  }
  \left ( \CM_{\Pred, \Pred \mid \I} -
    \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
         {\CM_{kk \mid \I}}
  \right )^{-1} &=
    \CM_{1, 1}^{-1} +
    \CM_{k, k \mid \Pred, \I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:select_mult_prec}.
Since the update is a rank-one update, it can be
computed in \( \BigO(\card{\Pred}^2) = \BigO(m^2) \).

\subsection{Updating the log determinant after a rank-one downdate}
\label{app:logdet_downdate}

Assuming we already have the log determinant of the covariance matrix of the
prediction points conditional on the selected entries, \( \logdet(\CM_{\Pred,
\Pred \mid \I}) \), we wish to compute the log determinant after we add an
index \( k \) to \( \I \), that is, to compute \( \logdet(\CM_{\Pred, \Pred
\mid \I \cup \{ k \}}) \).
From \cref{eq:cond_select}, selecting a new point
is a rank-one downdate on the covariance matrix.
\begin{align}
  \nonumber
  \logdet \left ( \CM_{\Pred, \Pred \mid \I \cup \{ k \}} \right )
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{Using the matrix determinant lemma,}
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( 1 -
       \frac{\CM_{\Pred, k \mid \I}^{\top} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{
    Focusing on the second term, we can turn
    the quadratic form into condtioning.
  }
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left (
       \frac{\CM_{k, k \mid \I} -
             \CM_{k, \Pred \mid \I} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{
    By the quotient rule \cref{eq:quotient_rule}, we combine the conditioning.
  }
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} \right ) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}

\newpage

\section{Algorithms}

\begin{figure}[th!]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Point selection by \\ explicit precision}
      \label{alg:select_prec}
      \input{figures/algorithms/select_prec.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Point selection by \\ Cholesky factorization}
      \label{alg:select_chol}
      \input{figures/algorithms/select_chol.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for single-point selection.}
  \label{fig:alg_select}
\end{figure}

\begin{algorithm}[H]
  \caption{Direct sparse Gaussian process regression by selection}
  \label{alg:infer_select}
  \input{figures/algorithms/infer_select.tex}
\end{algorithm}

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Multiple-point selection by explicit precision}
      \label{alg:select_mult_prec}
      \input{figures/algorithms/select_mult_prec.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Multiple-point selection by Cholesky factorization}
      \label{alg:select_mult_chol}
      \input{figures/algorithms/select_mult_chol.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Update Cholesky factor}
      \label{alg:chol_update}
      \input{figures/algorithms/chol_update.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for multiple-point selection.}
  \label{fig:alg_mult_select}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Partial point selection}
      \label{alg:select_partial}
      \input{figures/algorithms/select_partial.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Find \( j \)'s index in \( \Order \)}
      \label{alg:insert_index}
      \input{figures/algorithms/insert_index.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Compute \( j \)'s objective}
      \label{alg:partial_score}
      \input{figures/algorithms/partial_score.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Insert \( k \) into \( L \)}
      \label{alg:chol_insert}
      \input{figures/algorithms/chol_insert.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for partial selection.}
  \label{fig:alg_partial_select}
\end{figure}

\end{document}
