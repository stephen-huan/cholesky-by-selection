% SIAM Article Template

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Sparse Cholesky factorization by greedy conditional selection},
  pdfauthor={S. Huan, J. Guinness, M. Katzfuss, H. Owhadi, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Dense kernel matrices resulting from pointwise evaluations of a kernel
  function arise naturally in machine learning and statistics. Previous
  work in constructing sparse transport maps or sparse approximate inverse
  Cholesky factors for such matrices by minimizing Kullback-Leibler divergence
  recovers the Vecchia approximation for Gaussian processes. However, this
  method for Cholesky factorization relies only on geometry to construct the
  sparsity pattern, ignoring the conditional effect of adding an entry. In this
  work, we construct the sparsity pattern by leveraging a greedy selection
  algorithm which selects points that maximize mutual information with target
  points, conditional on all points selected previously. For selecting $k$
  points out of $N$, the naive time complexity is $\mathcal{O}(N k^4)$, but
  by maintaining a partial Cholesky factor we reduce this to $\mathcal{O}(N
  k^2)$. Furthermore, for multiple ($m$) targets we achieve a time complexity
  of $\mathcal{O}(N k^2 + N m^2 + m^3)$ which is maintained in the setting
  of Cholesky factorization where a selected point need not condition every
  target. We directly apply the selection algorithm to image classification
  and recovery of sparse Cholesky factors, improving upon $k$-th nearest
  neighbors in every case. Through Kullback-Leibler minimization we apply
  the algorithm to Cholesky factorization and Gaussian process regression,
  improving in high dimensional geometries as well as when preconditioning
  with the conjugate gradient.
\end{abstract}

% REQUIRED
\begin{keywords}
  \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\section{Introduction}

\paragraph{The problem and existing work}

This work is concerned with Gaussian process regression in the setting of \(
N \) points in \( D \) dimension space, whose covariance matrix \( \Theta
\in \mathbb{R}^{N \times N} \) is induced by a prescribed kernel function.
Gaussian processes enjoy widespread application in spatial statistics and
geostatistics \cite{rue2005gaussian}, machine learning through kernel
machines \cite{rasmussen2006gaussian}, and Bayesian experimental design
\cite{wu2018exploiting}, e.g. in sensor placement \cite{krause2008nearoptimal}.
However, statistical inference with Gaussian processes often requires the
log determinant or inverse of the covariance matrix, at a computational time
cost of \( \mathcal{O}(N^3) \) and a memory cost of \( \mathcal{O}(N^2) \)
to store the \emph{dense} \( N \times N \) covariance matrix.

Many approaches have been developed to address this computational bottleneck.
The machine learning community has developed many sparse (in the sense of
selecting a subset of \( k \) points of out the \( N \) total, \( k \ll N
\)) algorithms that greedily select the next point to include by maximizing
a information-theoretic objective \cite{smola2000sparse, herbrich2002fast,
seeger2003fast}. Similar algorithms have been developed in the context of
sensor placement \cite{krause2008nearoptimal, clark2018greedy} where it is
assumed the target phenomena is modeled by a Gaussian process or is otherwise
linearly dependent on the \( k \) selected measurements. More recent work
exploits the empirical observation that the marginal log-likelihood is
approximately linear, giving a principled way to determine the number of
selected points without viewing the whole dataset \cite{bartels2022adaptive}.
These greedy methods often leverage linear-algebraic factorization for
efficient computation, particularly the Cholesky factorization \cite{
herbrich2002fast, seeger2003fast, bartels2022adaptive} or the closely
related QR factorization \cite{clark2018greedy}. Since both the covariance
matrix \( \Theta \) and its precision \( \Theta^{-1} \) are by definition
positive-definite, a Cholesky factor \( L \) always exists. Another approach is
to directly leverage the Cholesky factor itself, through incomplete Cholesky
factorization of the covariance \( \Theta \) \cite{schafer2020compression,
chenparallel, chow2015finegrained} or a sparse approximate Cholesky
factorization of the precision \( \Theta^{-1} \) \cite{schafer2021sparse}.
As noted in \cite{schafer2021sparse}, the Cholesky factor of the precision
is often much sparser than the factor of the covariance, because the
precision encodes conditional independence while the covariance encodes
marginal independence. In the special case of the Mat{\'e}rn kernel function
with half-integer smoothness, the covariance matrix \( \Theta \) admits
a sparse, banded factorization, allowing for an exact algorithm linear
in both time and space with the number of points, and only additionally
depending on the smoothness parameter \( \nu \) \cite{chen2022kernel}. From
the perspective of transport maps, the Cholesky factor \( L \) satisfying
\( \Theta^{-1} = L^{\top} L \) can be viewed as a linear transport map
mapping an arbitrary multivariate Gaussian distribution \( \vec{x} \sim
\mathcal{N}(\vec{0}, \Theta) \) to the standard Gaussian \( \vec{z} \sim
\mathcal{N}(\vec{0}, I_N) \) through \( \vec{z} = L \vec{x} \). Thus, one
generalization of Cholesky factorization to nonlinear and non-Gaussian
distributions is the Knothe-Rosenblatt rearrangement, which preserves
lower triangularity (variables only interact with those before them in
the ordering), some sense of positive-definiteness (the diagonal of the
Jacobian is positive), and sparsity, particularly in the inverse mapping
\cite{spantini2018inference}. These triangular transport maps have been
applied to problems in spatial statistics, particularly for simulation and
sampling problems \cite{marzouk2016introduction, katzfuss2022scalable}.

\paragraph{Vecchia approximation}

Many of these existing approaches can be viewed through the lens of an early
approach for fast Gaussian process regression, the Vecchia approximation
\cite{vecchia1988estimation, katzfuss2021general}. The key observation is a
decomposition of the joint likelihood \( \pi \).
\begin{align}
  \label{eq:joint}
  \pi(\vec{x}) &= \pi(x_1) \pi(x_2 \mid x_1) \dots
    \pi(x_N \mid x_1, x_2, \dotsc, x_{N - 1})
  \shortintertext{Assuming that there are many points, the key assumption
    is that many of the points contribute little additional information,
    conditional on a carefully chosen subset of the points. Letting \(
    s_i \) denote the indices of training points to include for the \(
    i \)th variable, the Vecchia approximation proposes to approximate
    \cref{eq:joint} by the sparse approximation}
  \pi(\vec{x}) &\approx \pi(x_1) \pi(x_2 \mid x_{s_2}) \dots
    \pi(x_N \mid x_{s_N})
\end{align}
Note that \( s_i \) is often restricted to a subset of the indices
\( 1, \dotsc, i - 1 \) to maintain lower triangularity. Indices in
sparsity set \( s_i \) are often chosen to be the closest points to
\( \vec{x}_i \) by Euclidean distance \cite{vecchia1988estimation,
schafer2020compression, schafer2021sparse, katzfuss2022scalable}. This
choice can be justified by noting that popular kernel functions like the
Mat{\'e}rn family of kernel functions decay exponentially with increasing
distance. For more general kernel functions, geostatisticians have long
observed the ``screening effect'', or the observation that conditional
on points close to the point of interest, far away points are nearly
conditionally independent \cite{stein2002screening, stein20112010}.
Possible manifestations of this effect include the observed linearity
of the marginal log-likelihood \cite{bartels2022adaptive}, approximate
submodularity of common information-theoretic criteria \cite{das2011submodular,
jagalur-mohan2021batch}, and exponential decay in error for Cholesky
factorization \cite{schafer2020compression, schafer2021sparse}.

\begin{figure}[h!]
  \centering
  \input{figures/screening/uncond.tex}%
  \input{figures/screening/cond.tex}
  \caption{An illustration of the screening effect with the Mat{\'e}rn
    kernel with a length scale of 1 and smoothness \( \nu = \frac{5}{2}
    \). The first figure shows the unconditional covariance with the point
    at (0, 0). The second figure shows the conditional covariance after
    conditioning on the four points in orange.}
  \label{fig:screen}
\end{figure}

From the Vecchia approximation, the joint likelihood can be factored into
\( N \) independent regression problems, where each regression problem is
to approximate the conditional distribution of the \( i \)th variable.
Independence allows for embarrassingly parallel algorithms, which is
exploited in both Gaussian process regression \cite{katzfuss2021general}
and Cholesky factorization \cite{schafer2021sparse}. The difference is
that while the Gaussian process perspective emphasizes local regression
problems and sparsity in point selection (or variable interaction),
Cholesky factorization emphasizes minimizing the global Kullback-Leibler
(KL) divergence and sparsity in the resulting matrix factor.

\paragraph{Sparsity selection by geometry}

In the original Vecchia paper, the sparsity
sets are determined by Euclidean distance.

screening and conditional independence.

\paragraph{conditional selection / experimental design / omp}

omp: \cite{tropp2007signal, tropp2006algorithms,
hao2021efficient, siegel2022optimal, baptista2019greedy}

\begin{figure}[h]
  \centering
  \input{figures/selection/knn.tex}%
  \input{figures/selection/cknn.tex}
  \caption{Here, the \textcolor{lightblue}{blue} points are the
    \textcolor{lightblue}{candidates}, or training points, the
    \textcolor{orange}{orange} point is the \textcolor{orange}{unknown} point,
    or the point to make a prediction at, and the \textcolor{seagreen}{green}
    points are the \( k \) \textcolor{seagreen}{selected} points. The
    \textcolor{rust}{red} line is the \textcolor{rust}{conditional mean}
    \( \mu \), conditional on the \( k \) selected points, and the \( \mu
    - 2 \sigma \) to \( \mu + 2 \sigma \) confidence interval is shaded
    for the conditional variance \( \sigma^2 \).}
  \label{fig:select}
\end{figure}

\paragraph{Main results}

\paragraph{Outline}

The remainder of the paper is organized as follows.

\section{Greedy selection for directed inference}

\subsection{Conditional \textit{k}-th nearest neighbors}

Consider the simple regression algorithm \( k \)th-nearest neighbors (\( k
\)-NN). Given a training set \( X_\text{Tr} = \{ \vec{x}_1, \dotsc, \vec{x}_n
\} \) and corresponding labels \( \vec{y}_\text{Tr} = \{ y_1, \dotsc, y_n \}
\), the goal is to estimate the unknown label \( y_\text{Pr} \) of some unseen
prediction point \( \vec{x}_\text{Pr} \) Stated informally, the \( k \)-NN
approach is to select the \( k \) points in \( X_\text{Tr} \) \emph{most
informative} about \( \vec{x}_\text{Pr} \) and combine their results.

\begin{algorithm}
  \caption{Idealized \( k \)-NN regression}
  Given \( (X_\text{Tr}, \vec{y}_\text{Tr}) =
        \{ (\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n) \} \)
  and \( \vec{x}_\text{Pr} \)
  \begin{enumerate}
    \item Select the \( k \) points in \( X_\text{Tr} \)
      most informative about \( \vec{x}_\text{Pr} \)
    \item Combine the labels of the selected points to generate a prediction
  \end{enumerate}
\end{algorithm}

One \Ftodo{"specific" and "intuitively" are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much as
possible. It's normal to add them out of reflex initially, so it requires
active postprocessing.} specific approach is intuitively, points close
to \( \vec{x}_\text{Pr} \) should be similar to it. So we select the \(
k \) closest points in \( X_\text{Tr} \) to \( \vec{x}_\text{Pr} \) and
pool their labels (e.g., by averaging).

\begin{algorithm}
  \caption{\( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) closest to \( \vec{x}_\text{Pr} \)
    \item Compute \( \vec{y}_\text{Pr} \) by
      \( \frac{1}{k} \sum_{j = 1}^k y_{i_j} \)
  \end{enumerate}
\end{algorithm}

However, we can generalize the notion of ``closest'' with the \emph{kernel
trick}, by using an arbitrary kernel function to measure similarity. For
example, commonly used kernels like the Gaussian kernel and Mat{\'e}rn family
of covariance functions are \emph{isotropic}; they depend only on the distance
between the two vectors. If such isotropic kernels monotonically decrease with
distance, then selecting points based on the largest kernel similarity recovers
\( k \)-NN. However, kernels need not be isotropic in general --- they just
need to capture some sense of ``similarity'', motivating kernel \( k \)-NN.

\Stodo{not sure whether ``stationary'' or
``isotropic'' are the right word(s) to use here}

\begin{algorithm}
  \caption{Kernel \( k \)-NN regression}
  Given kernel function \( K(\vec{x}, \vec{y}) \)
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \} \subseteq X_\text{Tr} \) most similar to \( \vec{x}_\text{Pr} \)
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by similarity
  \end{enumerate}
\end{algorithm}

Although the kernel \( k \)-NN approach is more general than its normed
counterpart, it still suffers from a fundamental issue. Suppose the closest
point to \( \vec{x}_\text{Pr} \) has many duplicates in the training set. Then
the algorithm will select the same point multiple times, even though in some
sense the duplicate point has stopped giving additional information about the
prediction point. In order to fix this issue, we should be selecting new points
\emph{conditional} on the points we've already selected. This preserves the
idealized algorithm of selecting points based on the information they tell us
about the prediction point --- once we've selected a point, conditioning on
it reduces the information similar points tells us, encouraging diverse point
selection.

\begin{algorithm}
  \caption{\textcolor{lightblue}{Conditional} kernel \( k \)-NN regression}
  \begin{enumerate}
    \item Select the \( k \) points \( \{ \vec{x}_{i_1}, \dotsc, \vec{x}_{i_k}
      \}  \subseteq X_\text{Tr} \) most \textcolor{lightblue}{informative} to
      \( \vec{x}_\text{Pr} \) \\ \textcolor{lightblue}{after conditioning on
      all points selected beforehand}
    \item Compute \( \vec{y}_\text{Pr} \) by an average weighted by
      \textcolor{lightblue}{information}
  \end{enumerate}
\end{algorithm}

In order to make the notions of conditioning and information precise, we need a
specific framework. Kernel methods lead naturally to Gaussian processes, whose
covariance matrices naturally result from kernel functions and allows us to use
the rigorous statistical and information-theoretic notions of conditioning and
information.

\Stodo{mention sensor placement/spatial statistics perspective/literature}

\subsection{Sparse Gaussian process regression}
\label{subsec:gp_reg}

A \emph{Gaussian process} is a prior distribution over functions, such that
for any finite set of points, the corresponding function over the points
is distributed according to a multivariate Gaussian. In order to generate
such a distribution over an uncountable number of points consistently, a
Gaussian process is specified by a \emph{mean function} \( \mu(\vec{x}) \) and
\emph{covariance function} or \emph{kernel function} \( K(\vec{x}, \vec{y})
\). For any finite set of points \( X = \{ \vec{x}_1, \dotsc, \vec{x}_n \}
\) ,\( f(X) \sim \mathcal{N}(\vec{\mu}, \Theta) \), where \( \vec{\mu}_i =
\mu(\vec{x}_i) \) and \( \Theta_{ij} = K(\vec{x}_i, \vec{x}_j) \).

In order to compute a prediction at \( \vec{x}_\text{Pr} \), we can
simply condition the desired prediction \( \vec{y}_\text{Pr} \) on the
observed outputs and compute the conditional expectation. We can also
find the conditional variance, which will quantify the uncertainty of
our prediction. If we block our covariance matrix
\(
  \Theta =
  \begin{pmatrix}
    \Theta_{\text{Tr}, \text{Tr}} & \Theta_{\text{Tr}, \text{Pr}} \\
    \Theta_{\text{Pr}, \text{Tr}} & \Theta_{\text{Pr}, \text{Pr}}
  \end{pmatrix}
\)
where \( \Theta_{\text{Tr}, \text{Tr}}, \Theta_{\text{Pr}, \text{Pr}},
\Theta_{\text{Tr}, \text{Pr}}, \Theta_{\text{Pr}, \text{Tr}} \) are the
covariance matrices of the training data, testing data, and training and test
data respectively, then the conditional expectation and covariance are:
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &=
    \vec{\mu}_\text{Pr} +
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    (\vec{y}_\text{Tr} - \vec{\mu}_\text{Tr}) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] &=
    \Theta_{\text{Pr}, \text{Pr}} -
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    \Theta_{\text{Tr}, \text{Pr}}
  \shortintertext{For brevity of notation, we will
    often denote the conditional covariance matrix as}
  \label{eq:cond_cov_notation}
  \Theta_{\text{Pr}, \text{Pr} \mid \text{Tr}} &:=
    \Theta_{\text{Pr}, \text{Pr}} -
    \Theta_{\text{Pr}, \text{Tr}} \Theta_{\text{Tr}, \text{Tr}}^{-1}
    \Theta_{\text{Tr}, \text{Pr}}
  \shortintertext{When conditioning on multiple sets, the sets are given in
    order of computation. Although the resulting covariance matrix is the same,
    a different order of conditioning means different intermediate results in
    repeated application of \cref{eq:cond_cov}. In general,}
  \Theta_{I, J \mid K_1, K_2, \dotsc, K_n} &:=
    \Cov[\vec{y}_I, \vec{y}_J \mid
         \vec{y}_{K_1 \cup K_2 \cup \dotsb \cup K_n}]
  \shortintertext{denotes the covariance between the variables in index
    sets \( I \) and \( J \), conditional on the variables in \( K_1, K_2,
    \dots, K_n \). Note that by the quotient rule of Schur complementation:}
  \label{eq:quotient_rule}
  \Theta_{I, J \mid K_{1 \dots n}} &=
    \Theta_{I, J \mid K_{1 \dots n - 1}} -
    \Theta_{I, K_n \mid K_{1 \dots n - 1}}
    \Theta_{K_n, K_n \mid K_{1 \dots n - 1}}^{-1}
    \Theta_{K_n, J \mid K_{1 \dots n - 1}}
\end{align}
\Stodo{\( I \) is overloaded to be identity, an index set,
and mutual information/information gain. \( I \) should
refer to identity and the other two must be disambiguated.}

Note that calculating the posterior mean and variance requires inverting the
training covariance matrix \( \Theta_{\text{Tr}, \text{Tr}} \), which costs
\( \mathcal{O}(N^3) \), where \( N \) is the number of training points. This
scaling is prohibitive for large datasets, so many \emph{sparse} Gaussian
process regression techniques have been proposed. These methods often focus
on selecting a subset of the training data that is most informative about the
prediction points, which naturally aligns with our \( k \)-NN perspective.
If \( s \) points are selected out of the \( N \), then the inversion will
cost \( \mathcal{O}(s^3) \), which could be substantially cheaper if \( s \)
is significantly smaller than \( N \). The question is then how to select as
few points as possible while maintaining predictive accuracy.

\Stodo{cite sparse Gaussian regression papers}

\subsection{Problem: optimal selection}

The natural criterion justified from the \( k \)-NN perspective is to maximize
the \emph{mutual information} between the selected points and the target point
for prediction. Such information-theoretic objectives have seen success in
the spatial statistics community \cite{krause2008nearoptimal}, who use such
criteria to determine the best locations to place sensors in a Gaussian process
regression context. The mutual information, or \emph{information gain} is
defined as
\begin{align}
  \label{eq:info}
  \I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}] &= \entropy[\vec{y}_\text{Pr}] -
    \entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]
\end{align}
We can use the fact that the entropy of a multivariate Gaussian is
monotonically increasing with the log determinant of its covariance matrix to
efficiently compute these entropies. Because the entropy of \( \vec{y}_\text{Pr}
\) is constant, maximizing the mutual information is equivalent to minimizing
the conditional entropy. From \cref{eq:cond_cov} we see that minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix. Note that for a single predictive point, this
is monotonic with its variance. So another justification is that we are
reducing the \emph{conditional variance} of the desired point as much as
possible. In particular, because our estimator is the conditional expectation
\cref{eq:cond_mean}, it is unbiased because \( \E[\E[\vec{y}_\text{Pr} \mid
\vec{y}_\text{Tr}]] = \E[\vec{y}_\text{Pr}] \). Because it is unbiased, its
expected mean squared error is simply the conditional variance since \(
\E[(\vec{y}_\text{Pr} - \E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}])^2 \mid
\vec{y}_\text{Tr}] = \Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}] \) where
the expectation is taken under conditioning because of the assumption that
\( \vec{y}_\text{Pr} \) is distributed according to the Gaussian process. So
maximizing the mutual information is equivalent to minimizing the conditional
variance which is in turn equivalent to minimizing the expected mean squared
error of the prediction. Another perspective on the objective can be derived
from comparing the mutual information to the EV-VE identity, which states
\begin{align}
  \textcolor{orange}{\entropy[\vec{y}_\text{Pr}]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]} +
    \textcolor{rust}{\I[\vec{y}_\text{Pr};\vec{y}_\text{Tr}]} \\
  \textcolor{orange}{\Var[\vec{y}_\text{Pr}]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\text{Pr} \mid \vec{y}_\text{Tr}]]}
\end{align}
On the left hand side, entropy is monotone with variance. On the right hand
side, the expectation of the conditional variance can be interpreted to be
the fluctuation of the prediction point after conditioning, and is monotone
with the conditional entropy. Because the expectation of conditional variance
and variance of conditional expectation add to a constant, minimizing the
expectation of the conditional variance is equivalent to maximizing the
variance of conditional expectation, which we see corresponds to the mutual
information term. Supposing \( \vec{y}_\text{Pr} \) was independent of \(
\vec{y}_\text{Tr} \), then the conditional expectation becomes simply the
expectation, whose variance is 0. Thus, the variance of the conditional
expectation can be interpreted to be the ``information'' shared between \(
\vec{y}_\text{Pr} \) and \( \vec{y}_\text{Tr} \), as the larger it is, the
more the prediction for \( \vec{y}_\text{Pr} \) (the conditional expectation)
depends on the observed results of \( \vec{y}_\text{Tr} \).

\subsection{A greedy approach}
\label{subsec:greedy}

We now consider how to efficiently minimize the conditional variance
objective using a greedy approach. At each iteration, we pick the training
point which most reduces the conditional variance of the prediction point.
Let \( I = \{ i_1, i_2, \dotsc, i_j \} \) be the set of indexes of training
points selected already. Let the prediction point have index \( n + 1 \),
the last index. For a candidate index \( k \), we update the covariance
matrix after conditioning on \( y_k \), in addition to the indices already
selected according to \cref{eq:cond_cov}:
\begin{align}
  \nonumber
  \Theta_{:, : \mid I, k} &= \Theta_{:, : \mid I} -
    \Theta_{:, k \mid I} \Theta_{k, k \mid I}^{-1} \Theta_{k, : \mid I} \\
  \label{eq:cond_select}
                          &= \Theta_{:, : \mid I} -
    \frac{\Theta_{:, k \mid I} \Theta_{:, k \mid I}^{\top}}{\Theta_{kk \mid I}}
\end{align}

We see that conditioning on a new entry is a rank-one update on the current
covariance matrix \( \Theta_{\mid I} \), given by the vector \( \vec{u} =
\frac{\Theta_{:, k \mid I}}{\sqrt{\Theta_{kk \mid I}}} \). Thus, the amount
that the variance of \( \vec{y}_\text{Pr} \) will decrease after selecting
\( k \) is given by \( u_{n + 1}^2 \), or
\begin{align}
  \label{eq:obj_gp}
  \frac{\Cov[\vec{y}_{\text{Tr}}[k], \vec{y}_\text{Pr} \mid I]^2}
       {\Var[\vec{y}_{\text{Tr}}[k] \mid I]} &=
  \frac{\Theta_{k, n + 1 \mid I}^2}{\Theta_{kk \mid I}}
  = \Var[\vec{y}_\text{Pr} \mid I]
    \Corr[\vec{y}_\text{Tr}[k], \vec{y}_\text{Pr} \mid I]^2
\end{align}
For each candidate \( k \), we need to keep track of its conditional variance
and conditional covariance with the prediction point after conditioning on
the points already selected to compute \cref{eq:obj_gp}. We then simply
choose the candidate with the largest decrease in predictive variance. To
keep track of the conditional variance and covariance, we can simply start
with the initial values given by \( \Theta_{kk} \) and \( \Theta_{nk} \) and
update after selecting an index \( j \). We compute \( \vec{u} \) for \( j
\) directly according to \cref{eq:cond_cov} and update \( k \)'s conditional
variance by subtracting \( u_k^2 \) and update its conditional covariance
by subtracting \( u_k u_{n + 1} \).

In order to efficiently compute \( \vec{u} \), we rely on two main
strategies. The direct method is to keep track of \( \Theta_{I, I}^{-1} \),
or the precision of the selected entries, and update the precision every
time a new index is added to \( I \). This can be done efficiently in \(
\mathcal{O}(s^2) \), see \cref{app:prec_insert}. Once \( \Theta_{I, I}^{-1}
\) has been computed, \( \vec{u} \) is computed trivially according to
\cref{eq:cond_cov}. For each of the \( s \) rounds of selection, it takes
\( s^2 \) to update the precision, and costs \( Ns \) to compute \( \vec{u}
\), costing \( \mathcal{O}(N s^2 + s^3) = \mathcal{O}(N s^2) \) overall.

The second approach is to take advantage of the quotient rule of Schur
complementation. Stated statistically, the quotient rule states that
conditioning on \( I \) and then conditioning on \( J \) is the same as
conditioning on \( I \cup J \). We then remind ourselves that Cholesky
factorization can be viewed as iterative conditioning:
\begin{align}
  \shortintertext{Re-writing the joint covariance matrix,}
  \label{eq:chol_schur}
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    I & \textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \shortintertext{so we see that the Cholesky factorization of the joint
    covariance \( \Theta \) is}
  \label{eq:chol}
  \chol(\Theta) &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \chol(\Theta_{1, 1}) & 0 \\
    0 & \chol(\textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    })
  \end{pmatrix} \\
  \nonumber
  &=
  \begin{pmatrix}
    \chol(\Theta_{1, 1}) & 0 \\
    \Theta_{2, 1} \chol(\Theta_{1, 1})^{-\top} & \chol(
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    )
  \end{pmatrix}
\end{align}
Here \( \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} \)
corresponds to the conditional expectation in \cref{eq:cond_mean} and \\ \(
\textcolor{lightblue}{
  \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
} \)
corresponds to the conditional covariance in \cref{eq:cond_cov}. Thus, we
see that Cholesky factorization is iteratively conditioning the Gaussian
process. From the iterative conditioning perspective, the \( i \)th column
of the Cholesky factor corresponds precisely to the corresponding \( \vec{u}
\) for \( i \) since a iterative sequence of conditioning on \( i_1, i_2
\dotsc \) is equivalent to conditioning on \( I \) by the quotient rule.

The Cholesky factorization can be efficiently computed without excess
dependence on \( N \) with left-looking, so the conditioning only
happens when we need it. For each of the \( s \) rounds of selection,
it costs \( \mathcal{O}(Ns) \) to compute the next column of the
Cholesky factorization, for a total cost of \( \mathcal{O}(Ns^2) \),
matching the time complexity of the explicit precision approach.

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Point selection by \\ explicit precision}
  \label{alg:gp_select}
  \input{figures/algorithms/gp_select.tex}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Point selection by \\ Cholesky factorization}
  \label{alg:select_chol}
  \input{figures/algorithms/select_chol.tex}
\end{algorithm}
\end{minipage}

While both approaches have the same time complexity, the explicit precision
algorithm uses \( \mathcal{O}(s^2) \) space to store the precision while the
Cholesky factorization uses \( \mathcal{O}(N s) \) to store the first \( s \)
columns of the Cholesky factorization of \( \Theta \), which is always more
memory than the precision (\( N > s \)). Both algorithms use an additional \(
\mathcal{O}(N) \) space to store the conditional variances and covariances.
Although the precision algorithm uses asymptotically less memory than the
Cholesky algorithm, the Cholesky algorithm is preferred for better constant
factor speed and ease of implementation.
\Stodo{what is a good way to say non-asymptotically faster?}

Once the indices have been computed according to \cref{alg:gp_select}
or \cref{alg:select_chol}, inferring the conditional mean and
covariance of the unknown data can be done directly according to
\cref{eq:cond_mean} and \cref{eq:cond_cov} in time \( \mathcal{O}(s^3)
\) using \cref{alg:infer_select}.

This algorithm is in fact the covariance equivalent of the
signal recovery algorithm orthogonal matching pursuit (OMP)
\cite{tropp2007signal}, a connection elaborated in \cref{app:omp}.

\begin{algorithm}
  \caption{Gaussian process inference by selection}
  \label{alg:infer_select}
  \input{figures/algorithms/infer_select.tex}
\end{algorithm}

\subsection{Supernodes and blocked selection}

We now consider how to efficiently deal with multiple prediction points. The
first question is how to generalize the previous objective for a single point
\cref{eq:obj_gp} to multiple points. Following the same mutual information
justification as before, a natural criterion is to minimize the log determinant
of the prediction points' covariance matrix after conditioning on the
selected points, or \( \logdet(\Theta_{\text{Pr}, \text{Pr} \mid I}) \). This
objective, known as D-optimal \cite{krause2008nearoptimal}, has many intuitive
interpretations --- for example, as the volume of the region of uncertainty
or as the scaling factor in the density function for the Gaussian process.

We now need to be able to efficiently compute the effect of selecting an
index \( k \) on the log determinant. From \cref{eq:cond_select}, we know
that selecting an index is a rank-one update on the prediction covariance.
Using the matrix determinant lemma,
\Stodo{move to appendix}
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet
  \left (
    \Theta_{\text{Pr}, \text{Pr} \mid I \cup \{ k \}}
  \right ) &= \logdet
    \left (
      \Theta_{\text{Pr}, \text{Pr} \mid I} -
      \frac{\Theta_{\text{Pr}, k \mid I}
            \Theta_{\text{Pr}, k \mid I}^{\top}
           }{\Theta_{kk \mid I}}
    \right ) \\
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      1 -
      \frac{\Theta_{\text{Pr}, k \mid I}^{\top}
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \shortintertext{Focusing on the second term, we can turn
    the quadratic form into conditioning:}
  \nonumber
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      \frac{\Theta_{kk \mid I} -
            \Theta_{k, \text{Pr} \mid I}
            \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1}
            \Theta_{\text{Pr}, k \mid I}
           }{\Theta_{kk \mid I}}
    \right )
  \label{eq:greedy_mult}
  \shortintertext{By the quotient rule, we combine the conditioning:}
           &= \logdet \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} \right ) +
    \log
    \left (
      \frac{\Theta_{kk \mid I, \text{Pr}}}{\Theta_{kk \mid I}}
    \right )
\end{align}
\Stodo{this maybe should be changed to information gain by flipping the sign}
The greedy objective \cref{eq:greedy_mult} tells us that to minimize the
log determinant, we can simply select the index \( k \) with the smallest
ratio between the conditional variance after conditioning on the previously
selected points as well as the prediction points, and the conditional
variance after just conditioning on the selected points. Intuitively
this tells us that we can place sensors \emph{backwards}, where we imagine
placing sensors at the \emph{prediction points} instead of the candidates.
We then measure the conditional variance at a candidate, and pick the
candidate whose conditional variance decreases the most (relative from
what it started out as). Intuitively, these candidates are likely to give
information about the prediction points, because the prediction points
give information about the candidate.

Re-writing the objective in this way also gives an efficient algorithm to
compute the necessary quantities. We condition on the prediction points
essentially the same as described in \cref{subsec:greedy}, by simply
maintaining two structures instead of one, one for the conditional variance
after conditioning on the previously selected points, and the other for the
conditional variance after also conditioning on the prediction points. By
the quotient rule, the order of conditioning does not matter as long as the
order is consistent. For the second structure, we therefore condition on
the prediction points \emph{first} before any points have been selected.
We again have two strategies, one which explicitly maintains precisions and
the other which relies on Cholesky factorization.

For the precision algorithm, using \cref{eq:cond_cov} directly, for \(
m \) prediction points it costs \( \mathcal{O}(m^3) \) to compute \(
\Theta_{\text{Pr}, \text{Pr}}^{-1} \) and then \( \mathcal{O}(N m^2) \)
to compute \( \Theta_{kk \mid \text{Pr}} \) for the \( N \) candidates \(
k \). For each of the \( s \) rounds of selecting candidates, it costs
\( s^2 \) and \( m^2 \) to update the precisions \( \Theta_{I, I}^{-1}
\) and \( \Theta_{\text{Pr}, \text{Pr}}^{-1} \) respectively, where the
details of efficiently updating \( \Theta_{\text{Pr}, \text{Pr}}^{-1}
\) after the rank-one update in \cref{eq:obj_gp_mult} are given in
\cref{app:prec_cond}. Given the precisions, \( \vec{u} = \frac{\Theta_{:,
k \mid I}}{\sqrt{\Theta_{kk \mid I}}} \) and \( \vec{u}_\text{Pr} =
\frac{\Theta_{:, k \mid I, \text{Pr}}}{\sqrt{\Theta_{kk \mid I, \text{Pr}}}}
\) are computed as usual according to \cref{eq:cond_cov} in time \( Ns
\) and \( Nm \). Finally, for each candidate \( j \), the conditional
variance \( \Theta_{jj \mid I} \) is updated by subtracting \( u_j^2 \),
the conditional covariance \( \Theta_{\text{Pr}, k \mid I} \) is updated
for each prediction point index \( c \) each by subtracting \( u_j u_c
\), and the conditional variance \( \Theta_{jj \mid I, \text{Pr}} \) is
updated by subtracting \( {u_\text{Pr}}_j^2 \). The total time complexity
after simplification is \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

For the Cholesky algorithm, two Cholesky factorizations are stored. We first
compute the Cholesky factorization after selecting each prediction point,
for a cost of \( (N + m) m \) for each of the \( m \) columns. We then begin
selecting candidates, which requires updating both Cholesky factors in time \(
(N + m)(m + s) \) which is dominated by updating the preconditioned Cholesky
factor. The columns of the Cholesky factors correspond precisely to \( \vec{u}
\) and \( \vec{u}_\text{Pr} \) and both conditional variances \( \Theta_{jj
\mid I} \) and \( \Theta_{jj \mid I, \text{Pr}} \) can be computed as above.
The conditional covariances do not need to be computed. Over \( s \) rounds
the total time complexity is \( \mathcal{O}((N + m) m^2 + s(N + m)(m + s)) \)
which simplifies to \( \mathcal{O}(N s^2 + N m^2 + m^3) \).

Although both approaches have the same time complexity, like the single
point case they differ in memory usage. The explicit precision requires \(
\mathcal{O}(s^2 + m^2) \) memory to store both precisions, as well as \(
\mathcal{O}(N m) \) memory to store the conditional covariances. The Cholesky
algorithm, on the othe rhand, requires \( \mathcal{O}((n + m)(m + s)) \) to
store the first \( m + s \) columns of the Cholesky factorization of the joint
covariance matrix between training and prediction points, which simplifies
to \( \mathcal{O}(N s + N m + m^2) \). Comparing the memory usage, they are
the same except for \( s^2 \) versus \( N s \), so the Cholesky algorithm
again uses more memory than the explicit precision algorithm.

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Multiple prediction point selection by explicit precision}
  \label{alg:gp_select_mult}
  \input{figures/algorithms/gp_select_mult.tex}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Multiple prediction point selection by Cholesky factorization}
  \label{alg:select_mult_chol}
  \input{figures/algorithms/select_mult_chol.tex}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Update Cholesky factor}
  \label{alg:chol_update}
  \input{figures/algorithms/chol_update.tex}
\end{algorithm}
\end{minipage}

\subsection{Near optimality by empirical submodularity}

If the objective satisfies the property of submodularity, then it is guaranteed
that the greedy algorithm produces an objective within a a \( 1 - \frac{1}{e}
\) of the optimal objective. Unfortunately the information gain objective is
not submodular in general, see \cref{app:submodular}. However, it is possible
to empirically observe for a particular point set and choice of kernel function
whether it is submodular. We note that one-dimensional points with a Mat{\'e}rn
kernel function seems to be submodular, but not for any higher dimension.

\section{Greedy selection for \emph{global} approximation by KL-minimization}

We have a covariance matrix \( \Theta \) and wish to compute the Cholesky
factorization of \( \Theta \) into a lower triangular factor \( L \) such that
\( \Theta = L L^{\top} \). \Stodo{justify importance/downstream applications
of Cholesky factorization}. This can be done in \( \mathcal{O}(N^3) \) with
standard algorithms, which is often prohibitive. Recall the problem of
inference in Gaussian process regression as described in \cref{subsec:gp_reg}
also took \( \mathcal{O}(N^3) \) to invert the covariance matrix \( \Theta
\). Thus, similar to Guassian process regression, we will use \emph{sparsity}
to mitigate the computational cost. In fact, we will be able to re-use our
previous algorithms \cref{alg:gp_select,alg:gp_select_mult} on each column
of the Cholesky factorization.

We will first compute the Cholesky factorization of \( \Theta^{-1} \),
also known as the \emph{precision matrix}, and use the resulting sparse
factorization to efficiently compute an approximation for \( \Theta
\). Because the precision matrix encodes the distribution of the full
conditionals, the \( (i, j) \)th entry of the precision matrix is 0 if and
only if the variables \( x_i \) and \( x_j \) are conditionally independent,
conditional on the rest of the variables. Thus, the precision matrix \(
\Theta^{-1} \) can be sparse as a result of conditional independence even
if the original covariance matrix \( \Theta \) is dense. It therefore
makes sense to attempt to approximately ``sparsify'' \( \Theta^{-1} \)
instead of \( \Theta \) with iterated conditioning.

Because of sparsity, we can only get an approximate Cholesky factor \( L \),
\( \hat{L} \) belonging to a pre-specified sparsity pattern \( S \) --- a set
of (row, column) indices that are allowed to be nonzero. In order to measure
the performance of the estimator, we treat the matrices as covariance matrices
of centered Gaussian processes (mean \( \vec{0} \)). In order to compare the
resulting distributions, we use the \emph{KL divergence} according to
\cite{schafer2021sparse}, or the expected difference in log-densities:
\begin{align}
  \label{eq:L_obj}
  L \coloneqq \argmin_{\hat{L} \in S} \, \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})
  \right )
\end{align}

Note that here we are computing the Cholesky factorization
of \( \Theta^{-1} \). Surprisingly enough, it is possible to
exactly compute \( L \). First, we re-write the KL-divergence:
\begin{align}
  \label{eq:kl}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  = \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
\end{align}
where \( \Theta_1 \) and \( \Theta_2 \) are both of size
\( N \times N \). See \cref{app:kl_obj} for details.

\begin{theorem}
  \label{thm:L}
  \cite{schafer2021sparse}.
  The non-zero entries of the \( i \)th
  column of \( L \) in \cref{eq:L_obj} are:
  \begin{align}
    \label{eq:L_col}
    L_{s_i, i} = \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
  \end{align}
\end{theorem}

Plugging the optimal \( L \) \cref{eq:L_col} back
into the KL divergence \cref{eq:kl}, we obtain:
\begin{align}
  \label{eq:obj_chol}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\Theta)
\end{align}

See \cref{app:kl_L} for details. In particular, it is important
which direction the KL divergence is or else cancellation of
the \( \trace(\Theta_2^{-1} \Theta_1) \) term may not occur.

In order to maximize \cref{eq:obj_chol}, we can ignore \( \logdet(\Theta)
\) since it does not depend on \( L \) and maximize over each column
independently, since each term in the sum only depends on a single
column. We want to minimize \( (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1}
\vec{e}_1)^{-1} \), the term corresponding to the diagonal entry in the
inverse of the submatrix of \( \Theta \) corresponding to the entries we've
taken. We can give this value statistical interpretation by using the
fact that marginalization in covariance is conditioning in precision.
\begin{align}
  \label{eq:inverse_cond}
  \Theta_{1, 1 \mid 2} &=
    ((\Theta^{-1})_{1, 1})^{-1}
  \shortintertext{where \( \Theta \) is blocked according to}
  \label{eq:blocking}
  \Theta &=
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix}
  \shortintertext{Thus, we see that}
  \nonumber
  (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= ((\Theta_{s_i, s_i}^{-1})_{11})^{-1} \\
  \label{eq:L_cond_var}
         &= \Theta_{ii \mid s_i - \{ i \}}
\end{align}

So our objective on each column is to minimize the conditional variance of
the \( i \)th variable, conditional on the entries we've selected --- \( s_i
\) contains \( i \) to begin with, so \( s_i - \{ i \} \) is the selected
entries. We can therefore use algorithm \cref{alg:gp_select} directly on each
column, where the prediction point is the \( i \) variable and the number
of points selected is the number of nonzero entries per column. The only
difference is that the candidates is limited to indices lower than \( i \),
that is, candidate indices \( k \) such that \( k > i \) to maintain the
lower triangularity of \( L \). Once \( s_i \) has been computed for each \(
i \), \( L \) can be constructed according to \cref{thm:L}. Each column costs
\( \mathcal{O}(s^3) \) to compute \( \Theta_{s_i, s_i}^{-1} \) for a total
cost of \( \mathcal{O}(N s^3) \) for the \( N \) columns of \( L \).

\subsection{Aggregated sparsity pattern}

We can also use the Gaussian process regression viewpoint to efficiently
aggregate multiple columns, that is, to use the same sparsity pattern for
multiple columns. We denote aggregating the column indices \( i_1, \dotsc, i_m
\) into the same group as \( \tilde{i} = \{i_1, i_2, \dots i_m \} \), letting
\( s_{\tilde{i}} = \bigcup_{i \in \tilde{i}} s_i \) be the aggregated sparsity
pattern, and letting \( \tilde{s} = s_{\tilde{i}} - \tilde{i} \) be the set
of selected entries excluding the diagonal entries. Each \( s_i = \tilde{s}
\cup \, \{ j \in \tilde{i} \mid j \geq i \} \), that is, the sparsity pattern
of the \( i \) column is the selected entries plus all the diagonal entries
lower than it. We will enforce that all the selected entries, excluding the
indices of the diagonals of the columns themselves, are below the lowest index
so that indices are not selected ``partially'' --- that is, an index could be
above some indices in the aggregated columns, and therefore invalid to add to
their column, but below others. That is, we restrict the candidate indices \(
k > \max \tilde{i} \) so that the selected index can be added to each column
in \( \tilde{i} \) without violating the lower triangularity of \( L \). It
is in fact possible to properly account for these partial updates, but the
reasoning and eventual algorithm becomes more complicated. We defer a detailed
discussion of the partial update case to \cref{app:partial}.

We now show that the KL-minimization objective on the aggregated indices
corresponds precisely to \cref{eq:obj_gp_mult}, the objective multiple
point Gaussian regression with the chain rule of log determinant through
conditioning.
\begin{align}
  \label{eq:det_chain}
  \logdet(\Theta) &= \logdet(\Theta_{1, 1 \mid 2}) + \logdet(\Theta_{2, 2})
  \shortintertext{where \( \Theta \) is blocked according to
    \cref{eq:blocking}. The KL divergence objective for \( \tilde{i} \) is:}
  \nonumber
  \sum_{i \in \tilde{i}} \log(\Theta_{ii \mid s_i - \{ i \} })
  &= \log(\Theta_{i_m i_m \mid \tilde{s}}) +
     \log(\Theta_{i_{m - 1} i_{m - 1} \mid \tilde{s} \cup \{ i_m \}})
     + \dotsb \\
  \nonumber
  &= \logdet(\Theta_{\{ i_m, i_{m - 1} \}, \{ i_m, i_{m - 1} \}
             \mid \tilde{s}}) +
    \log(\Theta_{i_{m - 2} i_{m - 2} \mid \tilde{s} \cup \{ i_m, i_{m - 1} \}})
    + \dotsb \\
  \label{eq:obj_mult}
  &= \logdet(\Theta_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}

We see that the objective \cref{eq:obj_mult} is equivalent to the
objective \cref{eq:obj_gp_mult}, that is, to minimize the log
determinant of the conditional covariance matrix corresponding to a
set of prediction points, conditional on the selected entries. We can
therefore directly use \cref{alg:gp_select_mult} on the aggregated
columns, where the prediction points correspond to indices in the
aggregation and where we restrict the candidates \( k \) to those
below each column in the aggregation, \( k > \max \tilde{i} \).

Hence the sparse Cholesky factorization motivated by KL divergence can be
viewed as sparse Gaussian process selection over each column, where entries are
selected to maximize mutual information with the entry on the diagonal of the
current column. In the aggregated case, the multiple columns in the aggregated
group correspond directly to predicting for multiple prediction points, where
entries are again selected to maximize mutual information with each diagonal
entry in the aggregation. This viewpoint leads directly to \cref{alg:chol}.

\begin{algorithm}
  \caption{Cholesky factorization by selection}
  \label{alg:chol}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s,
      g = \{ \tilde{i}_1, \dotsc, \tilde{i}_{N/m} \} \)
    \ENSURE \( L \) such that
      \( (L L^{\top})^{-1} \approx K(\vec{x}, \vec{x}) \)

    \STATE \( n \gets \lvert \vec{x} \rvert \)
    \FOR{ \( \tilde{i} \in g \)}
      \STATE \( J \gets
        \{ \max(\tilde{i}) + 1, \max(\tilde{i}) + 2, \dotsc, n \}
      \)
      \STATE Compute \( I \) using
        \cref{alg:gp_select_mult} or \cref{alg:select_mult_chol} \\ where
        \( \vec{x}_\text{Tr} = \vec{x}[J],
           \vec{x}_\text{Pr} = \vec{x}[\tilde{i}],
           s = s - \lvert \tilde{i} \rvert
        \)
      \STATE \( \tilde{s} \gets J[I] \)
      \FOR{\( i \in \text{reversed}(\text{sorted}(\tilde{i})) \)}
        \STATE \( \tilde{s} \gets \tilde{s} \cup \{ i \} \)
        \STATE \( s_i \gets \text{reversed}(\tilde{s}) \)
      \ENDFOR
    \ENDFOR
    \RETURN \( L \) computed with \cref{alg:L_mult}
  \end{algorithmic}
\end{algorithm}

\noindent
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ without aggregation}
  \label{alg:L}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), s_i \)
    \ENSURE \( L_{s_i, i} \)

    \STATE \( \Theta_{s_i, s_i}^{-1} \gets
      K(\vec{x}[s_i], \vec{x}[s_i])^{-1}
    \)
    \STATE \( L_{s_i, i} \gets \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
      {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \)
    \RETURN \( L_{s_i, i} \)
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
  \caption{Computing \( L \) \\ with aggregation}
  \label{alg:L_mult}
  \begin{algorithmic}[1]
    \REQUIRE \( \vec{x}, K(\cdot, \cdot), \tilde{s}, \tilde{i} \)
    \ENSURE \( L_{s_i, i} \) for all \( i \in \tilde{i} \)

    \STATE \( s \gets \tilde{i} \cup \tilde{s} \)
    \STATE \( U \gets P^{\updownarrow}
      \chol(P^{\updownarrow} \Theta_{s, s} P^{\updownarrow}) P^{\updownarrow}
    \)
    \FOR{\( i \in \tilde{i} \)}
      \STATE \( k \gets \) index of \( i \) in \( \tilde{i} \)
      \STATE \( L_{s_i, i} \gets U^{-\top} \vec{e}_k \)
    \ENDFOR
    \RETURN L
  \end{algorithmic}
\end{algorithm}
\end{minipage}

Once the sparsity pattern has been determined, we need to compute each column
of \( L \) according to \cref{thm:L}. Because the sparsity pattern for each
column in the same group are subsets of each other, we can efficiently compute
all their columns at once. The observation is that the smallest index in the
group (corresponding to the entry highest in the matrix) will have the largest
sparsity pattern, the next index will have one less entry (lacking the entry
above it, which would violate lower triangularity), and so on. We need to
compute \( \Theta_{s_i, s_i}^{-1} \vec{e}_1 \) for each \( i \in \tilde{i} \),
or the precision of the marginalized covariance corresponding to the selected
entries. By \cref{eq:inverse_cond}, we can turn marginalization in covariance
into conditioning in precision:
\begin{align}
  \nonumber
  \label{eq:L_precision}
  L_{s_i, i} &= \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}} \\
             &= \frac{(\Theta_{s, s})^{-1}_{k:, k: \mid :k - 1} \vec{e}_1}
             {\sqrt{\vec{e}_1^{\top} (\Theta_{s, s})^{-1}_{k:, k: \mid :k - 1}
                    \vec{e}_1}}
\end{align}
where \( s = \tilde{i} \cup \tilde{s} \) and \( k \) is \( i \)'s
index in \( \tilde{i} \). So we want the \( k \)th column of
the precision of the marginalized covariance, conditional on all the
entries before it. From \cref{eq:chol}, this can be directly read
off the Cholesky factorization. Thus, we can simply compute:
\begin{align}
  \label{eq:L_chol}
  L &= \chol \left ( \Theta_{s, s}^{-1} \right )
\end{align}
and read off the \( k \)th column to compute \cref{eq:L_precision} for each
\( i \in \tilde{i} \). However, instead of computing a lower triangular
factor for the precision, we can compute an \emph{upper} triangular factor
the covariance whose inverse transpose will be a \emph{lower} triangular
factor for the original matrix. In particular, we see that
\begin{align}
  \label{eq:U_chol}
  U &= P^{\updownarrow} \chol(P^{\updownarrow} \Theta_{s, s} P^{\updownarrow})
       P^{\updownarrow}
  \shortintertext{satisfies \( U U^{\top} = \Theta_{s, s} \) where
  \( P^{\updownarrow} \) is the order-reversing permutation. Thus,}
  \nonumber
  \Theta_{s, s}^{-1} &= U^{-\top} U^{-1}
\end{align}
where \( U^{-\top} \) is an \emph{lower} triangular factor for \( \Theta_{s,
s}^{-1} \) equal to \cref{eq:L_col} because the Cholesky factorization is
unique. Computing \( U^{-\top} \) leads directly to \cref{alg:L_mult}.

Recall that the complexity of selecting \( s \) out of \( N \) total training
points for \( m \) prediction points using \cref{alg:gp_select_mult} or
\cref{alg:select_mult_chol} was \( \mathcal{O}(N s^2 + N m^2 + m^3) \). In the
context of Cholesky factorization, \( N \) is the size of the matrix, \( m \)
is the number of columns to aggregate, and \( s \) is the number of nonzero
entries in each column of \( L \). We therefore need to do \( \frac{N}{m} \)
selections, one for each aggregated group, where we only need to select \( s
- m \) entries (since the \( m \) prediction points are automatically added).
We then need to actually construct each column of \( L \) after determining
the sparsity pattern, with \cref{alg:L_mult}. This costs \( \mathcal{O}(s^3)
\) for each aggregated group to compute the Cholesky factor of the submatrix,
which dominates the time to compute each column of \( L \) for the \( m \)
columns in the group, \( \mathcal{O}(m s^2) \) (\( N > s > m \)). Thus, the
overall complexity is \( \mathcal{O}(\frac{N}{m} (N (s - m)^2 + N m^2 + m^3 +
s^3)) \), which simplifies to \( \mathcal{O}(\frac{N^2 s^2}{m}) \) by making
use of the bound that \( (s - m)^2 = \mathcal{O}(s^2 + m^2) \).

Note that the non-aggregated factorization is equivalent to \( m = 1 \),
which yields \( \mathcal{O}(N^2 s^2) \) (using the non-aggregated algorithms
\cref{alg:gp_select,alg:L}, but one can also use the aggregated versions
\cref{alg:gp_select_mult,alg:L_mult} with \( m = 1 \) and achieve equivalent
complexity). Thus, we see that the aggregated version is \( m \) times faster
than its non-aggregated counterpart, at the cost that the resulting sparsity
pattern will be lower quality (since the algorithm is forced to select the
same entry for \emph{all} columns in the group).

Unlike the geometric algorithms of \cite{schafer2021sparse,
schafer2020compression} which rely on the pairwise distance between points,
and whose covariance matrix is implicitly determined by a list of points and
kernel function, this algorithm relies only on the entries of the covariance
matrix \( \Theta \). Thus, it can factor arbitrary symmetric positive definite
matrices without access to points or an explicit kernel function.

\section{Numerical experiments}

All experiments were run on the Partnership for an Advanced Computing
Environment (PACE) Phoenix cluster at the Georgia Institute of
Technology, with 8 cores of a Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz
and 6 GB of RAM per core. Python code for all numerical experiments
can be found at \href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\Stodo{cite python libraries that want citations (e.g. scikit-learn)}

\subsection{\textit{k}th-nearest neighbors selection}

We justify that diverse point selection based on conditional information can
lead to better performance than simply selecting the nearest neighbor in a toy
example on the MNIST dataset. We compare \( k \)th-nearest neighbors (KNN)
directly to conditional \( k \)th-nearest neighbors (CKNN) in the following
experiment. We randomly select 1000 images to form the training set and 100
to form the testing set. For each image in the testing set, we select the
\( k \) ``closest'' training points with either KNN or CKNN. For KNN we use
the standard Euclidean distance and for CKNN we use Mat{\'e}rn kernel with
smoothness \( \nu = 1.5 \) and length scale \( l = 2^{10} \). Finally, we
predict the label of the test point by taking the most frequently occurring
label in the \( k \) selected points.

\Stodo{cite mnist dataset}

\begin{figure}[h!]
  \centering
  \input{figures/mnist/accuracy_k.tex}
  \caption{Accuracy with increasing \( k \)}
\end{figure}

As \( k \) increases, KNN degrades near-linearly in accuracy. We hypothesize
that nearby images are more likely to have the same label as a given test
image. By forcing the algorithm to select more points, it increases the
likelihood that the algorithm becomes confused by differently labeled
images. However, CKNN is more accurate than KNN for nearly every \( k
\), suggesting that conditional selection is able to take advantage of
selecting more points. We emphasize that the difference in accuracy is
solely a result of conditional selection --- because the Mat{\'e}rn kernel
degrades monotonically with distance, sorting by covariance is identical
to sorting by distance. In addition, we use the mode to aggregate the
labels of the selected points, rather than performing Gaussian process
classification. The difference in accuracy can therefore be attributed
to precisely the difference in which points were selected.

\subsection{Recovery of sparse Cholesky factors}

As noted in \cref{app:omp}, the selection algorithm can be viewed as orthogonal
matching pursuit \cite{tropp2007signal} in feature space. We experiment
with the sparse recovery properties of the selection algorithm by randomly
generating a sparse Cholesky factor \( L \). We prescribe a fixed number
of nonzeros per column \( s \) over \( N \) columns. For each column, we
uniformly randomly pick \( s \) entry that satisfies lower triangularity to
make nonzero. We randomly generate values according to i.i.d. standard normal
\( \mathcal{N}(0, 1) \). Finally, we fill the diagonal with a ``large``
positive value (10) to almost guarantee that the resulting matrix \( \Theta =
L L^{\top} \) is positive definite. The selection algorithms are then given
\( \Theta \) and \( s \) and are asked to reconstruct \( L \). The strategies
are as follows: ``cknn'' uses conditional selection on each column to minimize
the conditional variance of the diagonal entry, ``knn'' selects entries with
the largest covariance with the diagonal entry, ``corr'' selects entries with
the highest correlation objective \cref{eq:obj_gp} without accounting for
conditioning, and ``random'' simply randomly samples entries uniformly. The
strategies are given either the covariance \( \Theta \) or the precision \(
\Theta^{-1} \) depending on which results in higher accuracy, in particular,
the ``cknn'' strategy is given the precision while the rest of the methods
are given the covariance. Accuracy is measured by taking the cardinality of
the intersection of the recovered sparsity set with the ground truth sparsity
set over the cardinality of their union, intersection over union (IOU).

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_n.tex}
  \caption{Accuracy with increasing \( N \)}
\end{figure}

As the number of nonzero entries per column is fixed and the number of rows
and columns is increased, the ```cknn'' retains high accuracy near perfect
recovery, and the rest of the methods quickly degrade and asymptote to their
final accuracies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_s.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

If the number of rows and columns is fixed while the number of nonzero entries
per column is increased, all methods drop in accuracy with increasing density
into a tipping point where the problem starts to become easier. Accuracy then
increases until the Cholesky factor becomes fully dense, in which case perfect
recovery is trivial. The ``cknn'' strategy exhibits the same behavior, but
maintains much higher accuracy than the rest of the strategies.

\begin{figure}[h!]
  \centering
  \input{figures/recover/accuracy_noise.tex}
  \caption{Accuracy with increasing \( s \)}
\end{figure}

Finally, we experiment with the addition of noise. Noise sampled i.i.d
from \( \mathcal{N}(0, \sigma^2) \) is added to each entry of \( \Theta
\) symmetrically (i.e. \( \Theta_{ij} \) receives the same noise as \(
\Theta_{ji} \)) to preserve the symmetry of \( \Theta \). As expected,
accuracy degrades with increasing noise, but the algorithm is fairly robust
to low levels of noise. At higher levels of noise, \( \Theta \) can lose
positive definiteness, which causes the algorithm to break down.

\subsection{Cholesky factorization}

\subsection{Gaussian process regression}

\subsection{Preconditioning for conjugate gradient}

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{cholesky}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Computation in sparse Gaussian process selection}

\subsection{Updating precision after insertion}
\label{app:prec_insert}

We have the matrix \( \Theta_{I, I}^{-1} \) corresponding to the precision of
the selected entries, and wish to take into account the addition of a new entry
\( k \) into \( I \). That is, we wish to compute \( \Theta_{I', I'}^{-1} \)
for \( I' = I \cup \{ k \} \), which in effect adds a new row and column to \(
\Theta_{I, I}^{-1} \). In order to invert the new matrix efficiently, we can
block the matrix to separate the new and old information.
\begin{align}
  \begin{pmatrix}
    \Theta_{1, 1} & \Theta_{1, 2} \\
    \Theta_{2, 1} & \Theta_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    I & 0 \\
    \textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \Theta_{2, 2} - \Theta_{2, 1} \Theta_{1, 1}^{-1} \Theta_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    I & \textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \shortintertext{For notational convenience, we denote the Schur
    complement \textcolor{lightblue}{\( \Theta_{2, 2} - \Theta_{2, 1}
    \Theta_{1, 1}^{-1} \Theta_{1, 2} \)} as \textcolor{lightblue}{\(
    \Theta_{2, 2 \mid 1} \)}. Inverting both sides of the equation,}
  \Theta^{-1} &=
  \begin{pmatrix}
    I & -\textcolor{darkorange}{\Theta_{1, 1}^{-1} \Theta_{1, 2}} \\
    0 & I
  \end{pmatrix}
  \begin{pmatrix}
    \Theta_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    I & 0 \\
    -\textcolor{darkorange}{\Theta_{2, 1} \Theta_{1, 1}^{-1}} & I
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \Theta_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    } \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    } \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\Theta_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry to the matrix,
    \( \Theta_{1, 1} = \Theta_{I, I} \), \( \Theta_{1, 2} = \Theta_{I,
    k} \), and \( \Theta_{2, 2} = \Theta_{kk} \). Also note that \(
    \textcolor{lightblue}{\Theta_{kk \mid I}^{-1}} \) is the inverse
    of the variance of \( k \) conditional on the entries in \( I \),
    which has already been computed in \cref{alg:gp_select}. If we let
    \( \vec{v} = \textcolor{darkorange}{\Theta_{I, I}^{-1} \Theta_{I,
    k}} \), then we can write the update as:}
  &=
  \begin{pmatrix}
    \Theta_{I, I}^{-1} + \Theta_{kk \mid I}^{-1} \vec{v} \vec{v}^T &
    -\Theta_{kk \mid I}^{-1} \vec{v} \\
    -\Theta_{kk \mid I}^{-1} \vec{v}^{\top} & \Theta_{kk \mid I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:gp_select}. Note that
the update is a rank-one update to \( \Theta_{1, 1}^{-1} \), which can
be computed in \( \mathcal{O}(\lvert I \rvert^2) = \mathcal{O}(s^2) \).

\subsection{Updating precision after marginalization}
\label{app:prec_delete}

Suppose we have the precision \( \Theta^{-1} \) and wish to compute the
precision of the marginalized covariance after ignoring an index \( k \).
That is, we wish to compute the inverse of a matrix after deleting a row and
column, given the inverse of the original matrix. We could use the result
in \cref{app:prec_insert} by ``reading'' the update backwards. That is, we
could identify \( \Theta_{2, 2 \mid 1}^{-1} \) from \( (\Theta^{-1})_{kk}
\) and \( \vec{v} = \Theta_{1, 1}^{-1} \Theta_{1, 2} \) from \( -
\frac{(\Theta^{-1})_{-k, k}}{\Theta_{2, 2 \mid 1}^{-1}} \) where \( -k \)
denotes all rows excluding the \( k \)th row. We can then revert the rank-one
update by subtracting out the update, computing \( \Theta_{-k, -k}^{-1} =
(\Theta^{-1})_{-k, -k} - \Theta_{kk \mid I}^{-1} \vec{v} \vec{v}^{\top} \).
However, a more intuitive derivation relies on the fact that marginalization
in covariance is conditioning in precision. Using \cref{eq:inverse_cond},
we see that \( \Theta_{-k, -k}^{-1} = (\Theta^{-1})_{-k, -k \mid k} \), or
the precision conditional on the deleted entry. By \cref{eq:cond_cov}, we
immediately obtain the equivalent update
\begin{align}
  (\Theta^{-1})_{-k, -k \mid k} &= \Theta^{-1}_{-k, -k} -
    \frac{(\Theta^{-1})_{-k, k}
          (\Theta^{-1})_{-k, k}^{\top}}{(\Theta^{-1})_{kk}}
\end{align}
Since this is a rank-one update to the precision \( \Theta^{-1} \), this
can be computed in \\ \( \mathcal{O}(\text{\# rows}(\Theta^{-1}))^2 \).

\Stodo{this is not used in the paper but is nice to know +
used in the sensor placement}

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

We have the matrix \( \Theta_{\text{Pr}, \text{Pr} \mid I}^{-1} \), or
the precision of the prediction points, conditional on the selected
entries. We want to take into account selecting an entry \( k \), or to compute
\( \Theta_{\text{Pr}, \text{Pr} \mid I \cup \{ k \}}^{-1} \) which is a rank-one
update to the original matrix from \cref{eq:obj_gp_mult}.
We can directly apply
the Sherman–Morrison–Woodbury formula which states that:
\begin{align}
  \Theta_{1, 1 \mid 2}^{-1} &= \Theta_{1, 1}^{-1} +
    \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    \Theta_{2, 2 \mid 1}^{-1}
    \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning by definition,}
  \left (
    \Theta_{1, 1} - \Theta_{1, 2} \Theta_{2, 2}^{-1} \Theta_{2, 1}
  \right )^{-1} &=
    \Theta_{1, 1}^{-1} +
    \left (\Theta_{1, 1}^{-1} \Theta_{1, 2} \right )
    \Theta_{2, 2 \mid 1}^{-1}
    \left (\Theta_{2, 1} \Theta_{1, 1}^{-1} \right )
  \shortintertext{Letting \( \vec{u} = \Theta_{1, 2} \) and \( \vec{v}
    = \Theta_{1, 1}^{-1} \Theta_{1, 2} = \Theta_{1, 1}^{-1} \vec{u} \),}
  (\Theta_{1, 1} - \Theta_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \Theta_{1, 1}^{-1} + \Theta_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{So we see that a rank-one update to \( \Theta_{1, 1} \)
    then inverting is a rank-one update to \( \Theta_{1, 1}^{-1} \). In our
    context, \( \Theta_{1, 1} = \Theta_{\text{Pr}, \text{Pr} \mid I}, \vec{u} =
    \Theta_{\text{Pr}, k \mid I}, \Theta_{2, 2} = \Theta_{kk \mid I} \) so \(
    \Theta_{2, 2 \mid 1}^{-1} = \Theta_{kk \mid \text{Pr}, I}^{-1} \) (this can
    be rigorously shown by expanding the Schur complement and taking advantage
    of the quotient rule as in \cref{eq:greedy_mult}). \( \vec{v} \) can be
    computed according to definition as \( \Theta_{\text{Pr}, \text{Pr} \mid
    I}^{-1} \vec{u} \). Thus, we can write the update as}
  \left ( \Theta_{\text{Pr}, \text{Pr} \mid I} -
    \frac{\Theta_{\text{Pr}, k \mid I} \Theta_{\text{Pr}, k \mid I}^{\top}}
    {\Theta_{kk \mid I}}
  \right )^{-1} &=
    \Theta_{1, 1}^{-1} +
    \Theta_{kk \mid \text{Pr}, I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:gp_select_mult}.
Since the update is a rank-one update, it can be computed in \(
\mathcal{O}(\lvert \text{Pr} \rvert^2) = \mathcal{O}(m^2) \).

\subsection{Updating Cholesky factorization after rank-one downdate}
\label{app:chol_downdate}

\Stodo{remove because unnecessary, describe insertion instead}

We use the approach from Lemma 1 of \cite{krause2015more}, slightly adapted
to use in-place operations and to make no assumption on the particular row
ordering of the Cholesky factor. Let \( L \) be a Cholesky factorization of \(
\Theta \), that is, \( L = \chol(\Theta) \). We wish to compute the updated
Cholesky factor \( L' = \chol(\Theta') \) where \( \Theta' = \Theta - \vec{u}
\vec{u}^{\top} \). To do so, assume \( L \) and \( L' \) are blocked according
to the same block structure:
\begin{align}
  L &=
  \begin{pmatrix}
    r_1 & \vec{0} \\
    \vec{r}_2 & L_2
  \end{pmatrix},
  L' =
  \begin{pmatrix}
    r_1' & \vec{0} \\
    \vec{r}_2' & L_2'
  \end{pmatrix}
  \shortintertext{Multiplying, we find}
  L L^{\top} = \Theta &=
  \begin{pmatrix}
    r_1^2 & r_1 \vec{r}_2^{\top} \\
    r_1 \vec{r}_2 & L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
  \end{pmatrix}
  \\ L' L'^{\top} = \Theta' &=
  \begin{pmatrix}
    r_1'^2 & r_1' \vec{r}_2'^{\top} \\
    r_1' \vec{r}_2' & L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top}
  \end{pmatrix}
  \shortintertext{From here, we solve for
    \( r'_1 \), \( \vec{r}' \), and \( L_2' \)}
  r_1'^2 &= \Theta'_{11} = \Theta_{11} - u_1^2 \\
         &= r_1^2 - u_1^2 \\
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  r_1' \vec{r}_2' &= \Theta'_{2:, 1} = \Theta_{2:, 1} - u_1 \vec{u}_2 \\
                  &= r_1 \vec{r}_2 - u_1 \vec{u}_2 \\
  \vec{r}_2' &= \frac{1}{r_1'} (r_1 \vec{r}_2 - u_1 \vec{u}_2) \\
  % L_2' L_2'^{\top} + \vec{r}_2' \vec{r}_2'^{\top} &= \Theta'_{22}
  %   = \Theta_{22} - \vec{u}_2 \vec{u}_2^{\top} \\
  L_2' L_2'^{\top} &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \vec{r}_2' \vec{r}_2'^{\top}
  \shortintertext{Plugging in the expresion for \( \vec{r}'_2 \),}
                   &= L_2 L_2^{\top} + \vec{r}_2 \vec{r}_2^{\top}
    - \vec{u}_2 \vec{u}_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right ) \left (
    \frac{r_1}{r_1'} \vec{r_2} - \frac{u_1}{r_1'} \vec{u_2} \right )^{\top} \\
                   &=  L_2 L_2^{\top} +
    \left ( 1 - \frac{r_1^2}{r_1'^2} \right ) \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
    \left ( 1 + \frac{u_1^2}{r_1'^2} \right ) \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{Using \( r_1' = \sqrt{r_1^2 - u_1^2} \),}
                   &=  L_2 L_2^{\top} -
      \frac{u_1^2}{r_1'^2} \vec{r}_2 \vec{r}_2^{\top} +
    \frac{r_1 u_1}{r_1'^2} \vec{r_2} \vec{u_2}^{\top} +
    \frac{u_1 r_1}{r_1'^2} \vec{u_2} \vec{r_2}^{\top} -
      \frac{r_1^2}{r_1'^2} \vec{u}_2 \vec{u}_2^{\top}
  \shortintertext{After factoring we find}
  L_2' L_2'^{\top} &= L_2 L_2^{\top} - \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right ) \left (
    \frac{r_1}{r_1'} \vec{u}_2 - \frac{u_1}{r_1'} \vec{r}_2
  \right )^{\top}
  \shortintertext{which is a rank-one downdate to the subfactor \( L_2 \).
    Recursively updating \( L_2 \) yields a \( \mathcal{O}(N^2) \) algorithm.
    We now re-write the algorithm to be in-place to take advantage of BLAS
    routines. The updates can be summarized as:}
  r_1' &= \sqrt{r_1^2 - u_1^2} \\
  \vec{r}' &= \frac{r_1}{r_1'} \vec{r} - \frac{u_1}{r_1'} \vec{u} \\
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'} \vec{r}
  \shortintertext{Note that we drop the subscripting on \( \vec{r} \) and \(
    \vec{u} \). By updating the entire vector on each iteration, we can avoid
    keeping track of the lower triangular structure of \( L \). We will first
    update \( \vec{r}' \) and then use it to update \( \vec{u} \). Solving for
    \( \vec{r} \) in terms of \( \vec{r}' \),}
  \vec{r} &= \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \\
  \shortintertext{Plugging the expression for \( \vec{r} \) into
    the update for \( \vec{u}' \),}
  \vec{u}' &= \frac{r_1}{r_1'} \vec{u} - \frac{u_1}{r_1'}
    \left ( \frac{r_1'}{r_1} \vec{r}' + \frac{u_1}{r_1} \vec{u} \right ) \\
           &= \frac{r_1^2 - u_1^2}{r_1 r_1'} \vec{u}
            - \frac{u_1}{r_1} \vec{r}' \\
           &= \frac{r_1'}{r_1} \vec{u} - \frac{u_1}{r_1} \vec{r}'
  \shortintertext{Thus, the updates proceed sequentally as follows:}
  \gamma &\gets \sqrt{r_1^2 - u_1^2} \\
  \alpha &\gets \frac{r_1}{\gamma} \\
  \beta &\gets \frac{u_1}{\gamma} \\
  \vec{r} &\gets \alpha \vec{r} - \beta \vec{u} \\
  \vec{u} &\gets \frac{1}{\alpha} \vec{u} - \frac{\beta}{\alpha} \vec{r}
\end{align}
These can be efficiently performed in-place by
BLAS as level-one \texttt{daxpy} operations.

\subsection{Updating Cholesky factor after insertion}
\label{app:chol_insert}

Suppose we have a cholesky factor \( L \) of \( \Theta \) and we insert
a new point into \( \Theta \). We wish to update the Cholesky \( L \) to
account for this insertion. Using the recursive conditioning interpretation
of Cholesky factorization in \cref{eq:chol}, we see that the columns of \( L
\) before the insertion will remain unchanged, the column at the insertion
point is a new column given by the conditional covariance of the new point
with the rest of the points, conditional on the points before it, which can
be computed with standard left-looking, and the columns of \( L \) after the
insertion correspond to the Cholesky factor of the conditional covariance,
conditional on the newly inserted point in addition to the previous points.
From \cref{eq:cond_select} we know that conditioning on an additional point is
a rank-one update of the covariance, so we can use rank-one downdating from
\cref{app:chol_downdate} to update \( L \) for the columns after the insertion
point, where the vector in the rank-one downdate is the newly inserted column.
This update touches every value in the Cholesky factor exactly once, so its
complexity is \( \mathcal{O}(N^2) \) as opposed to the \( \mathcal{O}(N^3)
\) cost of completely regenerating the Cholesky factor from scratch.

\subsection{Partial updates in the selection algorithm}
\label{app:partial}

In the context of the selection algorithm, we have \( M \) prediction points
and wish to minimize the log determinant of the resulting covariance matrix
of the prediction points, conditional on the points we've selected from the
training data. In the specific context of Cholesky factorization, it is
possible to add a training point and have it apply \emph{partially} on the
prediction points. If nonadjacent columns indices are aggregated, a entry
selected between two indices can be higher than one column, but lower than
another. Adding the entry to the sparsity pattern would therefore only add
to some, but not all, columns in the aggregation. We will model this as
partially conditioning the variables of interest. In particular, if we have
prediction variables \( y_1, y_2, \dotsc, y_M \), a partial condition ignoring
the first \( j \) variables on the selected index \( k \) would result in
\( y_1 , y_2, \dotsc, y_{j}, y_{j + 1 \mid k}, \dotsc, y_{M \mid k} \).

The first question is to compute the resulting covariance matrix. We know \(
\vec{y} \sim \mathcal{N}(\vec{0}, \Theta) \) and \( \vec{y}_{\mid k} \) has
conditional distribution according to \cref{eq:cond_cov}, \( \vec{y}_{\mid k}
\sim \mathcal{N}(\mu, \Theta - \Theta_{:, k} \Theta_{k, k}^{-1} \Theta_{k,
:}) \). Taking the Cholesky factorization of both covariance matrices, let \(
L = \chol(\Theta) \) and \( L_{\mid k} = \chol(\Theta_{\mid k}) \). We can
then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z} \) is distributed
according to \( \mathcal{N}(\vec{0}, I) \). Similarly, \( \vec{y}_{\mid k} =
L_{\mid k} \vec{z} + \vec{\mu} \). For unconditioned \( y_i \) and \( y_j \), the
covariance between them is defined to be \( \Theta_{ij} \). Similarly, for
conditioned \( y_i \) and \( y_j \), the covariance is \( \Theta_{ij \mid k}
\). The only question is what the covariance between unconditioned \( y_i \)
and conditioned \( y_j \) is. By definition,
\begin{align}
  \Cov[y_i, y_j] &= \E[(y_i - \E[y_i])(y_j - \E[y_j])] \\
                 &= \E[(L_i \vec{z}) (L_{i \mid k} \vec{z})] \\
                 &= \E[(L_{1, i} z_1 + \dotsb + L_{N, i} z_N)
                       (L_{1, j \mid k} z_1 + \dotsb + L_{N, j \mid k} z_N)]
  \shortintertext{For \( i \neq j \), \( E[z_i z_j] = \E[z_i] \E[z_j] = 0\)
    since \( z_i \) is independent of \( z_j \) and has mean 0.}
                 &= \E[L_{1, i} L_{1, j \mid k} z_1^2 + \dotsb +
                       L_{N, i} L_{N, j \mid k} z_N^2] \\
                 &= L_{1, i} L_{1, j \mid k}  \E[z_1^2] + \dotsb +
                    L_{N, i} L_{N, j \mid k} \E[z_N^2]
  \shortintertext{For any \( i \), \( \E[z_i^2]
    = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \)}
                 &= L_{1, i} L_{1, j \mid k} + \dotsb +
                    L_{N, i} L_{N, j \mid k} \\
                 &= L_i \cdot L_{j \mid k}
  \shortintertext{Thus, the new covariance matrix can be written as:}
  \label{eq:chol_partial}
  \begin{pmatrix}
    L_{:j} L_{:j}^{\top} & L_{:j} L_{j: \mid k}^{\top} \\
    L_{j: \mid k} L_{:j}^{\top} & L_{j: \mid k} L_{j: \mid k}^{\top}
  \end{pmatrix} &=
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:j} \\
    L_{j: \mid k}
  \end{pmatrix}^{\top}
  \shortintertext{We will denote a partially conditioned matrix as}
  \Theta_{:, :, \shortmid k}
\end{align}

\begin{figure}[h!]
  \centering
  \input{figures/cholesky_factor.tex}
  \caption{Illustration of the Cholesky
    factorization of a partially conditioned matrix.}
\end{figure}

We can now connect minimization of the log determinant of the partially updated
covariance matrix to the KL divergence objective of Cholesky factorization.
Computing the log determinant of the partially updated covariance
matrix, we make use of \cref{eq:chol_partial} and make use of the fact
that the determinant of a  triangular matrix is the product of its
diagonal entries:
\begin{align}
  \label{eq:partial_logdet}
  \frac{1}{2} \logdet(\Theta_{:, : \shortmid k}) &=
  \underbrace{\log(L_{11}) + \dotsb + \log(L_{jj})}_{\text{the same}} +
  \underbrace{
    \log(L_{j + 1, j + 1 \mid k}) + \dotsb + \log(L_{M, M \mid k})
  }_{\text{conditioned}}
  \shortintertext{Comparing to the KL divergence \cref{eq:obj_chol},
    \( \mathbb{D}_{\text{KL}}
      \left (
        \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
        \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
      \right )
    \) which is equivalent to maximizing}
  &= \sum_{i = 1}^M
      \log \left (
        \Theta_{ii \mid s_i - \{ i \}}
      \right )
  \shortintertext{Recalling that \( k \) is added partially to some \( s_i \),
    only those \( i > j \)}
  &= \underbrace{
      \log \left ( \Theta_{11 \mid s_1 - \{ 1 \}} \right ) + \dotsb +
      \log \left ( \Theta_{jj \mid s_j - \{ j \}} \right )
     }_\text{the same} + \\
  \nonumber
  &  \underbrace{
      \log \left ( \Theta_{j + 1, j + 1 \mid s_{j + 1} - \{ j + 1 \}} \right )
      + \dotsb + \log \left ( \Theta_{MM \mid s_M - \{ M \}} \right )
     }_\text{conditioned}
  \shortintertext{Since \( L_{ii} \) is the square root
    of the variance of the \( i \)th variable conditional
    on each entry before it in the ordering, we have}
  2 \log(L_{ii}) &= \log(\Theta_{ii \mid s_i - \{ i \}})
\end{align}
So minimizing the log determinant of the partially
conditioned covariance matrix \cref{eq:partial_logdet} is
the same as minimizing the KL divergence \cref{eq:obj_chol}.

\subsection{Algorithm for partial updates}
\label{app:partial_alg}

We now need an efficient algorithm to keep track of partial updates.
The key idea is to implicitly maintain the prediction matrix with
selected points inserted to maintain proper ordering, and keep track
of the log determinant throughout selection. We first give how this
different perspective affects the interpretation of the multiple
point selection algorithm. In the example, let \( x \) and \( y \)
be selected points and \( 1 \) and \( 2 \) be prediction points.
\begin{align}
  \Theta &=
  \begin{pmatrix}
    \Theta_{xx} & \Theta_{xy} & \Theta_{x1} & \Theta_{x2} \\
    \Theta_{yx} & \Theta_{yy} & \Theta_{y1} & \Theta_{y2} \\
    \Theta_{1x} & \Theta_{1y} & \Theta_{11} & \Theta_{12} \\
    \Theta_{2x} & \Theta_{2y} & \Theta_{21} & \Theta_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\Theta) &= \log(\Theta_{xx}) + \log(\Theta_{yy \mid x}) +
    \log(\Theta_{11 \mid x, y}) + \log(\Theta_{22 \mid x, y, 1})
  \shortintertext{Isolating the objective --- the variances of the
    prediction points}
  \log(\Theta_{11 \mid x, y}) + \log(\Theta_{22 \mid x, y, 1}) &=
    \logdet(\Theta) - \log(\Theta_{xx}) - \log(\Theta_{yy \mid x})
  \shortintertext{Now consider how inserting \( y \) changed the objective
    from when it was just \( x \).}
  \log(\Theta_{11 \mid x}) + \log(\Theta_{22 \mid x, 1}) &=
    \logdet(\Theta_{-y, -y}) - \log(\Theta_{xx}) \\
  \Delta &=
    \logdet(\Theta) - \log(\Theta_{yy \mid x}) - \logdet(\Theta_{-y, -y})
  \shortintertext{But from \cref{eq:greedy_mult} we know}
  \Delta &= \log \left (
      \frac{\Theta_{yy \mid x, 1, 2}}{\Theta_{yy \mid x}}
    \right )
  \shortintertext{Substituting,}
  \log(\Theta_{yy \mid x, 1, 2}) - \log(\Theta_{yy \mid x}) &=
    \logdet(\Theta) - \log(\Theta_{yy \mid x}) - \logdet(\Theta_{-y, -y}) \\
  \log(\Theta_{yy \mid x, 1, 2}) &=
    \logdet(\Theta) - \logdet(\Theta_{-y, -y})
  \shortintertext{In general,}
  \label{eq:logdet_diff}
  \log(\Theta_{kk \mid I, \text{Pr}}) &=
    \logdet(\Theta) - \logdet(\Theta_{-k, -k})
\end{align}
Another way to arrive at the same result is to note that if we inserted
\( y \) at the \emph{end} of \( \Theta_{-y, -y} \), to compute the log
determinant of the new, bigger matrix \( \Theta \) we would add the
variance of \( y \) conditional on every entry in the matrix to the
old determinant by chain rule. Since the determinant is invariant to
symmetric permutation, the matrix inserting \( y \) at the end has the
same determinant as inserting \( y \) where it should be.

So we see that the conditional variance of a candidate point conditional
on everything else in the matrix is the difference in log determinant
between the matrix with the candidate inserted and the original matrix.
The multiple prediction point algorithm can therefore be interpreted as
we insert the candidate \emph{after} all the previously selected points
(so it is conditional on all the previous points) and \emph{before}
the prediction points (which conditions all of them). We then compute
\( \log(\Theta_{kk \mid I, \text{Pr}}) \) for some candidate \( k \)
which represents the difference in log determinant and then subtract \(
\log(\Theta_{kk \mid I}) \) which is the spurious variance introduced by
inserting \( k \) into the matrix. We do not need to subtract the spurious
variances from the previously selected points because \( k \) does not
affect them, and we select candidates by \emph{relative} score.

We now apply this result to partial selection. In the example,
let \( 1 \) and \( 2 \) be prediction points while \( x \) and \(
y \) are both a selected points below \( 2 \) but above \( 1 \), where
\( x \) has already been selected and \( y \) is a candidate.
\begin{align}
  \Theta &=
  \begin{pmatrix}
    \Theta_{11} & \Theta_{1y} & \Theta_{1x} & \Theta_{12} \\
    \Theta_{y1} & \Theta_{yy} & \Theta_{yx} & \Theta_{y2} \\
    \Theta_{x1} & \Theta_{xy} & \Theta_{xx} & \Theta_{x2} \\
    \Theta_{21} & \Theta_{2y} & \Theta_{2x} & \Theta_{22}
  \end{pmatrix}
  \shortintertext{Computing the log determinant by chain rule,}
  \logdet(\Theta) &= \log(\Theta_{11}) + \log(\Theta_{yy  \mid 1}) +
    \log(\Theta_{xx \mid 1, y}) + \log(\Theta_{22 \mid 1, y, x})
  \shortintertext{We see that \( y \) conditions \( 2 \) but not \( 1 \),
    precisely what we want to encode. However, we have introduced a spurious
    term \( \log(\Theta_{yy \mid 1}) \) and changed the variance of \( x \),
    both of which must be subtracted out.}
  \log(\Theta_{11}) + \log(\Theta_{22 \mid 1, y, x}) &=
  \logdet(\Theta) - \log(\Theta_{yy \mid 1}) - \log(\Theta_{xx \mid 1, y})
  \shortintertext{We can substitute \( \log(\Theta_{yy \mid 1, x, 2}) \)
    for \( \logdet(\Theta) \) by \cref{eq:logdet_diff}. Athough
    it differs by a constant, this does not change the objective.}
  &= \log(\Theta_{yy \mid 1, x, 2}) -
    \log(\Theta_{yy \mid 1}) - \log(\Theta_{xx \mid 1, y})
\end{align}

As long as we can compute conditional variances of our candidate on each
\emph{prefix} of the current ordering of prediction points interspersed with
selected points, we can use the conditional variances to compute the updated
conditional variances of the selected points by using their conditional
covariances with the candidate. We are then able to compute every term
in the objective. To do so, we maintain a partial Cholesky factor whose
ordering is given by the current ordering. When we select a new point, we
insert it in its appropriate place in the Cholesky factor. To update the
Cholesky factor after an insertion efficiently, we left-look to get the
column of its insertion position, and then update all columns right of the
column by a rank-one downdate as described in \cref{app:chol_insert} which
touches every entry in the Cholesky factor, \( \mathcal{O}(N(m + s) \) per
update for a total cost of \( \mathcal{O}(N(m + s)(s)) \) over \( s \)
selections. In addition, the algorithm can be considerably simplified by
simply adding the conditional variances of the prediction points, instead
of starting with a proxy for the log determinant of the entire matrix and
subtracting out the spurious interactions from the training points.

By inspecting the Cholesky factor, we get the covariance of a selected point
with a candidate, conditional on all the points prior to the selected point in
the ordering. The conditional variance of the selected point is the diagonal
entry. We can then compute the new conditional variance given the variance of
the candidate, conditional on all points prior to the selected point. Suppose
we are at index \( i \) and the candidate is index \( j \), the updates are as
follows:
\begin{align}
  \Theta_{ii \mid :i - 1} &= (L_{ii})^2 \\
  \Theta_{ij \mid :i - 1} &= L_{ij} \cdot L_{ii} \\
  \Theta_{ii \mid :i - 1, j} &= \Theta_{ii \mid :i - 1} -
    \frac{\Theta_{ij \mid :i - 1}^2}{\Theta_{jj \mid :i - 1}} \\
  \Theta_{jj \mid :i - 1, i} &= \Theta_{jj \mid :i - 1} -
    \frac{\Theta_{ij \mid :i - 1}^2}{\Theta_{ii \mid :i - 1}} \\
                             &= \Theta_{jj \mid :i}
\end{align}
Of course, the base case \( \Theta_{jj} \) is simply \(
K(\vec{x}_j, \vec{x}_j) \), the variance of the \( j \)th point.

For each of the \( N \) candidates, it requires \( m + s \) operations from
the above updates to compute the objective. Over \( s \) selections, the
total time is the same as the cost to update the Cholesky factor, matching
the complexity of the non-partial multiple point algorithm. However, the
asymptotic work in the non-partial algorithm can be implemented as BLAS
level-2 calls, while the partial algorithm relies heavily on vector (level-1)
calls, affecting the constant-factor performance of the algorithm.

\subsection{Global greedy selection}
\label{app:global_greedy}

Although each column is essentially independent from the perspective of
selection, if there is a prescribed budget for the number of nonzeros then
there is the problem of distributing the nonzeros over the columns. A
natural method is to distribute as evenly as possible, this is efficient and
practically useful. However, one principled way of allocating nonzeros is
to maintain a \emph{global} priority queue over all columns, and selecting
from this queue determines not only which entry out of the candidate set is
added as a nonzero, but also which column to select from. This allows the
algorithm to greedily select the next entry which will decrease the global
objective \cref{eq:obj_chol} as much as possible. The main change is that
within a column, any monotonic transformation of the objective will preserve
the relative ranking of candidates, for example adding a constant, multiplying
by a constant, taking the log, etc. However, from the global perspective, if
one column adds a different constant to their objectives than another column,
the relative ranking of candidates between columns is skewed. Thus, each
column must maintain an objective that corresponds directly to minimizing the
global objective \cref{eq:obj_chol}. Here we describe the modifications that
must be made to the selection algorithms to support global comparison.

\subsubsection{Single column selection}

For a single prediction point, the objective is \( \frac{\Theta_{k, \text{Pr}
\mid I}^2}{\Theta_{kk \mid I}} \) \cref{eq:obj_gp} which is exactly the amount
the variance of the prediction point is decreased if the \( k \)th candidate is
selected, that is, \( \Theta_{\text{Pr}, \text{Pr} \mid I} - \Theta_{\text{Pr},
\text{Pr} \mid I, k} = \frac{\Theta_{k, \text{Pr} \mid I}^2}{\Theta_{kk \mid
I}} \). From the global perspective, all other prediction points are untouched,
so the amount the sum of the log variances of all the prediction points changes
is
\begin{align}
  \min \Delta &= \min \left [
    \log(\Theta_{\text{Pr}, \text{Pr} \mid I, k}) -
  \log(\Theta_{\text{Pr}, \text{Pr} \mid I}) \right ] \\
              &= \min \frac{\Theta_{\text{Pr}, \text{Pr} \mid I, k}}
                           {\Theta_{\text{Pr}, \text{Pr} \mid I}} \\
              &= \min \frac{\Theta_{\text{Pr}, \text{Pr} \mid I}
              - \frac{\Theta_{k, \text{Pr} \mid I}^2}{\Theta_{kk \mid I}}}
                           {\Theta_{\text{Pr}, \text{Pr} \mid I}} \\
              &= \min \left [ 1 -
                \frac{\Theta_{k, \text{Pr} \mid I}^2}
                     {\Theta_{kk \mid I} \Theta_{\text{Pr}, \text{Pr} \mid I}}
                 \right ] \\
  \label{eq:global_obj}
              &= \max
                \frac{\Theta_{k, \text{Pr} \mid I}^2}
                     {\Theta_{kk \mid I} \Theta_{\text{Pr}, \text{Pr} \mid I}}
\end{align}
where \cref{eq:global_obj} can be interpreted as the
\emph{percentage} of the decrease in variance from selecting
the \( k \)th point to the variance before selecting the point.

\subsubsection{Aggregated selection}

Since the nonadjacent algorithm directly computes the sum of the log of
the conditional variances of the prediction points, few modifications
have to be made. One heuristical improvement is to take into account
for ``bang-for-buck'', that is, to account for the fact that different
candidates cost a different amount of nonzero entries. Selecting a
candidate can add between 1 and the number of columns in its aggregated
group, depending on its relative index. Thus, candidates with larger
groups will appear to decrease the global variance more, even if they
are not as efficient as a candidate with a single group. In practice, it
is better to use the objective \( \frac{\Delta}{n} \) where \( \Delta
\) is the change in variance after selecting the candidate and \( n \)
is the number of nonzero entries selecting the candidate adds.

\subsection{Equivalence of Cholesky and QR factorization}
\label{app:qr}

We show a well-known fact that QR factorization can be viewed
as the feature-space equivalent of Cholesky factorization,
which can be viewed as operating in covariance-space.
\begin{align}
  \shortintertext{Let \( \Theta \) be a symmetric
    positive definite matrix such that}
  \Theta &= F^{\top} F
  \shortintertext{for some matrix \( F \) whose
    columns can be viewed as vectors in feature space:}
  \Theta_{ij} &= \langle F_i, F_j \rangle
  \shortintertext{where \( F_i \) is the \( i \)th column of
    \( F \). Now suppose \( F \) has the \( QR \) factorization}
  F &= QR
  \shortintertext{where \( Q \) is a \( N \times N \) orthonormal
    matrix and \( R \) is a \( N \times N \) upper triangular matrix.}
  \Theta &= F^{\top} F = (Q R)^{\top} (Q R) \\
         &= R^{\top} Q^{\top} Q R \\
         &= R^{\top} R
\end{align}
from the orthogonality of \( Q \). But note that \( R \) is an upper triangular
matrix, so \( L = R^{\top} \) is an lower triangular matrix. So we have \(
\Theta = L L^{\top} \) for lower triangular \( L \). By the uniqueness of
Cholesky factorization, \( R^{\top} \) is precisely the Cholesky factor of \(
\Theta \). In addition, the columns of \( Q \) are formed from Gram-Schmitt
orthogonalization on the columns of \( F \) (in feature-space), and \( R \)
the coefficients resulting from the Gram-Schmitt procedure. From \( R^{\top} =
\chol(\Theta) \) and the statistical interpretation of Cholesky factorization
\cref{eq:chol}, this iterative orthogonalization in feature-space is equivalent
to iterative conditioning in covariance.

\subsection{Equivalence of selection and orthogonal matching pursuit}
\label{app:omp}

We show that the single-point selection algorithm described in
\cref{alg:gp_select} is the covariance space equivalent to the
feature space orthogonal matching pursuit (OMP) algorithm described
in \cite{tropp2007signal}. The equivalence comes from the fact
that Cholesky factorization is Gram-Schmitt in feature space.

\begin{align}
  \shortintertext{Let \( \Theta \) be a symmetric
    positive definite matrix such that}
  \Theta &= F^{\top} F
  \shortintertext{for some matrix \( F \)
    whose columns are vectors in feature space,}
  F &=
  \begin{pmatrix}
    \vec{x}_1 & \vec{x}_2 & \hdots & \vec{x}_N
  \end{pmatrix}
  \shortintertext{Immediately we have}
  \Theta_{ij} &= \langle \vec{x}_i, \vec{x}_j \rangle
  \shortintertext{where \( \langle \cdot, \cdot \rangle \)
    denotes the ordinary inner product on \( \mathbb{R}^N \).}
  \shortintertext{It suffices to see a single step of Cholesky
    factorization. Selecting \( \vec{x}_1 \),}
  \Theta' &= \Theta - \frac{\vec{x}_1 \vec{x}_1^{\top}}
    {\Theta_{11}} \\
  \label{eq:cov_step}
  \Theta_{ij}' &= \Theta_{ij} -
    \frac{\Theta_{i1} \Theta_{j1}}{\Theta_{ii}}
  \shortintertext{Switching to the feature space perspective,
    if we select \( \vec{x}_1 \) we force the rest of the
    feature vectors to be orthogonal to \( \vec{x}_1 \),}
  \vec{x}_i' &= \vec{x}_i -
    \frac{\langle \vec{x}_i, \vec{x}_1 \rangle}
         {\langle \vec{x}_1, \vec{x}_1 \rangle} \vec{x}_1 \\
  \label{eq:feature_step}
  \langle \vec{x}_i', \vec{x}_j' \rangle &=
    \langle \vec{x}_i, \vec{x}_j \rangle -
      \frac{\langle \vec{x}_i, \vec{x}_1 \rangle
            \langle \vec{x}_j, \vec{x}_1 \rangle}
          {\langle \vec{x}_1, \vec{x}_1 \rangle}
  \shortintertext{Comparing \cref{eq:cov_step} and \cref{eq:feature_step},
    we see that they are the same as expected. As a corollary, the objective
    of selecting the point \( \vec{x}_k \) that minimizes the residual of some
    target point \( \vec{x}_\text{Pr} \) can be written as}
  \lVert
    \vec{x}_\text{Pr} - \text{proj}_{\vec{x}_k} \vec{x}_\text{Pr}
  \rVert &= \langle \vec{x}_\text{Pr}, \vec{x}_\text{Pr} \rangle -
    \frac{\langle \vec{x}_\text{Pr}, \vec{x}_k \rangle^2}
        {\langle \vec{x}_k, \vec{x}_k \rangle}
  % \shortintertext{By induction, one can show}
  % F_2' = F_2 - P_1 F_2
  % \shortintertext{where \( F_2 \) is a set of feature
  %   vectors being conditioned and \( P_1 \) is the projection
  %   matrix onto the subspace spanned by \( F_1 \).}
\end{align}
which is precisely the squared covariance of the candidate with the
prediction over the variance of the candidate, as in \cref{eq:obj_gp}.
This shows the equivalence as the objective is the same.

\subsection{Checking submodularity}
\label{app:submodular}

Our objective is the mutual information between the prediction and training
points \cref{eq:info}. A natural question is whether this objective is
submodular with respect to the training set. The answer is no in general,
see \cite{krause2008nearoptimal}, section 8.3 for a simple counterexample.
However, we can empirically check submodularity for particular geometries
and choices of kernel function. If \( \text{Pr} \) is the set of prediction
points, \( I \) is a set of indexes, and \( x_1, x_2 \) are additional
indices not in \( I \), then the set function is submodular if and only if
\begin{align*}
  \I(\text{Pr}, I \cup \{ x_2 \}) - \I(I) &\overset{?}{\geq}
    \I(\text{Pr}, I \cup \{ x_1, x_2 \}) - \I(\text{Pr}, I \cup \{ x_1 \})
  \shortintertext{Expanding from the definition
    of mutual information \cref{eq:info},}
  \entropy[\text{Pr} \mid I] - \entropy[\text{Pr} \mid I \cup \{ x_2 \}]
    &\overset{?}{\geq}
    \entropy[\text{Pr} \mid I \cup \{ x_1 \}] -
    \entropy[\text{Pr} \mid I \cup \{ x_1, x_2 \}]
  \shortintertext{Since this is the objective \cref{eq:obj_gp_mult}
    (with additional log),}
  \frac{\Theta_{x_2, x_2 \mid I}}{\Theta_{x_2, x_2 \mid I, \text{Pr}}}
    &\overset{?}{\geq}
    \frac{\Theta_{x_2, x_2 \mid I} -
          \frac{\Theta_{x_1, x_2 \mid I}^2}{\Theta_{x_1, x_1 \mid I}}}
         {\Theta_{x_2, x_2 \mid I, \text{Pr}} -
         \frac{\Theta_{x_1, x_2 \mid I, \text{Pr}}^2}
              {\Theta_{x_1, x_1 \mid I, \text{Pr}}}}
    \shortintertext{From the fact that \(
    \frac{a}{b} \geq \frac{a - c}{b - d} \) if and only if
     \( \frac{a}{b} \leq \frac{c}{d} \),}
  \frac{\Theta_{x_2, x_2 \mid I}}{\Theta_{x_2, x_2 \mid I, \text{Pr}}}
    &\overset{?}{\leq}
    \frac{\frac{\Theta_{x_1, x_2 \mid I}^2}{\Theta_{x_1, x_1 \mid I}}}
         {\frac{\Theta_{x_1, x_2 \mid I, \text{Pr}}^2}
               {\Theta_{x_1, x_1 \mid I, \text{Pr}}}}
  \shortintertext{Multiplying by \( \frac{\Theta_{x_1, x_1 \mid
    I}}{\Theta_{x_1, x_1 \mid I, \text{Pr}}} \) on both sides,}
  \frac{\Theta_{x_1, x_1 \mid I} \Theta_{x_2, x_2 \mid I}}
       {\Theta_{x_1, x_1 \mid I, \text{Pr}}
        \Theta_{x_2, x_2 \mid I, \text{Pr}}}
    &\overset{?}{\leq}
    \frac{\Theta_{x_1, x_2 \mid I}^2}
         {\Theta_{x_1, x_2 \mid I, \text{Pr}}^2}
  \shortintertext{Multiplying by \( \frac{\Theta_{x_1,
    x_2 \mid I, \text{Pr}}^2}{\Theta_{x_1, x_1 \mid
    I} \Theta_{x_2, x_2 \mid I}} \) on both sides,}
  \frac{\Theta_{x_1, x_2 \mid I, \text{Pr}}^2}
       {\Theta_{x_1, x_1 \mid I, \text{Pr}}
        \Theta_{x_2, x_2 \mid I, \text{Pr}}}
    &\overset{?}{\leq}
    \frac{\Theta_{x_1, x_2 \mid I}^2}
         {\Theta_{x_1, x_1 \mid I} \Theta_{x_2, x_2 \mid I}}
  \shortintertext{By definition, this is}
  \Corr[x_1, x_2 \mid I, \text{Pr}] &\overset{?}{\leq}
    \Corr[x_1, x_2 \mid I]
\end{align*}
so the mutual information objective is submodular if and only if
conditioning on additional point(s) decreases the correlation between
every pair of points. Intuitively, this corresponds to the screening
effect observed in spatial statistics literature --- conditioning on
a nearby point decreases the correlation for far away points.

\section{Derivations in KL-minimization}

\subsection{Linear-algebraic formulation of objective}
\label{app:kl_obj}

We want to show that the KL divergence between two multivariate
Gaussians centered at \( \vec{0} \) with covariance matrices
\( \Theta_1 \) and \( \Theta_2 \) can be written as
\begin{align}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  &= \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
  \shortintertext{where \( \Theta_1 \) and \( \Theta_2 \) are both
    of size \( N \times N \). Recall that the log density \( \log
    \pi(\vec{x}) \) for \( \vec{x} \sim \mathcal{N}(\vec{0}, \Theta) \) is}
  \log \pi(\vec{x}) &= -\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta) +
    \vec{x}^{\top} \Theta^{-1} \vec{x})
  \shortintertext{By the definition of KL divergence,}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right ) &= 2 \E_P[\log P - \log Q]
  \shortintertext{where \( P \) and \( Q \) are the corresponding
    densities for \( \Theta_1 \) and \( \Theta_2 \) respectively,
    and \( \E_P \) denotes expectation under \( P \).}
  &= 2 \E_P[-\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta_1) +
            \vec{x}^{\top} \Theta_1^{-1} \vec{x}) \\
  \nonumber
  & \hphantom{= 2 \E_P[}
            +\frac{1}{2} (N \log(2 \pi) + \logdet(\Theta_2) +
            \vec{x}^{\top} \Theta_2^{-1} \vec{x})] \\
  \label{eq:kl_after_logdet}
  &= \E_P[\vec{x}^{\top} \Theta_2^{-1} \vec{x} -
          \vec{x}^{\top} \Theta_1^{-1} \vec{x}]
          + \logdet(\Theta_2) - \logdet(\Theta_1) \\
  \E_P[\vec{x}^{\top} \Theta_2^{-1} \vec{x} -
          \vec{x}^{\top} \Theta_1^{-1} \vec{x}]
  &=
  \E_P[\trace(\vec{x}^{\top} \Theta_2^{-1} \vec{x}) -
       \trace(\vec{x}^{\top} \Theta_1^{-1} \vec{x})]
\end{align}
because the trace of a scalar is a scalar, and the linearity of trace.
\begin{align}
  &=
  \E_P[\trace(\Theta_2^{-1} \vec{x} \vec{x}^{\top}) -
       \trace(\Theta_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{cyclic property of trace} \\
  &=
  \E_P[\trace(\Theta_2^{-1} \vec{x} \vec{x}^{\top} -
              \Theta_1^{-1} \vec{x} \vec{x}^{\top})]
  && \text{linearity of trace} \\
  &= \E_P[\trace \left (
    (\Theta_2^{-1} - \Theta_1^{-1}) \vec{x} \vec{x}^{\top} \right )]
  && \text{factoring} \\
  &= \trace(\E_P \left [
    (\Theta_2^{-1} - \Theta_1^{-1}) \vec{x} \vec{x}^{\top} \right])
  && \text{swapping trace and expectation} \\
  &= \trace((\Theta_2^{-1} - \Theta_1^{-1})
    \E_P \left [ \vec{x} \vec{x}^{\top} \right])
  && \text{linearity of expectation} \\
  &= \trace((\Theta_2^{-1} - \Theta_1^{-1}) \Theta_1)
  && \text{\( \Theta_1 = \E_P[\vec{x} \vec{x}^{\top} \)]} \\
  &= \trace(\Theta_2^{-1} \Theta_1 - I)
  && \text{multiplying} \\
  &= \trace(\Theta_2^{-1} \Theta_1) - \trace(I)
  && \text{linearity of trace} \\
  &= \trace(\Theta_2^{-1} \Theta_1) - N
  \label{eq:kl_after_trace}
  && \text{trace of \( N \times N \) identity \( N \)}
\end{align}
Combining \cref{eq:kl_after_trace} with \cref{eq:kl_after_logdet}, we obtain
\begin{align}
  \nonumber
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta_1) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta_2)
  \right )
  &= \trace(\Theta_2^{-1} \Theta_1) + \logdet(\Theta_2) - \logdet(\Theta_1) - N
\end{align}
as desired.

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL divergence between \( \Theta \) and the Cholesky
factor \( L \) computed according to \cref{thm:L}. From \cref{eq:kl},
\begin{align}
  \label{eq:kl_L}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= \trace(L L^{\top} \Theta) - \logdet(L L^{\top}) - \logdet(\Theta) - N
  \shortintertext{Ignoring terms not depending on \( L \),}
  &= \trace(L L^{\top} \Theta) - \logdet(L L^{\top})
  \shortintertext{By the cyclic property of trace,}
  &= \trace(L \Theta L^{\top}) - \logdet(L L^{\top})
  \shortintertext{Focusing on \( \trace(L \Theta
    L^{\top}) \) and expanding on the columns of \( L \),}
  \trace(L \Theta L^{\top}) &= \sum_{i = 1}^N
    L_{s_i, i}^{\top} \Theta_{s_i, s_i} L_{s_i, i}
  \shortintertext{Plugging in \( L_{s_i, i} \) from \cref{thm:L},}
  &= \sum_{i = 1}^N
    \left (
      \frac{\left ( \Theta_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \Theta_{s_i, s_i}
    \left (
      \frac{\Theta_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1}}
    \right ) \\
  &= \sum_{i = 1}^N
    \frac{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1}
          \Theta_{s_i, s_i} \Theta_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1} \\
  &= \sum_{i = 1}^N 1 = N
\end{align}
\begin{align}
  \shortintertext{Using \( N \) for \( \trace(L
    L^{\top} \Theta) \) in \cref{eq:kl_L},}
  2 \mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right )
  &= -\logdet(L L^{\top}) - \logdet(\Theta)
  \shortintertext{\( L^{\top} \) has the same log determinant
    as \( L \), and because \( L \) is lower triangular, its
    log determinant is just the sum of its diagonal entries:}
  &= -2 \sum_{i = 1}^N \left [ \log(L_{ii}) \right ] - \logdet(\Theta)
  \shortintertext{Plugging \cref{eq:L_col} for the diagonal entries,}
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\Theta)
  \shortintertext{Bringing the negative inside,}
  &= \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\Theta)
\end{align}
So minimizing the KL divergence (given optimal \( L \)) corresponds to
minimizing the sum of the inverse of the diagonal entries. We can give
an intuitive view of this result by making use of \cref{eq:L_cond_var}
and expanding the log determinant by the chain rule \cref{eq:det_chain}.
\begin{align}
  \sum_{i = 1}^N
    \left [
      \log \left (
        (\vec{e}_1^{\top} \Theta_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\Theta)
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \Theta_{ii \mid s_i - \{ i \}} \right )
    \right ]
    - \logdet(\Theta) \\
  &= \sum_{i = 1}^N
      \log \left ( \Theta_{ii \mid s_i - \{ i \}} \right ) -
    \sum_{i = 1}^N
      \log \left ( \Theta_{ii \mid i + 1:} \right ) \\
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \Theta_{ii \mid s_i - \{ i \}} \right ) -
      \log \left ( \Theta_{ii \mid i + 1:} \right )
    \right ]
\end{align}
We can view this sum as the accumulated \emph{difference} in prediction
error for a series of prediction problems, where each prediction problem is
to predict the value of the \( i \)th variable given variables \( i + 1, i
+ 2, \dotsc, N \). The left term \( \log \left ( \Theta_{ii \mid s_i - \{ i
\}} \right ) \) is restricted to only using those variables in the sparsity
pattern \( s_i \), while the right term \( \log \left ( \Theta_{ii \mid i +
1:} \right ) \) is able to use every variable after \( i \). The left term
will necessarily have greater variance than the right, and the goal is to
minimize the accumulated deviation. Thus, the KL divergence gives a natural
way to measure the quality of a sparsity pattern as a good sparsity pattern
should maintain predictive accuracy while subject to the constraint that
some variables have no interaction with others. This interpretation is also
given in \cite{katzfuss2022scalable}.

Another interpretation is from \cite{bartels2022adaptive}, where they view
the full prediction problems as the log likelihood of the variables. Under
this interpretation, conditional independence (through the screening effect)
corresponds to a near-constant value of \( \log(\Theta_{ii \mid i + 1:}) \),
which results in a linear plot of log-likelihood with increasing \( N \).

Because the KL divergence is not symmetric, it matters which way the
KL divergence is taken as well as whether both matrices have been
inverted or not. This seems to imply that there are four possible
ways to compare two covariance matrices. However, note that
\begin{align}
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, \Theta) \, \Big \| \,
    \mathcal{N}(\vec{0}, (L L^{\top})^{-1})
  \right ) &=
\mathbb{D}_{\text{KL}}
  \left (
    \mathcal{N}(\vec{0}, L L^{\top}) \, \Big \| \,
    \mathcal{N}(\vec{0}, \Theta^{-1})
  \right )
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL divergence. There are
therefore only two possible ways to compare the two, which depends on
the order of the arguments. A statistical interpretation comes from the
fact that the KL divergence can be interpreted as the likelihood-ratio
test, so the non-symmetry of the order of the arguments corresponds to
the asymmetry between the null and alternative hypotheses.

\end{document}
