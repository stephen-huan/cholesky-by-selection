% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{experimental_design_linalg_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Sparse Cholesky factorization by greedy conditional selection},
  pdfauthor={S. Huan, J. Guinness, M. Katzfuss, H. Owhadi, F. Sch{\"a}fer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{experimental_linalg_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name">
%</named-content>
%<named-content content-type="funder-identifier">
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Dense kernel matrices resulting from pairwise evaluations of a
  kernel function arise naturally in machine learning and statistics.
  Previous work in constructing sparse transport maps or sparse approximate
  inverse Cholesky factors of such matrices by minimizing Kullback-Leibler
  divergence recovers the Vecchia approximation for Gaussian processes.
  However, these methods often rely only on geometry to construct the
  sparsity pattern, ignoring the conditional effect of adding an entry.
  In this work, we construct the sparsity pattern by leveraging a
  greedy selection algorithm that maximizes mutual information with
  target points, conditional on all points selected previously.
  For selecting \( k \) points out of \( N \), the naive time
  complexity is \( \BigO(N k^4) \), but by maintaining a
  partial Cholesky factor we reduce this to \( \BigO(N k^2) \).
  Furthermore, for multiple (\( m \)) targets we achieve a time complexity
  of \( \BigO(N k^2 + N m^2 + m^3) \) which is maintained in the setting of
  aggregated Cholesky factorization where a selected point need not condition
  every target.
  We directly apply the selection algorithm to image
  classification and recovery of sparse Cholesky
  factors, improving upon \( k \)-th nearest neighbors.
  By minimizing Kullback-Leibler divergence, we apply the
  algorithm to Cholesky factorization, Gaussian process
  regression, and preconditioning with the conjugate gradient.
\end{abstract}

% REQUIRED
\begin{keywords}
  \todo{to do}
\end{keywords}

% REQUIRED
\begin{AMS}
  % 68Q25, 68R10, 68U05
\end{AMS}

\Stodo{20 pages, move things to supplement}
\section{Introduction}

\paragraph{The problem}

Gaussian processes enjoy widespread application in spatial
statistics and geostatistics \cite{rue2005gaussian}, machine
learning through kernel methods \cite{rasmussen2006gaussian},
optimal experimental design \cite{mutny2022experimental}, and
sensor placement \cite{krause2008nearoptimal}.
However, Gaussian process statistics from \( N \) data points requires
computing with the covariance matrix \( \CM \in \Reals^{N \times N} \)
to obtain quantities such as \( \CM \vec{v} \), \( \CM^{-1} \vec{v} \),
\( \logdet(\CM) \).
For dense \( \CM \), directly computing these quantities has a
computational cost of \( \BigO(N^3) \) and a memory cost of \(
\BigO(N^2) \), which is prohibitively expensive for large \( N \).
Beyond Gaussian processes, computations with large positive-definite
matrices are required across computational mathematics,
motivating the search for faster, approximate algorithms.

\paragraph{Existing work}

\todo{I would put knothe rosenblatt, FSAI, and sparse Cholesky
factorization into the Vecchia paragraph and use the ``existing work''
paragraph to disucuss other approaches such as inducing points,
wavelet method, h matrices, fast multiplol, random features, Nystrom
approximation, sparse Cholesky of covariance, etc.}
TODO

\paragraph{Vecchia approximation}

\todo{I would make this one integrated section on
Vecchia, FSAI, KL-Cholesky, and transport maps}
The Vecchia approximation \cite{vecchia1988estimation} decomposes the
joint distribution into the product of univariate densities, each
density conditional only on a subset of those prior in the ordering.
If an ordering and sparsity pattern are fixed, then the entries
of the resulting sparse approximate Cholesky factor can be computed
by optimizing a chosen objective, for example, minimizing the
Kullback-Leibler (KL) divergence \cite{schafer2021sparse} is a
natural objective for Gaussian process regression.
Independently of the Vecchia approximation, within the context of factorized
sparse approximate inverse (FSAI) preconditioners, the Kaporin condition number
\cite{kaporin1990alternative} and the Frobenius norm with an additional Jacobi
scaling constraint \cite{yeremin2000factorized} have also been proposed.
These three objectives actually converge to the same closed-form expression
for the entries of the resulting Cholesky factor, equivalent to one
used to compute the Vecchia approximation \cite{schafer2021sparse}.
In addition, the KL divergence is used to compute Knothe-Rosenblatt transport
maps \cite{marzouk2016introduction}, generalizing Cholesky factors to
non-Gaussian distributions while preserving triangularity and sparsity
\cite{spantini2018inference} and often applied to simulation and sampling
problems \cite{marzouk2016introduction, katzfuss2022scalable}.
Finally, exploiting the independence of the Vecchia approximation allows
for embarrassingly parallel algorithms for regression and factorization.

\paragraph{Ordering and sparsity selection by geometry}

\todo{Might want to add a quick comment explaining the maximin ordering here?}
The chosen ordering and sparsity pattern significantly
affect the quality of the resulting factor.
Vecchia originally proposed ordering points lexicographically
\cite{vecchia1988estimation}, but more recent work exploits space-covering
orderings like the maximum-minimum ordering \cite{guinness2018permutation}
which has become popular \cite{schafer2020compression, schafer2021sparse,
katzfuss2021general, kang2021correlationbased, katzfuss2022scalable}.
After choosing an ordering, following Vecchia's recommendation the sparsity
set is often formed by selecting the closest points by Euclidean distance
\cite{vecchia1988estimation, schafer2020compression, schafer2021sparse,
katzfuss2022scalable}.
This choice is often justified by the \emph{screening effect},
or the observation that conditional on points near the point of
interest, far away points are almost conditionally independent
\cite{stein2002screening, stein20112010}.
For example, popular kernel functions like the Mat{\'e}rn family decay
exponentially with increasing distance (see \cref{fig:screening}).
However, selecting by distance ignores the conditional
effect of adding points to the sparsity set.
Imagine that the closest point to the point
of interest is duplicated multiple times.
Conditional on the original point, the
duplicates provide no additional information.
But these redundant points are still just as
close to the target, so they are still added.

\paragraph{Conditional selection}

\todo{We should start with something like ``In this work we propose
a conditional...'' or so. That is, first make it clear that applying
conditional selection is the main contribution of the present work
and then relate it to all the other approaches / techniques.}
Instead of adding points to the sparsity pattern by distance, we propose
greedily selecting points that maximize mutual information with the
point of interest, conditional on all points selected previously.
The machine learning community has long developed similar algorithms that
greedily optimize information-theoretic objectives in the context of
sparse Gaussian process inference \cite{smola2000sparse, herbrich2002fast,
seeger2003fast}.
Similar algorithms have also been developed in the context of
sensor placement \cite{krause2008nearoptimal, clark2018greedy}
and experimental design \cite{mutny2022experimental} where it is
assumed the target phenomena is modeled by a Gaussian process or
is otherwise linearly dependent on the selected measurements.
However, these works often focus on global approximation of the entire process,
e.g. through sparse approximation of the likelihood or covariance matrix
\cite{liu2020when, chalupka2012framework, quinonero-candela2005unifying}.
In contrast \cite{wada2013gaussian} uses inference \emph{directed} towards
a point of interest, selecting the active (sparsity) set by the kernel
function itself like the later work \cite{kang2021correlationbased};
\cite{gramacy2014local} and the follow up work \cite{gramacy2015speeding}
use the more sophisticated active learning Cohn (ALC) objective, yielding
an algorithm equivalent to ours for a single point of interest.
Our proposed algorithm can also be viewed as a variant of orthogonal matching
pursuit (OMP) \cite{tropp2007signal, tropp2006algorithms}, a workhorse
algorithm in compressive sensing which seeks to approximate a target signal
as the sparse linear combination from a given collection of signals.

\paragraph{Main results}

\todo{Want to start a one-sentence description of our contributions, something
like ``We propose...''. We should first fully describe what we are doing
(greedy selection, integration into the KL framework etc.). We want to start
talking about technical things like the computational costs and the improvement
only afterward, once we have established what it is that we are accelerating.}
Our main contribution is a selection algorithm that
greedily maximizes mutual information with point(s) of
interest, conditional on all points selected previously.
We use this algorithm to select the sparsity pattern of sparse approximate
Cholesky factors of precision matrices in the KL-minimization framework of
\cite{schafer2021sparse}, yielding more accurate factors at the same density
compared to nearest neighbors.
The final algorithm extends kernel-based selection \cite{wada2013gaussian,
kang2021correlationbased} to account for conditioning and extends directed
Gaussian process regression \cite{gramacy2014local, gramacy2015speeding} to
multiple points as well as global approximation.
For a single target point, direct computation of the mutual information
criterion would have time complexity \( \BigO(N k^4) \) to select \( k
\) points out of \( N \), but by maintaining a partial Cholesky factor
we reduce the complexity to \( \BigO(N k^2) \).
We extend the algorithm to maximize mutual information with \emph{multiple}
targets, naturally taking advantage of the ``two birds with one stone'' effect.
For \( m \) target points we achieve a time complexity of \(
\BigO(N k^2 + N m^2 + m^3) \), which for \( m \approx k \) is
essentially \( m \) times faster than the single-target algorithm.
In the setting of aggregated (or supernodal) Cholesky factorization where
the sparsity patterns of multiple columns are determined simultaneously,
a candidate entry may only condition a \emph{subset} of the targets.
By efficient rank-one downdating of Cholesky factors, we capture
this structure at the same time complexity for multiple targets.
Finally, we show how to adaptively determine the number of nonzeros
per column in order to minimize the overall KL divergence by
maintaining a global priority queue shared between all columns.

\paragraph{Outline}

This paper is organized as follows.
In \cref{sec:chol}, we show how minimizing KL divergence to compute sparse
Cholesky factors reduces to solving independent regression problems and
extend this result to adjacent and nonadjacent aggregated factorization in
\cref{subsec:kl}.
In \cref{sec:select}, we develop greedy algorithms to select
the sparsity pattern independently for each regression problem.
In \cref{sec:chol_select}, we combine the greedy selection algorithms with
KL minimization to yield algorithms for sparse Cholesky factorization.
In \cref{sec:experiments}, we present numerical experiments applying our method
to image classification, recovery of \textit{a priori} sparse Cholesky factors,
Cholesky factorization, Gaussian process regression, and preconditioning with
the conjugate gradient.
In \cref{sec:conclusion}, we summarize our results.
Proofs and details on implementing our algorithms are
provided in the appendix and supplementary material.

\begin{figure}[t]
  \centering
  \input{figures/screening/uncond.tex}%
  \input{figures/screening/cond.tex}
  \caption{
    An illustration of the screening effect with the Mat{\'e}rn kernel with
    length scale \( \ell = 1 \) and smoothness \( \nu = \frac{5}{2} \).
    The first panel shows the unconditional
    covariance with the point at (0, 0).
    The second panel shows the conditional covariance after
    conditioning on the four points in \textcolor{orange}{orange}.
  }
  \label{fig:screening}
\end{figure}

\Ftodo{``specific'' and ``intuitively'' are fluff, meaning that they
don't really add information. In order to achieve crisp, high-quality
academic writing, it is important to try to those fluff words as much
as possible. It's normal to add them out of reflex initially, so it
requires active postprocessing.}

\section{Sparse Cholesky factorization by KL-minimization}
\label{sec:chol}

Let \( \Theta \in \Reals^{N \times N} \) be a symmetric positive-definite
matrix; we view \( \Theta \) as the covariance matrix of a Gaussian process.
We say a function \( f(\vec{x}) \) is distributed according to a Gaussian
process prior with mean function \( \mean(\vec{x}) \) and covariance
function or kernel function \( \K(\vec{x}, \vec{x}') \), which we will
denote as \( f(\vec{x}) \sim \GP(\mean(\vec{x}), \K(\vec{x}, \vec{x}'))
\), if for any finite set of points \( X = \{ \vec{x}_i \}^N_{i = 1} \),
\( f(X) \sim \N(\vec{\mean}, \CM) \), where \( \mean_i = \mu(\vec{x}_i)
\) and \( \CM_{ij} = \K(\vec{x}_i, \vec{x}_j) \).

In many applications of Gaussian processes, we
wish to infer unknown data given known data.
Given the training dataset \( \mathcal{D} = \{ (\vec{x}_i, y_i) \}^N_{i =
1} \) where the inputs \( \vec{x}_i \in \Reals^D \) are collected in the
matrix \( X_\Train = [\vec{x}_1, \dotsc, \vec{x}_N]^{\top} \in \Reals^{N
\times D} \) and the measurements at those points are collected in the
vector \( \vec{y}_\Train = [y_1, \dotsc, y_N]^{\top} \in \Reals^N \), we
wish to predict the values at \( m \) new points \( X_\Pred \in \Reals^{m
\times D} \) for which \( \vec{y}_\Pred \in \Reals^m \) is unknown.
We assume the function \( f(\vec{x}) \) that maps input points to their
outputs is distributed according to a Gaussian process with zero mean
function, \( f(\vec{x}) \sim \GP(\vec{0}, \K(\vec{x}, \vec{x}')) \).
From the distribution of \( f(\vec{x}) \), the joint distribution
of training and testing data \( \vec{y} \) has covariance
\(
  \CM =
  \begin{pmatrix}
    \CM_{\Train, \Train} & \CM_{\Train, \Pred} \\
    \CM_{\Pred, \Train} & \CM_{\Pred, \Pred}
  \end{pmatrix}
\)
where \( \CM_{\I, \J} \defeq \K(X_\I, X_\J) \) for index sets \( \I, \J \).
In order to make predictions at \( X_\Pred \), we condition the desired
prediction \( \vec{y}_\Pred \) on the known data \( \vec{y}_\Train \).
For Gaussian processes, the closed-form posterior distribution is
\begin{align}
  \label{eq:cond_mean}
  \E[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \vec{\mean}_\Pred +
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    (\vec{y}_\Train - \vec{\mean}_\Train) \\
  \label{eq:cond_cov}
  \Cov[\vec{y}_\Pred \mid \vec{y}_\Train] &=
    \CM_{\Pred, \Pred} -
    \CM_{\Pred, \Train} \CM_{\Train, \Train}^{-1}
    \CM_{\Train, \Pred}
 \shortintertext{
    where \( \CM_{\Train, \Train}^{-1} \defeq (\CM_{\Train, \Train})^{-1} \).
    To denote the covariance between the variables in index
    sets \( \I \) and \( \J \), conditional on the variables
    in the index sets \( \V_1, \V_2, \dotsc, \V_n \) we write
  }
  \label{eq:cond_cov_notation}
  \CM_{I, J \mid \V_1, \V_2, \dotsc, \V_n} &\defeq
    \Cov[\vec{y}_\I, \vec{y}_\J \mid
         \vec{y}_{\V_1 \cup \V_2 \cup \dotsb \cup \V_n}]
  \shortintertext{
    Although the final covariance matrix is invariant to the written order of
    \( \V_1, \dotsc, \V_n \), a different order of conditioning means different
    intermediate results in repeated application of \cref{eq:cond_cov}.
    Letting \( W = \bigcup_{i = 1}^{n - 1} V_i \),
    by the quotient rule of Schur complements:
  }
  \label{eq:quotient_rule}
  \CM_{\I, \J \mid \V_1, \dotsc, \V_n} &=
    \CM_{\I,   \J   \mid W} -
    \CM_{\I,   \V_n \mid W}
    \CM_{\V_n, \V_n \mid W}^{-1}
    \CM_{\V_n, \J   \mid W}
\end{align}
Calculating the posterior mean \cref{eq:cond_mean} and covariance
\cref{eq:cond_cov} requires inverting the training covariance matrix.
Often the Cholesky factorization is used instead of the direct
inverse for improved numerical stability and performance.
However, the time complexity of computing the Cholesky factorization
is \( \BigO(N^3) \), which is prohibitive for large \( N \).
Instead, we will enforce that \( L \) is
\emph{sparse}, yielding an approximate factor.

\subsection{Vecchia approximation}
\label{subsec:vecchia}

Sparse factors naturally arise from the Vecchia approximation
for Gaussian processes \cite{vecchia1988estimation}.
Decomposing the joint likelihood \( \pi \),
\begin{align}
  \label{eq:joint}
  \p(\vec{y}) &= \p(y_1) \p(y_2 \mid y_1) \p(y_3 \mid y_1, y_2) \dotsm
    \p(y_N \mid y_1, y_2, \dotsc, y_{N - 1})
  \shortintertext{
    The key assumption is that many of the points are redundant
    after conditioning on a carefully chosen subset of the points.
    Letting \( i_1, \dotsc, i_N \) denote an ordering of the points and
    \( s_k \) the indices of points that condition the \( k \)th point
    in the ordering, the Vecchia approximation proposes to approximate
    \cref{eq:joint} by the sparse approximation
  }
  \label{eq:vecchia}
  \p(\vec{y}) &\approx \p(y_{i_1}) \p(y_{i_2} \mid y_{s_2})
    \p(y_{i_3} \mid y_{s_3}) \dotsm \p(y_{i_N} \mid y_{s_N})
\end{align}
Following \cref{eq:vecchia} if an elimination ordering \( \prec \) is fixed
(a permutation of \( \{ 1, \dotsc, N \} \)) and a lower triangular sparsity
pattern \( S \defeq \{ (i, j) : i \in s_j, i \succeq j \} \) is specified
then all is needed is a functional criterion \( \Loss: \Reals^{N \times N}
\to \Reals \) to specify the optimization problem
\begin{align}
  \label{eq:generic_obj}
  L &\defeq \argmin_{\hat{L} \in \SpSet} \Loss(\hat{L})
\end{align}
where \( \SpSet \defeq \{ M \in \Reals^{N \times N} :
M_{ij} \neq 0 \Rightarrow (i, j) \in S \} \) is the space
of matrices satisfying the sparsity pattern \( S \).
Proposed functionals which compute inverse Cholesky factors of the
covariance, \( L \chol(\CM) \approx \Id \), include the Kaporin
condition number \( (\trace(L \CM L^{\top})/N)^N/\det(L \CM L^{\top})
\) \cite{kaporin1990alternative} and the Frobenius norm \( \norm{\Id
- L \chol(\CM)}_{\FRO} \) additionally subject to the constraint \(
\diag(L \CM L^{\top}) = 1 \) \cite{yeremin2000factorized}, while the KL
divergence \( \KL{\N(\vec{0}, \CM)} {\N(\vec{0}, (L L^{\top})^{-1})}
\) \cite{schafer2021sparse} computes factors of the precision, \( L
L^{\top} \approx \CM^{-1} \).

As observed in \cite{schafer2021sparse}, factors of the precision are often
much sparser than factors of the covariance, because the precision encodes
conditional independence while the covariance encodes marginal independence.
The same phenomenon is observed by \cite{spantini2018inference}
working with the more general transport maps.
Covariance matrices arising from kernel functions are often fully
dense, but the approximate factors of their precision can be
sparse if the ordering and sparsity pattern are chosen carefully.

\subsection{Ordering and sparsity pattern}
\label{subsec:ordering}

Although in this work we focus on constructing sparsity
patterns, a good ordering is critical since the sparsity for
a point can only include points after it in the ordering.
Vecchia originally proposed ordering points lexicographically, which is
most natural in a one-dimensional setting \cite{vecchia1988estimation}.
More recent work finds that in higher dimensions, exploiting
space-covering orderings leads to significantly better
approximation quality \cite{guinness2018permutation}.
We specifically use the maximum-minimum (maximin) ordering
\cite{guinness2018permutation}, which has become popular for the
Vecchia approximation \cite{katzfuss2021general} and Cholesky
factorization \cite{schafer2020compression, schafer2021sparse,
kang2021correlationbased, katzfuss2022scalable}.
The reverse-maximin ordering \( i_1, \dotsc, i_N \) on a set of \( N \) points
\( \{ \vec{x}_i \}^N_{i = 1} \) is defined by first selecting the last index \(
i_N \) arbitrarily and then choosing for \( k = N - 1, N - 2, \dotsc, 1 \) the
index
\begin{align}
  i_k = \argmax_{i \in -\Order_{k + 1}} \; \min_{j \in \Order_{k + 1}}
    \norm{\vec{x}_i - \vec{x}_j}
\end{align}
where \( -\Order \defeq \{ 1, \dotsc, N \} \setminus \Order \)
and \( \Order_n \defeq \{ i_n, i_{n + 1}, \dotsc, i_N \} \),
i.e. select the point farthest from previously selected points.
The ordering is reversed for factorizing the precision.

Vecchia also originally proposed to select the sparsity set by Euclidean
distance \cite{vecchia1988estimation}, which, unlike the lexicographic
ordering, still remains widely used \cite{schafer2020compression,
schafer2021sparse, katzfuss2022scalable}.
Instead, we show how to select the sparsity pattern to directly
optimize the objective \( \Loss \) \cref{eq:generic_obj} in an
end-to-end manner by decomposing the KL divergence along each column.

\subsection{Review of KL-minimization}
\label{subsec:kl}

The Kullback-Leibler (KL) divergence between two probability distributions \( P
\) and \( Q \) is defined as \( \KL*{P}{Q} \defeq \E_P[\log(\frac{P}{Q})] \).
As the expected difference between true and approximate log-densities, the
KL divergence naturally judges the quality of an approximating distribution.
We identify the positive-definite matrix \( \CM \in \Reals^{N \times N} \)
as the covariance matrix of a centered Gaussian process \( \N(\vec{0}, \CM)
\) which we seek to approximate by a sparse approximate Cholesky factor \(
L \in \SpSet \) of its precision, \( \N(\vec{0}, (L L^{\top})^{-1}) \).
We compare these distributions by the KL divergence as \cite{schafer2021sparse}
does, specializing the generic optimization problem \cref{eq:generic_obj} to
\begin{align}
  \label{eq:L_obj}
  L \defeq \argmin_{\hat{L} \in S} \,
    \KL*{\N(\vec{0}, \CM)}
        {\N(\vec{0}, (\hat{L} \hat{L}^{\top})^{-1})}
\end{align}
For multivariate Gaussians, the KL divergence
has a closed-form expression given by
\begin{align}
  \label{eq:kl}
  2 \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \trace(\CM_2^{-1} \CM_1) + \logdet(\CM_2) - \logdet(\CM_1) - N
\end{align}
where \( \CM_1, \CM_2 \in \Reals^{N \times N} \).
Using this expression for the KL divergence and optimizing for \( L \) yields
the following closed-form expression for the nonzero entries in the \( i \)th
column of \( L \) with sparsity pattern \( s_i \), reproduced from Theorem 2.1
of \cite{schafer2021sparse}:
\begin{align}
  \label{eq:L_col}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
\end{align}
where the notation for \( \CM_{s_i, s_i}^{-1} \) is from
\cref{eq:cond_cov_notation} and \( \vec{e}_1 \in \Reals^{\card{s_i}
\times 1} \) denotes the vector with first entry one and the rest zero.
We enforce the convention that \( i \) is the first entry
of \( s_i \), also implying that \( L \) is of full rank.
Plugging the optimal \( L \) \cref{eq:L_col} back into the KL divergence
\cref{eq:kl}, we obtain the objective as a function of the sparsity pattern.
\begin{align}
  \label{eq:obj_chol}
  2 \KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} &=
    \sum_{i = 1}^N
      \left [
        \log \left (
          (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
        \right )
      \right ]
      - \logdet(\CM)
\end{align}
See \cref{app:kl_L} for details. In particular, it is important
which direction the KL divergence is taken or cancellation
of the \( \trace(\CM_2^{-1} \CM_1) \) term may not occur.

In order to pick a sparsity pattern that minimizes the KL divergence
\cref{eq:obj_chol}, we can ignore the constant \( \logdet(\CM) \) and minimize
over each column independently, as each term \( (\vec{e}_1^{\top} \CM_{s_i,
s_i}^{-1} \vec{e}_1)^{-1} \) in the sum depends only on \( s_i \).
To do so, we recall that conditioning in
covariance is marginalization in precision.
For
\( \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\),
\begin{align}
  \label{eq:inverse_cond}
  \CM_{1, 1 \mid 2} &= \left ( \CM^{-1} \right )_{1, 1}^{-1} \\
  \shortintertext{
    Viewing \( (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1} =
    \left (\CM_{s_i, s_i}^{-1} \right )_{1, 1}^{-1} \) as a marginalization
    of \( \CM_{s_i, s_i}^{-1} \) to apply \cref{eq:inverse_cond},
  }
  \label{eq:L_cond_var}
  (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
         &= \CM_{i, i \mid s_i \setminus \{ i \}}
\end{align}
So minimizing the KL divergence of a sparse Cholesky factor is equivalent
to independently minimizing over each column the variance of the \( i \)th
variable, conditional on the selected entries \( s_i \) (excluding itself).
Decomposition into independent regression problems was also observed
in \cite{katzfuss2022scalable} for lower triangular transport maps.

\subsubsection{Aggregated sparsity pattern}
\label{subsubsec:aggregated}

We now derive a similar decomposition of the KL divergence
if the same sparsity pattern is reused for multiple columns,
known as aggregated or supernodal Cholesky factorization.
Aggregation can lead to substantial time
and space savings \cite{schafer2021sparse}.
For this section we focus on a single group \( \tilde{i}
= \{i_1, \dotsc, i_m \} \) composed from aggregating the
column indices \( i_1 \succ i_2 \succ \dotsb \succ i_m \).
Let \( \tilde{i} \) have aggregated selected entries \(
s_{\tilde{i}} \) satisfying \( s_{\tilde{i}} \supseteq \tilde{i}
\) to guarantee that the Cholesky factor has full rank.
Let \( \tilde{s} \defeq s_{\tilde{i}} \setminus \tilde{i} \)
be the selected entries excluding the columns in the group.
The sparsity pattern for the \( k \)th column in the group is then the
aggregated selected entries excluding the entries that violate lower
triangularity, \( s_k \defeq \{ j \in s_{\tilde{i}} : j \succeq k \} \).
Assuming every entry of \( \tilde{s} \) is after every index in \( \tilde{i}
\), then \( s_k = \tilde{s} \cup \{ j \in \tilde{i} : j \succeq k \} \).
This condition is guaranteed if the aggregated columns
are adjacent in the ordering, for example; we defer
handling the general case to \cref{subsubsec:partial}.
We now simplify the KL divergence on aggregated indices
with the conditional chain rule of log determinants.
Using the same blocking as \cref{eq:inverse_cond},
\begin{align}
  \label{eq:det_chain}
  \logdet(\CM) &= \logdet(\CM_{1, 1}) + \logdet(\CM_{2, 2 \mid 1})
\end{align}
The KL divergence objective \cref{eq:obj_chol} restricted
to the contribution from the group \( \tilde{i} \) is
\begin{align}
  \sum_{i \in \tilde{i}} \log(\CM_{i \mid s_i \setminus \{ i \} }) &=
    \log(\CM_{i_1 \mid \tilde{s}}) +
    \log(\CM_{i_2 \mid \tilde{s} \cup \{ i_1 \}}) + \dotsb +
    \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{
    where we write \( \CM_j \defeq \CM_{j, j} \).
    Combining the first two terms by the chain rule \cref{eq:det_chain},
  }
  &= \logdet(\CM_{\{ i_1, i_2 \} \mid \tilde{s}}) +
     \log(\CM_{i_3 \mid \tilde{s} \cup \{ i_1, i_2 \}}) + \dotsb +
     \log(\CM_{i_m \mid \tilde{s} \cup \tilde{i}}) \\
  \shortintertext{
    Proceeding by induction, we are able to
    reduce the entire sum to the single term
  }
  \label{eq:obj_mult}
  &= \logdet(\CM_{\tilde{i}, \tilde{i} \mid \tilde{s}})
\end{align}
So the suitable generalization of the conditional variance
in \cref{eq:obj_chol} to aggregated columns is the log
determinant of the covariance matrix of the columns,
conditional on (well-behaved) selected entries.
We briefly discuss what happens in the general case.

\subsubsection{Nonadjacent or partial aggregation}
\label{subsubsec:partial}

\begin{figure}[t]
  \centering
  \input{figures/partial_factor.tex}
  \caption{
    Illustration of the Cholesky factorization of
    a partially conditioned covariance matrix.
    Here \textcolor{darksilver}{grey} denotes fully unconditional,
    \textcolor{darklightblue}{blue} denotes fully conditional, and the
    \textcolor{silver!50!lightblue}{mixed color} denotes interaction
    between the two.
    Surprisingly, such a matrix factors into a ``pure'' Cholesky
    factor by ``gluing'' the prefix of the fully unconditional
    factor with the suffix of the fully conditional factor.
  }
  \label{fig:partial_factor}
\end{figure}

Let the random variables corresponding to the indices \( i_1,
\dotsc, i_m \) be collected in a vector \( \vec{y} = [y_1, \dotsc,
y_m]^{\top} \) with joint density \( \vec{y} \sim \N(\vec{0}, \CM) \).
We select an index \( k \) that conditions all but the first \( p \)
variables (recall that the indices are sorted in decreasing order, so
if \( k \) conditions a variable, it conditions all those afterwards).
After this partial conditioning the variables become \( \vec{y}_{\parallel
k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k}, \dotsc, y_{m \mid k} \)
and the covariance matrix \( \Cov[\vec{y}_{\parallel k}] \) becomes
\begin{align}
  \label{eq:chol_partial}
  \CM_{\tilde{i}, \tilde{i} \parallel k} &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}
where \( L = \chol(\CM) \) and \( L' = \chol(\CM_{:,
: \mid k}) \) (see \cref{fig:partial_factor} for an
illustration and \cref{app:partial} for details).
Armed with this representation, we equate the log
determinant of \( \CM_{\tilde{i}, \tilde{i} \parallel
k} \) to the KL divergence in \cref{eq:obj_chol}.
Recalling that the determinant of a triangular
matrix is the product of its diagonal entries,
\begin{align}
  \nonumber
  \frac{1}{2} \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
  \underbrace{
    \log(L_{1, 1}) + \dotsb + \log(L_{p, p})
  }_{\text{the same}} +
  \underbrace{
    \log({L'}_{p + 1, p + 1}) + \dotsb + \log({L'}_{m, m})
  }_{\text{conditioned}}
\end{align}
Comparing to the KL divergence \cref{eq:obj_chol} and
recalling that \( k \) is added to \( s_i \) if \( i > p \),
\begin{align}
  \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
  &= \underbrace{
      \log \left ( \CM_{1, 1 \mid s_1 \setminus \{ 1 \}} \right ) + \dotsb +
      \log \left ( \CM_{p, p \mid s_p \setminus \{ p \}} \right )
     }_\text{the same} + \\
  \nonumber
  % disgusting hack, do properly later --- looks good enough to me, though
  & \hphantom{=} \: \,
  \underbrace{
    \log \left (
      \CM_{p + 1, p + 1 \mid s_{p + 1} \setminus \{ p + 1 \}}
    \right ) + \dotsb +
    \log \left ( \CM_{m, m \mid s_m \setminus \{ m \}} \right )
  }_\text{conditioned}
  \shortintertext{
    Since \( L_{i, i} \) (and \( {L'}_{i, i} \)) is the square root
    of the conditional variance of the \( i \)th variable from the
    statistical perspective in \cref{eq:chol}, we have \( 2 \log(L_{i,
    i}) = \log(\CM_{i, i \mid s_i \setminus \{ i \}}) \) and so
  }
  \label{eq:partial_kl}
  \logdet(\CM_{\tilde{i}, \tilde{i} \parallel k}) &=
    \sum_{i = 1}^m \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
\end{align}
Like the aggregated case in \cref{subsubsec:aggregated},
minimizing the log determinant of the \emph{partially} conditioned
covariance matrix is the same as minimizing the KL divergence.

In every case we have considered, picking the right sparsity pattern
to minimize KL divergence reduces to selecting the points \( s_i \)
out of the possible candidates \( i + 1, \dotsc, N \) that most reduce
predictive error at point(s) of interest.
In the next section, we develop such a selection
algorithm for directed inference in Gaussian processes.
We apply this algorithm for sparsity selection
of Cholesky factors in \cref{sec:chol_select}.

\section{Greedy selection for directed inference}
\label{sec:select}

\begin{figure}[t]
  \centering
  \input{figures/selection/knn.tex}%
  \input{figures/selection/cknn.tex}
  \caption{
    Here, the \textcolor{lightblue}{blue} points are the
    \textcolor{lightblue}{candidates}, the \textcolor{orange}{orange}
    point is the \textcolor{orange}{target} point to predict
    at, and the \textcolor{seagreen}{green} points are the
    \textcolor{seagreen}{selected} points.
    The \textcolor{rust}{red} line is the \textcolor{rust}{conditional mean}
    \( \mu \), conditional on the selected points, and the \( \pm 2 \sigma \)
    confidence interval is shaded for the conditional variance \( \var \).
    Each method has a budget of two points; the left panel shows selection
    by Euclidean distance and the right by conditional variance.
    Euclidean distance prefers the two points right of the target.
    However, a more balanced view of the situation is obtained when picking
    the slightly further but ultimately more informative point to the left,
    reducing variance at the target and thereby reducing predictive error.
  }
  \label{fig:selection}
\end{figure}

In directed Gaussian process regression we are given \( N \) points of
training data and predict at a target point with unknown value by selecting
the \( s \) points most ``informative'' to the target, \(s \ll N \).
From KL-minimization, the criterion for informativity
should be to minimize the variance of the target point
conditional on the selected points \cref{eq:L_cond_var}.
The variance objective was first described by \cite{cohn1996neural} for optimal
experimental design and later applied to directed Gaussian process inference
by \cite{gramacy2014local} who refer to it as the active learning Cohn (ALC)
objective in honor of \cite{cohn1996neural}.
In addition, the variance objective is equivalent to maximizing
the \emph{mutual information} or \emph{information gain} with the
target point as well as minimizing the expected mean squared error
(see \cref{app:mutual_info}).
The mutual information (in a slightly different context) is
also used by \cite{krause2008nearoptimal} for sensor placement.

In contrast to Euclidean distance \cite{vecchia1988estimation} or unconditional
correlation \cite{wada2013gaussian, kang2021correlationbased}, conditional
variance incentives the often contradictory demands of being near the target
point (nearby points have higher covariance), but away from previously
selected points (to avoid redundancy); the resulting spread-out selections
are illustrated in \cref{fig:selection}.

\subsection{A greedy approach}
\label{subsec:greedy_select}

Minimizing the conditional variance over all possible \(
\binom{N}{s} \) subsets is intractable, so we greedily select
the next point which most reduces the conditional variance.
Let \( \I = \{ i_j \}^t_{j = 1} \subseteq \Train \) be
the indices of previously selected training points.
For a newly selected index \( k \), we condition the current
covariance matrix on \( y_k \) according to the posterior
\cref{eq:quotient_rule}, which we see is a rank-one downdate.
\begin{align}
  \label{eq:cond_select}
  \CM_{:, : \mid \I, k} &= \CM_{:, : \mid \I} - \vec{u} \vec{u}^{\top} &
    \vec{u} &= \frac{\CM_{:, k \mid \I}}{\sqrt{\CM_{k, k \mid \I}}}
\end{align}
The decrease in the variance of \( y_\Pred \) after
selecting \( k \) is given by \( u_\Pred^2 \), or
\begin{align}
  \label{eq:obj_gp}
  u_\Pred^2 = \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
  = \frac{\Cov[y_\Pred, y_k \mid \I]^2}{\Var[y_k \mid \I]}
  = \Var[y_\Pred \mid \I] \Corr[y_\Pred, y_k \mid \I]^2
\end{align}
To compute the objective \cref{eq:obj_gp} for each candidate index \( j \),
we start with the unconditional variance \( \CM_{j, j} \) and covariance \(
\CM_{\Pred, j} \), updating these quantities if an index \( k \) is selected.
If we can compute \( \vec{u} \) for \( k \), then we can update
\( j \)'s conditional variance by subtracting \( u_j^2 \) and
update its conditional covariance by subtracting \( u_j u_\Pred \).

We have two efficient strategies to compute \( \vec{u} \).
The direct method is update \( \CM_{\I, \I}^{-1} \) (the precision
of the selected entries) whenever a new index is added to \( \I \)
in time complexity \( \BigO(s^2) \) (see \cref{app:prec_insert}).
With \( \CM_{\I, \I}^{-1} \) in hand \( \vec{u} \) is
computed directly according to \cref{eq:cond_cov}.
For each of the \( s \) rounds of selection it takes \( \BigO(s^2)
\) to update the precision and \( \BigO(Ns) \) to compute \(
\vec{u} \), for an overall time complexity of \( \BigO(N s^2) \).

The second strategy exploits the iterative
conditioning in Cholesky factorization.
Re-writing the joint covariance matrix by
two steps of block Gaussisan elimination,
\begin{align}
  \nonumber
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    }
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{
    so we see that the Cholesky factorization
    of the joint covariance \( \CM \) is
  }
  \label{eq:chol}
  \chol(\CM) &=
  \begin{pmatrix}
    \textcolor{darkorange}{\chol(\CM_{1, 1})} & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \chol(\CM_{1, 1})^{-\top}} &
    \chol(\textcolor{lightblue}{
      \CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}
    })
  \end{pmatrix}
\end{align}
where the conditional expectation \cref{eq:cond_mean} corresponds
to \( \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} \) and
the conditional covariance \cref{eq:cond_cov} corresponds to
\(
  \textcolor{lightblue}{\CM_{2, 2} - \CM_{2, 1} \CM_{1, 1}^{-1} \CM_{1, 2}}
\).
Blocking \( \CM \) such that 1 is the current column and 2 is
all the indices past 1, we observe the column of the Cholesky
factor is a conditional covariance \( \CM_{2, 1} \) divided by
\( \chol(\CM_{1, 1})^{\top} \), the square root of the variance
(the Cholesky factor of a scalar is its square root).
We conclude that the \( k \)th column of the Cholesky factor equals \( \vec{u}
\) in \cref{eq:cond_select} since iteratively conditioning on the columns \(
i_1, i_2, \dotsc, i_{k - 1} \) is equivalent to conditioning on \( \I \) by the
quotient rule \cref{eq:quotient_rule}.

We maintain a partial Cholesky factor whose rows are the training points
and target point, and whose columns are the currently selected points.
Adding a column to the Cholesky factor can be efficiently
computed with left-looking (see \cref{alg:chol_update}).
For each of the \( s \) rounds of selection it costs \( \BigO(N s)
\) to compute the next column of the Cholesky factor, the same \(
\BigO(N s^2) \) time complexity as the explicit precision approach.

However, the precision \( \Theta_{\I, \I}^{-1} \) takes \( \BigO(s^2)
\) space while the first \( s \) columns of the Cholesky factor of \(
\CM \) uses \( \BigO(N s) \) space, always more memory (\( N > s \)).
Both algorithms use at least \( \BigO(N) \) space
to store the conditional variances and covariances.
Although the precision algorithm uses less memory than the
Cholesky algorithm, the Cholesky algorithm is preferred
for ease of implementation and better performance.

\subsection{Supernodes and blocked selection}
\label{subsec:mult_select}

For multiple (\( m > 1 \)) target points, \cite{gramacy2014local} suggests
simply independently applying the single-target algorithm to each target.
Instead, we will use \emph{the same} selected points for \emph{all} the target
points, essentially speeding up selection by a factor of \( m \), furthermore,
the cost of computing the entries of the resulting Cholesky factor is reduced
by this aggregation.
The primary downside is reduced accuracy per sparsity entry
since each target no longer receives individual attention.
We mitigate this by paying heed to the ``two birds with
one stone'' maxim, or by considering a candidate's
simultaneous effect on \emph{all} prediction points.
In practice, this approach yields better accuracy
per unit time than single-target selection.

The first question is generalizing the objective for
a single target \cref{eq:obj_gp} to multiple targets.
Continuing with KL-minimization, the criterion should be to minimize \(
\logdet(\CM_{\Pred, \Pred \mid I}) \), the log determinant of the prediction
covariance matrix conditional on the selected points \cref{eq:obj_mult}.
This objective, known as D-optimal design in the literature
\cite{krause2008nearoptimal}, can be intuitively interpreted as
a volume of the region of uncertainty or as a scaling factor
in the density function of multivariate Gaussians.
In addition, it is equivalent to maximizing mutual information
since the differential entropy of a Gaussian is monotonically
increasing with its log determinant (see \cref{app:mutual_info}).

We want to quickly compute how selecting an
index \( k \) affects the log determinant.
From \cref{eq:cond_select}, selecting an index
is a rank-one downdate on the covariance matrix:
\begin{align}
  \label{eq:obj_gp_mult}
  \logdet(\CM_{\Pred, \Pred \mid \I, k}) &=
    \logdet(\CM_{\Pred, \Pred \mid \I} -
       \CM_{\Pred, k \mid \I} \CM_{k, k \mid \I}^{-1}
       \CM_{\Pred, k \mid \I}^{\top}) \\
  \shortintertext{
    By application of the matrix determinant lemma
    (the details are in \cref{app:logdet_downdate}),
  }
  \label{eq:greedy_mult}
  \logdet(\CM_{\Pred, \Pred \mid \I, k}) - \logdet(\CM_{\Pred, \Pred \mid \I})
  &= \log(\CM_{k, k \mid \I, \Pred}) - \log(\CM_{k, k \mid \I})
\end{align}
Equation \cref{eq:greedy_mult} swaps the roles of the targets and the
candidate; instead of the targets being conditioned by the candidate, the
candidate is conditioned by the targets, reducing to single-target selection.
Using the recipes from the previous section to compute conditional variances,
we can compute the objective by maintaining a data structure for each term: one
for \( \CM_{k, k \mid \I, \Pred} \) and the other
for \( \CM_{k, k \mid \I} \).
By the quotient rule \( \CM_{k, k \mid \I, \Pred} = \CM_{k,
k \mid \Pred, \I} \) so we condition on the prediction
points \emph{before} any points have been selected.
After this initialization we select the best candidate by
the objective \cref{eq:greedy_mult} and simultaneously
update both data structures after selecting each index.
We again have two strategies from the two
approaches of the single-target algorithm.

The precision approach initializes the precision \( \CM_{\Pred, \Pred}^{-1} \)
for \( m \) prediction points in time \( \BigO(m^3) \) and computes the initial
conditional variances \( \CM_{k, k \mid \Pred} \) for \( N \) candidates by
direct application of the posterior \cref{eq:cond_cov} in time \( \BigO(N m^2)
\).
For each of the \( s \) rounds of selecting candidates, it costs \(
\BigO(s^2) \) and \( \BigO(m^2) \) to update the precisions \( \CM_{\I,
\I}^{-1} \) and \( \CM_{\Pred, \Pred}^{-1} \) respectively, where the details
of efficiently updating \( \CM_{\Pred, \Pred}^{-1} \) after the rank-one
update in \cref{eq:obj_gp_mult} are given in \cref{app:prec_cond}.
Given the precisions, \( \vec{u} \) \cref{eq:cond_select} and \( \vec{u}_\Pred
\defeq \frac{\CM_{:, k \mid \I, \Pred}}{\sqrt{\CM_{k, k \mid \I, \Pred}}} \)
are computed as usual according to \cref{eq:cond_cov} in time \( \BigO(N s) \)
and \( \BigO(N m) \).
Finally, for each candidate \( j \) the conditional variance \( \CM_{j, j \mid
\I} \) is updated by subtracting \( u_j^2 \), the conditional covariance \(
\CM_{\Pred, k \mid \I} \) is updated for each prediction point index \( c \)
by subtracting \( u_j u_c \), and the conditional variance \( \CM_{j, j \mid
I, \Pred} \) is updated by subtracting \( (u_\Pred)_j^2 \).
The total time complexity after simplification
is \( \BigO(N s^2 + N m^2 + m^3) \).

The approach storing two partial Cholesky factors is
considerably simpler from the symmetry of the factors and
the lack of explicitly computing conditional covariances.
We first add each prediction point to one Cholesky factor,
for a total time complexity of \( \BigO((N + m) m^2) \).
After a candidate is selected, both Cholesky factors are updated in time \(
\BigO((N + m)(m + s)) \) dominated by updating the initialized Cholesky factor.
The desired covariances \( \vec{u} \) and \( \vec{u}_\Pred \) are directly
read off the columns of the Cholesky factors and both conditional variances \(
\CM_{j, j \mid I} \) and \( \CM_{j, j \mid I, \Pred} \) are computed as above.
Over \( s \) rounds the total time complexity after
simplification is \( \BigO(N s^2 + N m^2 + m^3) \).

Like the single-target case, both approaches have the
same time complexity but differ in space complexity.
The precision algorithm requires \( \BigO(s^2 + m^2) \)
memory to store both precisions as well as \( \BigO(N
m) \) memory to store the conditional covariances.
The Cholesky algorithm requires \( \BigO(N s + N m + m^2) \) memory
to store a partial Cholesky factor for the joint covariance matrix.
The memory usages are identical except for \( \BigO(N s) \) versus \(
\BigO(s^2) \), so the Cholesky algorithm again uses more memory than the
precision algorithm but is preferred for simplicity and performance.

\subsection{Partial selection}
\label{subsec:partial_select}

We now generalize the multiple-target algorithm to track the
log determinant of the target points' covariance matrix if a
selected index \( k \) only conditions \emph{suffixes} of the
target points, skipping the first \( p \) targets, say.
Following the suggestive form of the target covariance matrix
\cref{eq:chol_partial}, we maintain a partial Cholesky factor \( L
\) of the joint covariance matrix of target and training points,
whose columns consist of the target points and previously selected
points sorted in reverse order with respect to \( \prec \).
Inserting the index \( k \) into its proper column in \( L \),
\begin{align}
  \label{eq:chol_partial_update}
  L &\gets
  \begin{pmatrix}
    L_{:, :p} & \vec{u} & {L'}_{:, p + 1:}
  \end{pmatrix} &
    \vec{u} &= \frac{\CM_{:, k \mid :p}}{\sqrt{\CM_{k, k \mid :p}}}
\end{align}
where \( L' \) is the Cholesky factor of the covariance matrix
conditional on \( k \) and the updated form of \( L \) after insertion
is derived from the statistical perspective in \cref{eq:chol}.

To account for this insertion, the first \( p \) columns \( L_{:, :p}
\) are unchanged, the newly inserted column \( \vec{u} \) of the form
\cref{eq:cond_select} can be computed as usual with left-looking (see
\cref{alg:chol_update}), and the remaining columns \( {L'}_{:, p + 1:} \) are
the Cholesky factor of the covariance matrix conditional on the point \( k \).
From \cref{eq:cond_select} conditioning on an additional point is a rank-one
downdate of the covariance matrix by the vector \( \vec{u} \), allowing the
updated factor \( {L'}_{:, p + 1:} \) to be efficiently computed from the
original factor \( L_{:, p + 1:} \).
Specifically we adapt Lemma 1 of \cite{krause2015more} to make
no assumption on the row ordering of \( L \), allowing the
downdate to be implemented in-place (see \cref{alg:chol_insert}).
For a Cholesky factor with \( R \) rows and \( C \) columns, the
time complexity of the downdate is \( \BigO(R C) \) compared to
\( \BigO(R C^2) \) if the factor was recomputed from scratch.

For \( N \) training points, \( m \) target points,
and a budget of \( s \) selected points, the Cholesky
factor \( L \) has size \( (N + m) \times (s + m) \).
When adding a new index \( k \) that ignores the first \( p \) targets, the
first \( j \) columns of \( L \) are ignored, the new column \( \vec{u} \)
must be computed with left-looking in time \( \BigO((N + m) p) \), and the
columns past \( p \) are computed with downdating in time \( \BigO((N + m)
(s + m - p)) \).
The cost for a single insertion is \( \BigO((N + m)(s + m)) \) for a total time
complexity of \( \BigO(s(N + m)(s + m)) \) over \( s \) rounds of selection.
With the factor \( L \) in hand, we discuss
actually computing the log determinant objective
\cref{eq:partial_kl} after adding a candidate index \( j \).
We directly compute each individual target point's variance,
conditional on \( j \) as well as previously selected points, and
simply add these variances together to get the overall objective.

We proceed by induction on the columns of \( L \), assuming we know
the candidate's variance conditional on all points prior; starting
at the first column we know the candidate's unconditional variance.
We proceed to the next column by reading the requisite
statistical quantities from \( L \) \cref{eq:chol_partial_update}
and conditioning \cref{eq:quotient_rule}.
For the \( i \)th column,
\begin{align}
  \label{eq:partial_diag}
  \CM_{i, i \mid :i - 1} &= L_{i, i}^2 \\
  \CM_{j, i \mid :i - 1} &= L_{j, i} \cdot L_{i, i} \\
  \label{eq:obj_partial}
  \CM_{i, i \mid :i - 1, j} &= \CM_{i, i \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{j, j \mid :i - 1} \\
  \label{eq:partial_induct}
  \CM_{j, j \mid :i - 1, i} &= \CM_{j, j \mid :i - 1} -
    \CM_{j, i \mid :i - 1}^2/\CM_{i, i \mid :i - 1} = \CM_{j, j \mid :i}
\end{align}
where \cref{eq:partial_induct} satisfies the conditions of the inductive
hypothesis for the next column and \cref{eq:obj_partial} is the desired
conditional variance when the \( i \)th point is a target (recall the
columns of \( L \) include previously selected training points, whose
variances are ignored in the objective but whose columns still need to
be processed to compute \cref{eq:partial_induct}).

For each of the \( N \) candidates, it requires \( \BigO(1) \) work per column
for \( s + m \) columns. Over \( s \) selections, the total time complexity is
\( \BigO(s N (s + m)) \) which is dominated by the time to update the Cholesky
factor, meaning the partial selection algorithm matches the asymptotic time
complexity of the multiple-target algorithm.
However, the downdate of the Cholesky factor is implemented with
repeated BLAS level-one \texttt{daxpy} operations while the bulk of
left-looking takes place in a BLAS level-two \texttt{dgemv} operation.
Higher level operations often have better constant-factor
performance for the same asymptotic time complexity.
In addition, the objective for the partial selection algorithm is
more complicated to compute than its multiple-target counterpart.

\section{Greedy selection for global approximation by KL-minimization}
\label{sec:chol_select}

\begin{figure}[t]
  \centering
  \input{figures/cholesky_factor.tex}%
  \qquad
  \input{figures/points/selected_points.tex}
  \caption{
    For a column of a Cholesky factor in isolation, the
    \textcolor{orange}{target} point is the \textcolor{orange}{diagonal}
    entry, \textcolor{lightblue}{candidates} are \textcolor{lightblue}{below}
    it, and the \textcolor{seagreen}{selected} entries are added to
    the \textcolor{seagreen}{sparsity pattern}.
    Points violating lower triangularity are
    shown in \textcolor{darksilver}{grey}.
    Thus, sparsity selection in Cholesky factorization
    (left panel) is analogous to point selection in
    directed Gaussian process regression (right panel).
  }
  \label{fig:select_chol}
\end{figure}

Directed Gaussian process regression infers the
\emph{local} distribution at points of interest.
We turn our attention to \emph{global} approximation of the entire Gaussian
process; given a kernel function \( \K(\vec{x}, \vec{x}') \) and a set of \(
N \) points \( \{ \vec{x}_i \}^N_{i = 1} \) we have the covariance matrix \(
\CM_{ij} = K(\vec{x}_i, \vec{x}_j) \) for which we seek a sparse approximate
Cholesky factor \( L \) of the precision, \( L L^{\top} \approx \CM^{-1} \).
We first order the points by the reverse-maximin ordering described in
\cref{subsec:ordering} and then straightforwardly apply our suite of selection
algorithms developed in \cref{sec:select} to form the sparsity pattern.
For the \( i \)th column of \( L \), the target point is the \( i \)th point
in the ordering, the candidate points are those satisfying lower triangularity
(after the target in the ordering), and running the selection algorithm for
the desired number of nonzeros entries picks out indices which we add to the
sparsity set \( s_i \); this process is illustrated in \cref{fig:select_chol}.
Finally, we compute the values of the selected nonzero
indices by the closed-form expression \cref{eq:L_col}.
Both sparsity selection and computing entries are
embarrassingly parallel over \( L \)'s columns.

Using the single-target selection algorithm from \cref{subsec:greedy_select}
has time complexity \( \BigO(C s^2) \) to select \( s \) nonzero entries out of
\( C \) candidates, for a total of \( \BigO(N C s^2) \) over \( N \) columns.
Computing the corresponding values \( \CM_{s_i, s_i}^{-1} \vec{e}_1
\) has cost \( \BigO(s^3) \) for a total of \( \BigO(N s^3) \).
Sparsity selection has the same complexity as entry computation
if the number of candidates \( C \) is \( \BigO(s) \), suggesting
the need to limit the number of candidates considered.
In practice, we pick the candidate set to be the nearest neighbors
of the point of interest as \cite{gramacy2014local} does.
We specifically use the framework of \cite{schafer2021sparse} which considers
all points within a radius systematically decreasing earlier in the ordering.

\subsection{Aggregated sparsity pattern}

In an aggregated sparsity pattern, columns are partitioned into groups and
selecting an index for a group \( \tilde{i} \) adds it to the sparsity
pattern of every column in \( \tilde{i} \) satisfying lower triangularity.
We group columns by the framework of \cite{schafer2021sparse} which aggregates
points that are close both geometrically as well as in the ordering.
To select sparsity entries, the targets are all points in \( \tilde{i} \)
and the candidates are the union of the nearest neighbors to each target.
If every candidate \( k \) satisfies \( k \succ \max{\tilde{i}} \) which
occurs if the group is contiguous in the ordering e.g., then every candidate
conditions every target and so the multiple-target selection algorithm
(\cref{subsec:mult_select}) can be directly applied.
However, we empirically observe that forcing this condition irreparably
damages the accuracy of the resulting factor: forming groups contiguous in
the ordering no longer guarantees that grouped points are spatially close,
and removing candidates between targets filters many of them out.
If the condition is not forced, then selecting candidates can
condition subsets of the group; the multiple-target algorithm
now systematically overestimates the effect on targets.
Using the partial selection algorithm (\cref{subsec:partial_select})
instead on unmodified grouping and candidate sets
significantly improves the approximation quality.

We still compute the entries of \( L \) by equation \cref{eq:L_col}.
Because the sparsity patterns for columns in the same group are subsets of
each other, we can efficiently compute the group's entries together in the
time complexity for a single column, \( \BigO(s^3) \) (see \cref{app:L_mult}
or Algorithm 3.2 in \cite{schafer2021sparse}).
If each group has \( m \) points, both the multiple-target and
partial selection algorithms have time complexity \( \BigO(C s^2 +
C m^2 + m^3) \) to select \( s \) points out of \( C \) candidates.
Over \( N/m \) groups the time complexity for both selection and entry
computation is \( \BigO(\frac{N}{m} (C s^2 + C m^2 + m^3 + s^3)) \),
simplifying to \( \BigO(\frac{N C s^2}{m}) \) assuming \( m = \BigO(s) \),
a \( m \) times improvement over non-aggregated factorization, yielding
denser and hopefully more accurate factors in the same amount of time.
Inheriting the primary downside of the multiple-target selection algorithm,
the aggregated factor is less efficient at reducing the KL divergence per
nonzero since the sparsity pattern is shared between all columns in the
group, not tailored to any particular column.

\subsection{Allocating nonzeros by global greedy selection}
\label{subsec:global_greedy}

With a budget on the total number of nonzeros, one
inevitably decides how many nonzeros each column gets.
\cite{schafer2021sparse} implicitly determines both the total
number of nonzeros as well as their distribution over columns by
selecting points within a radius decreasing earlier in the ordering.
We observe in our numerical experiments as \cite{kang2021correlationbased}
does that this approach is less accurate than simply
allocating the same number of nonzeros per column.
Distributing nonzeros as evenly as possible also maximizes computational
efficiency since denser columns have an outsized impact on the
computational time from the cubic scaling cost with the number of nonzeros.
For these reasons we recommend allocating to each column
the budget of nonzeros divided by the number of columns.

A principled way of distributing nonzeros might be to minimize
KL divergence end-to-end like was done for sparsity selection.
The \emph{local} greedy algorithms select the sparsity entry that
minimizes prediction error at \emph{particular} columns of interest.
In \emph{global} greedy selection, we pick from \emph{any} column the
candidate that minimizes the overall KL divergence \cref{eq:obj_chol}.
We maintain a priority queue containing all candidates from every
column, keyed by the candidate's effect on the KL divergence.
The data structure must support popping the largest element off the
queue as well as updating the value for an element in the queue.
Both operations have time complexity \( \BigO(\log n) \) for \( n \) elements
if efficiently implemented as an array-backed binary heap, for example.

The greedy selection algorithms already compute the effect of
an entry on the KL divergence, up to monotonically increasing
transformation (which preserve the ranking of candidates).
But in the global context, if different columns use different
transformations, then the ranking of candidates between columns is skewed.
We describe the necessary modifications to
compute exactly the difference in KL divergence.

\subsubsection{Single column selection}
\label{subsubsec:single_column}

Selecting an entry \( k \) for a single target only affects its conditional
variance, so exactly one term in the KL divergence \cref{eq:obj_chol} changes:
\begin{align}
  \argmin_k \left [
    \log(\CM_{\Pred, \Pred \mid \I, k}) - \log(\CM_{\Pred, \Pred \mid \I})
  \right ] &=
    \argmin_k \left (
      \CM_{\Pred, \Pred \mid \I, k}
    \right ) \CM_{\Pred, \Pred \mid \I}^{-1}
  \shortintertext{
    Using the original objective \cref{eq:obj_gp} to
    compute the change in variance from selecting \( k \),
  }
  \label{eq:global_obj}
  \argmin_k \left (
    \CM_{\Pred, \Pred \mid \I} -
      \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
  \right ) \CM_{\Pred, \Pred \mid \I}^{-1} &=
    \argmax_k \left (
      \frac{\CM_{\Pred, k \mid \I}^2}{\CM_{k, k \mid \I}}
    \right ) \CM_{\Pred, \Pred \mid \I}^{-1}
\end{align}
where the new objective \cref{eq:global_obj} is easily computed as
the original objective \cref{eq:obj_gp} divided by the target's
conditional variance, the percentage the decrease in variance takes up.

\subsubsection{Aggregated selection}
\label{subsubsec:mult_column}

The multiple-target algorithm already computes the exact
difference in log determinant after selecting a candidate.
The partial selection algorithm computes the log determinant itself, not
the difference, so the log determinant before the selection needs to be
subtracted, easily computed as the sum of the squared ``diagonal'' entries
of \( L \) corresponding to target points from \cref{eq:partial_diag}.

One heuristical improvement is to measure ``bang for the buck'',
i.e. to consider that the number of nonzeros added from selecting
a candidate varies depending on the number of targets conditioned.
The KL divergence adds a candidate's effect on each target together,
so candidates conditioning more targets tend to decrease the KL
divergence more, even if they are less efficient per target.
In practice, it is often better to divide the change
in KL divergence by the number of targets conditioned.

For the multiple-target algorithm, this modification makes no difference
within a group since every candidate conditions every target point, but can
make a difference in the global setting if groups have different sizes.
This modification can make a difference both within a column and
globally for the partial algorithm since different candidates
within the same group can condition a different number of targets.

\section{Numerical experiments}
\label{sec:experiments}

All experiments ran on the Partnership for an Advanced Computing Environment
(PACE) Phoenix cluster at the Georgia Institute of Technology, with 8
cores of a Intel Xeon Gold 6226 CPU @ 2.70GHz and 22 GB of RAM per core.
The code is written in Python using standard scientific libraries
\texttt{numpy} \cite{harris2020array}, \texttt{scipy} \cite{virtanen2020scipy},
\texttt{scikit-learn} \cite{pedregosa2011scikitlearn}, \texttt{matplotlib}
\cite{hunter2007matplotlib} as well as Cython \cite{behnel2011cython} which
provides direct transpilation of Python code into C.
Cython also allows Python code to access native C interfaces
to the \texttt{BLAS} and Intel \texttt{oneMKL} libraries.
Code for all numerical experiments can be found at
\href{https://github.com/stephen-huan/conditional-knn}
{https://github.com/stephen-huan/conditional-knn}.

\subsection{\textit{k}-nearest neighbors selection}
\label{subsec:knn_exp}

We directly compare \( k \)-nearest neighbors (\( k \)-NN) to our selection
algorithm, which we will call conditional \( k \)-nearest neighbors (C\( k
\)-NN), by comparing classification accuracy in the following toy example.
From the MNIST database of handwritten digits
\cite{lecun1998gradientbased}, we randomly select 1000 images
to form the training set and 100 to form the testing set.
For each image in the testing set, we select the \( k \) ``closest''
training images to it using either selection method and make a prediction
by the mode (most frequently occurring) label in the selected images.
For \( k \)-NN, we use the standard Euclidean distance and
for C\( k \)-NN, we use a Mat{\'e}rn kernel with smoothness
\( \nu = \frac{3}{2} \) and length scale \( \ell = 2^{10} \).
Accuracy is measured by the number (or
percentage) of test images classified correctly.

As seen in \cref{fig:mnist}, the accuracy of both
methods degrades linearly with increasing \( k \).
We hypothesize that nearby images are more likely
to have the same label as a given test image.
Forcing the algorithm to select more points therefore increases the likelihood
that the algorithm becomes confused by far away, differently labeled images.
However, C\( k \)-NN is more accurate than \( k \)-NN
for every \( k > 2 \), suggesting that conditional
selection consistently selects more informative points.
We emphasize that the difference in accuracy results solely from conditioning:
because the Mat{\'e}rn kernel degrades monotonically with distance,
sorting by unconditional covariance is identical to sorting by distance.
In addition, we use the mode to summarize the labels of the selected
points, rather than performing Gaussian process classification.
The difference in accuracy results exactly
from the methods selecting different points.

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed},
    {very thick, orange,    style=solid},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/mnist/accuracy_k.tex}%
  \input{figures/mnist/time_k.tex}
  \caption{
    We compare selection by Euclidean distance (\( k \)-NN) to conditional
    selection using a Mat{\'e}rn kernel (C\( k \)-NN) on image classification.
    From the MNIST database, \( N = 1000 \) training images and \( m = 100
    \) testing images are randomly chosen; each test image is classified by
    taking the mode label of the \( k \) selected training images, and this
    process is repeated 100 times for each value of \( k \).
    On the left, C\( k \)-NN is more accurate
    than \( k \)-NN for every \( k > 2 \).
    On the right, the time to select \( k \) points using C\( k \)-NN
    seems to scale linearly with \( k \) although its asymptotic
    complexity is quadratic; this is a result of performing highly
    optimized BLAS operations on relatively small matrices.
  }
  \label{fig:mnist}
\end{figure}

\subsection{Recovery of sparse Cholesky factors}
\label{subsec:recover_exp}

Motivated by the similarity of the selection algorithm to orthogonal
matching pursuit \cite{tropp2007signal}, we experiment with its sparse
recovery properties by attempting to recover a randomly generated
\textit{a priori} sparse Cholesky factor \( L \) solely from the inner
products \( \CM = L L^{\top} \).
For each column of \( L \), we uniformly randomly pick \( s \)
entries from those that satisfy lower triangularity to make nonzero.
We sample their values i.i.d. from the standard normal \( \N(0, 1) \).
Finally, we fill \( L \)'s diagonal with a ``large'' positive value
(10) to ensure both the resulting covariance matrix \( \CM \) and
precision matrix \( \CM^{-1} \) are reasonably well-conditioned.
The various selection algorithms are given the target number of nonzeros per
column \( s \) and either the covariance matrix \( \CM \) or the precision
matrix \( \CM^{-1} \) depending on which results in higher accuracy, and are
asked to reconstruct the sparse Cholesky factor \( L \).
Accuracy is measured by taking the cardinality of the intersection of
the recovered sparsity set with the ground truth sparsity set over the
cardinality of their union, which we call intersection over union (IOU).
Measuring accuracy by the KL divergence between \( \Theta \) and \(
(\hat{L} \hat{L}^{\top})^{-1} \) where \( \hat{L} \) is the optimal
inverse Cholesky factor following the recovered sparsity as computed
by \cref{eq:L_col} was found to be mostly equivalent to IOU.

As seen in \cref{fig:recover_acc}, C\( k \)-NN retains its near-perfect
recovery accuracy while the rest of the methods quickly degrade and
asymptote to their final accuracies with an increasing number of rows
and columns of \( L \) for a fixed \( s = 32 \) nonzeros per column.
However, if the number of columns is fixed at \( N = 256 \) and \( s
\) is increased instead, all methods drop in accuracy with increasing
density until a tipping point where the problem starts to become easier.
Accuracy then increases until the Cholesky factor
becomes fully dense, when perfect recovery is trivial.
The C\( k \)-NN strategy follows this pattern, but maintains
much higher accuracy than the rest of the strategies.

In the setting of noisy measurements, noise sampled i.i.d from \(
\N(0, \var) \) is added to each entry of \( \CM \) symmetrically
(i.e. \( \CM_{ij} \) receives the same noise as \( \CM_{ji} \)).
Accuracy slightly degrades with increasing noise for all
methods, but C\( k \)-NN seems to be the most sensitive
to noise as can be seen in \cref{fig:recover_noise}.
At high levels of noise \( \CM \) can lose positive-definiteness,
which causes C\( k \)-NN to break down entirely.

An important setting where \( \CM \) (and possibly \( L \)) are known
to be \textit{a priori} sparse is when solving Laplacian systems.
In exploratory numerical experiments we found that the factors
generated by our method were unreasonably dense compared the sparsity
of the Laplacian matrix, a manifestation of \emph{fill-in}.
We comment that our method is designed for \emph{dense} kernel matrices
resulting from a set of points and a ``well-behaved'' kernel function.
For solving sparse Laplacian systems, a method designed
to exploit the sparsity and graph structure of Laplacian
matrices like \cite{kyng2016approximate} is more appropriate.

\pgfplotsset{
  cycle list={
    {very thick, silver,    style=densely dotted},
    {very thick, lightblue, style=dashed},
    {very thick, seagreen,  style=dashdotted},
    {very thick, orange,    style=solid},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/recover/accuracy_n.tex}%
  \input{figures/recover/accuracy_s.tex}
  \caption{
    We attempt to recover sparse Cholesky factors \( L \)
    from their covariance matrix \( \Theta = L L^{\top} \).
    ``C\( k \)-NN'' minimizes the conditional variance of the target (diagonal)
    entry, ``\( k \)-NN'' maximizes covariance with the target entry,
    ``corr.'' maximizes correlation with the target \cref{eq:obj_gp} without
    conditioning, and ``rand.'' randomly samples entries uniformly.
    All methods achieve the highest accuracy when given
    the covariance matrix \( \CM \) except for C\( k \)-NN
    which is given the precision matrix \( \CM^{-1} \).
    The left panel shows accuracy with increasing size of \( L \) with a
    fixed number of nonzeros per column \( s = 32 \) while the right panel
    shows accuracy with increasing \( s \) and fixed size \( N = 256 \).
  }
  \label{fig:recover_acc}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/recover/accuracy_noise.tex}%
  \input{figures/recover/accuracy_noise_cknn.tex}
  \caption{
    We experiment with adding noise sampled i.i.d.
    from \( \N(0, \var) \) to \( \Theta \) entrywise.
    On the left is accuracy with increasing noise for \( N
    = 1024 \) columns and \( s = 32 \) nonzeros per column.
    On the right is accuracy for the C\( k \)-NN method at various
    noise levels for \( N = 256 \) and increasing \( s \).
  }
  \label{fig:recover_noise}
\end{figure}

\subsection{Cholesky factorization}
\label{subsec:chol_exp}

We empirically verify that greedy selection minimizes KL divergence in
Cholesky factorization compared to selection by Euclidean distance.
We sample \( N \) points uniformly randomly from the unit hypercube in \(
D = 3 \) dimensions and use a Mat{\'e}rn kernel with smoothness \( \nu =
\frac{5}{2} \) and length scale \( \ell = 1 \), implicitly determining the
covariance matrix \( \Theta \).
As a baseline for comparison, we use both the single-column and aggregated
variants of the KL-minimization framework of \cite{schafer2021sparse}, which
uses the reverse-maximin ordering and selects sparsity entries by selecting
points within a radius that decreases earlier in the ordering.
The density of the factor is tuned by the hyperparameter \( \rho \), increasing
\( \rho \) increases the radius and therefore the number of nonzeros.

We specifically use the framework of \cite{schafer2021sparse} which selects all
points within a radius of \( \rho \ell_i \) to the \( i \)th point, where \(
\rho \) is a tuning parameter for density and \( \ell_i \) is a length scale
systematically decreasing with decreasing position in the ordering.

We compare with a simpler \( k \)-nearest neighbors approach where
\( k \) is chosen to match the density of the baseline factor.
For our method, we run the baseline \cite{schafer2021sparse} with a larger
\( \rho' = \rho \cdot \rho_s \) to get an initial candidate set where \(
\rho_s \) is a hyperparameter controlling the number of candidates considered.
If not stated otherwise, \( \rho_s = 2 \) is used in the following experiments.
We then use both the single-column and aggregated variants of the
conditional selection algorithm described in \cref{sec:chol_select}
to subsample the actual sparsity entries from this candidate set,
in these experiments, each column gets the same number of nonzeros.
In exploratory numerical experiments, we found using the global
selection procedure in \cref{subsec:global_greedy} to determine
the number of nonzeros for each column led to little improvement
in accuracy at a significant performance penalty.
We use the same aggregation procedure for conditional selection as the
baseline aggregated factor and in all experiments we use an aggregation
parameter of \(\lambda = 1.5 \) as recommended by \cite{schafer2021sparse}.

As \cref{fig:chol_n} shows, the KL divergence and running time
increases linearly with the number of points for all methods.
Conditional methods are more accurate their unconditional counterparts
and aggregated variants are both more accurate and faster than their
non-aggregated counterparts; however the aggregated factors are denser,
which may impact their computational efficiency in downstream tasks.

As \cref{fig:chol_rho} shows, the conditional selection methods achieve
significantly better KL divergence for the same number of sparsity entries.
Part of that improvement is from spreading the number of nonzeros
evenly over the columns as shown by the performance of \( k \)-NN.
The conditional methods achieve better accuracy per computational cost than
the baseline, but the simple method of \( k \)-NN remains hard to beat.

Finally, we experiment with increasing the size of the candidate set \(
\rho_s \) but \emph{without} increasing the density of the factor \( \rho \).
Since the conditional selection methods scale linearly with the number of
candidates, increasing \( \rho_s \) is much cheaper than increasing \( \rho \).
As shown in \cref{fig:chol_s}, increasing \( \rho_s \)
has diminishing returns past \( \rho_s \approx 4 \).

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed,         mark=*},
    {very thick, seagreen,  style=dashdotted,     mark=o},
    {very thick, silver,    style=densely dotted, mark=triangle*},
    {very thick, orange,    style=solid,          mark=square*},
    {very thick, rust,      style=dotted,         mark=square},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/n_kl_div.tex}%
  \input{figures/cholesky/n_time.tex}
  \caption{
    Performance of various Cholesky factorization
    methods with increasing number of points.
    ``KL'': baseline from \cite{schafer2021sparse}, ``(agg.)'' denotes
    aggregated version, ``select (\( k \)-NN)'' is selection by \(
    k \)-nearest neighbors, ``select'' is conditional selection.
    The density is fixed at \( \rho = 2 \).
    The left panel shows KL divergence and the right
    panel shows time with an increasing number of points.
  }
  \label{fig:chol_n}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/rho_kl_div.tex}%
  \input{figures/cholesky/rho_time.tex}
  \caption{
    The left panel shows the KL divergence with increasing
    density \( \rho \) and the right panel shows the time to
    accuracy trade-off over different values of \( \rho \).
    The number of points is \( N = 65536 \).
  }
  \label{fig:chol_rho}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cholesky/s_kl_div.tex}%
  \input{figures/cholesky/s_time.tex}
  \caption{
    We increase the number of candidates considered, \(
    \rho_s \), with the density of the factor fixed at \(
    \rho = 4 \) and the number of points at \( N = 65536 \).
    The left panel shows KL divergence with an increasing number of candidates
    considered, the right shows the corresponding increases in time.
  }
  \label{fig:chol_s}
\end{figure}

\subsection{Gaussian process regression}
\label{gp_exp}

For Gaussian process regression we use the ``predictions points first'' method
of \cite{schafer2021sparse} which seeks to compute a sparse Cholesky factor
of the joint covariance matrix between the training and prediction points.
In this Cholesky factor, prediction points are placed before training points
in the reverse ordering (afterwards in the forward ordering), conditioning
them in the factorization from the statistical perspective of \cref{eq:chol}.
For a covariance matrix blocked as
\( \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\)
and a Cholesky factor of its precision
\( L =
    \begin{pmatrix}
      L_{1, 1} & 0 \\
      L_{2, 1} & L_{2, 2}
    \end{pmatrix}
\),
\( L L^{\top} = \CM^{-1} \), then the posterior distribution can be read off as
\( \E[\vec{y}_1 \mid \vec{y}_2] = -L_{1, 1}^{-\top} L_{2, 1}^{\top} \vec{y}_2
\) and \( \Cov[\vec{y}_1 \mid \vec{y}_2] = L_{1, 1}^{-\top} L_{1, 1} \).
These quantities can be computed efficiently since
they involve only the sparse submatrices of \( L \).

In order to empirically test the performance of the method
we sample 65536 points uniformly randomly from the unit
hypercube in \( D = 3 \) dimensions and split these points into a
train-test-split of 90\% training points and 10\% testing points.
We use a Mat{\'e}rn kernel with smoothness \( \nu =
\frac{5}{2} \) and length scale \( \ell = 1 \) and draw
1000 realizations from the resulting Gaussian process.
We consider three accuracy metrics for the Gaussian process regression
problem: the log determinant of the testing points' posterior covariance
matrix, the empirical 90\% coverage computed from the posterior mean and
variance averaged over all realizations, and the root mean square error
(RMSE) of the prediction averaged over all realizations.
The log determinant is equivalent to the KL divergence by the
discussion in \cref{subsubsec:aggregated}, so the results are
nearly identical to the ones shown in \cref{fig:chol_rho}.
We find that for our particular geometry and kernel function coverage is
extremely accurate for all methods (within \( 0.1\% \) for \( \rho \geq 2 \)).
Finally, the RMSE is shown in \cref{fig:gp_rho}.
Unlike the KL divergence, aggregated variants are indistinguishable
in accuracy from their non-aggregated counterparts.
Despite an increased cost to select points, the conditional methods have
better accuracy per unit computational cost than their unconditional
counterparts as a result of their superior accuracy at the same sparsity.

\pgfplotsset{
  cycle list={
    {very thick, lightblue, style=dashed,         mark=*},
    {very thick, seagreen,  style=dashdotted,     mark=o},
    {very thick, orange,    style=solid,          mark=square*},
    {very thick, rust,      style=dotted,         mark=square},
  }
}

\begin{figure}[t]
  \centering
  \input{figures/gp/rho_loss.tex}%
  \input{figures/gp/rho_time_loss.tex}
  \caption{
    We perform Gaussian process regression by sparse
    Cholesky factorization of the joint covariance matrix.
    Like Cholesky factorization in \cref{subsec:chol_exp},
    we use a candidate size scaling factor of \( \rho_s = 2
    \) and an aggregation parameter of \( \lambda = 1.5 \).
    The left panel shows the difference in RMSE from
    exact Gaussian process regression using the same
    training points with increasing density \( \rho \).
    The right panel shows the time to accuracy
    trade-off over different values of \( \rho \).
  }
  \label{fig:gp_rho}
\end{figure}

\subsection{Preconditioning for conjugate gradient}
\label{subsec:cg_exp}

Motivated by the equivalence of functionals like the Kaporin condition number
to the KL divergence, we investigate solving symmetric positive-definite
systems \( \CM \vec{x} = \vec{y} \) using the conjugate gradient and a sparse
Cholesky factor \( L \) as a preconditioner.
We note that from \cref{eq:kl} the KL divergence strongly
penalizes zero eigenvalues of the preconditioned matrix
\( \CM L L^{\top} \), improving its condition number.
In order to generate the covariance matrix \( \CM \) we sample up to \( N
= 32768 \) points from the unit hypercube in \( D = 3 \) dimensions with a
Mat{\'e}rn kernel with smoothness \( \nu = \frac{1}{2} \) and length scale
\( \ell = 1 \).
In exploratory numerical experiments, we found higher smoothnesses
like \( \nu = \frac{5}{2} \) led to high numerical instability and
extremely poor condition (thousands of iterations to converge).
Increasing length scale also worsens the
condition but to a less extreme extent.
Rather than generate a right hand side \( \vec{y} \) directly, we first
sample a solution \( \vec{x} \sim \N(\vec{0}, \Id_N) \) and then compute
\( \vec{y} = \CM \vec{x} \) so \( \vec{y} \) is realistically smooth.
When computing the preconditioner \( L \) by sparse Cholesky factorization,
unless stated otherwise, we use a candidate size scaling factor of \(
\rho_s = 2 \) and an aggregation parameter of \( \lambda = 1.5 \).
We then run conjugate gradient iterations with \( L \) as a
preconditioner until a relative tolerance of \( 10^{-12} \) is reached.

As shown in \cref{fig:cg_iter}, the conditional methods converge
in half the iterations of their unconditional counterparts.
Aggregation seems not to reduce the number of iterations,
but does make each iteration slightly slower since denser
aggregated factors result in slower matrix-vector products.
The minimum number of nonzeros per column for the
conjugate gradient to converge within 50 iterations
seems to grow logarithmically with the number of points.
Although this logarithmic growth likely holds for the conditional
methods, it appears near constant due to the slow growth.

We observe a characteristic ``U'' shape for the total wall-clock
time (the total time to form the preconditioner and for
the conjugate gradient to converge) in \cref{fig:cg_rho}.
If the preconditioner is too sparse, the conjugate gradient dominates
the running time, and if the preconditioner is too dense, it can be
expensive to form without significantly reducing the number of iterations.
We note there always exists a situation where more accurate methods for the
same sparsity have better wall-clock time by simply increasing the number of
iterations, e.g. by demanding better tolerance or using matrices with worse
condition (e.g. Mat{\'e}rn kernels with higher smoothness or length scale).
The only way to have better wall-clock time in \emph{every} situation is to be
both more accurate and faster, which is not achieved by conditional selection,
so it is hard to compare the conditional methods to the unconditional methods.
We do observe that aggregated variants perform worse than their non-aggregated
counterparts due to barely reducing the number of iterations while resulting
in significantly denser factors, leading to slower matrix-vector products.
The optimal density for conditional methods is sparser than their
unconditional counterparts due to less iterations at the same
sparsity and selecting nonzeros being more expensive to compute.

\begin{figure}[t]
  \centering
  \input{figures/cg/n_iter-res.tex}%
  \input{figures/cg/nnz_nnz.tex}
  \caption{
    We use the conjugate gradient preconditioned with
    sparse Cholesky factors to solve the symmetric
    positive-definite system \( \CM \vec{x} = \vec{y} \).
    The left panel shows iteration progress for \( N =
    32768 \) points and a factor density of \( \rho = 4 \).
    The right panel shows the minimum number of nonzeros
    per column for conjugate gradient to converge within
    50 iterations with an increasing number of points.
  }
  \label{fig:cg_iter}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figures/cg/rho_iter.tex}%
  \input{figures/cg/rho_time.tex}
  \caption{
    The left panel shows how iterations of the conjugate gradient decrease with
    increasing preconditioner density \( \rho \) for \( N = 32768 \) points.
    The right panel shows the corresponding total wall-clock time (both to
    compute the preconditioner and to converge) with increasing density.
  }
  \label{fig:cg_rho}
\end{figure}

\subsection{Comparison to other methods}
\label{subsec:compare}

In this section we compare our selection algorithm and
resulting Cholesky factorization procedure to existing work.

\paragraph{Local approximate Gaussian process (laGP)}

Our conditional variance objective for point selection \cref{eq:obj_gp} was
first described in \cite{cohn1996neural} for optimal experimental design,
which has subsequently been named the active learning Cohn (ALC) objective.
\cite{gramacy2014local} and the follow up work \cite{gramacy2015speeding}
apply the ALC objective to directed Gaussian process regression, yielding
an algorithm equivalent to ours in the case of a single point of interest.
We note that their definition of conditional variance in equation 5 is
analogous to our equation \cref{eq:cond_cov}, equating their conditional
variance objective in equation 8 to our equation \cref{eq:obj_gp} (the
connection is explicitly stated in SM\S4's equation SM.22).
We also note that they update the precision after a new
point is selected through the blocked equations in SM\S1
like our \cref{app:prec_insert} and \cref{alg:select_prec}.

For inference at multiple points of interest, \cite{gramacy2014local}
seems to only consider direct parallelization of the
single-target algorithm to each of the target points.
They mention that the \texttt{laGP} function in their \texttt{R}
package jointly considers target points if multiple input values
are provided, although without providing details on this procedure.
We generalize the ALC objective to multiple prediction points through
the log determinant of the covariance matrix \cref{eq:greedy_mult}
and provide an explicit algorithm \cref{alg:select_mult_chol}.
In addition, integration of the selection algorithm into
Cholesky factorization provides a global decomposition
of the joint training and prediction covariance matrix.

\paragraph{Orthogonal matching pursuit (OMP)}

Our selection algorithm can be viewed as the covariance equivalent
of the sparse signal recovery algorithm orthogonal matching
pursuit (OMP) \cite{tropp2007signal, tropp2006algorithms}.
OMP measures the approximation of a target signal by its residual after
orthogonal projection onto the subspace of chosen training signals,
which is efficiently calculated by maintaining a QR factorization.
In contrast, our selection algorithm uses a kernel function to evaluate inner
products, so orthogonalization becomes conditioning, the residual norm
becomes variance, and the QR factorization becomes a Cholesky factorization.
A major difference from OMP is that the computational time of conditioning
dominates that of evaluating the kernel function since the feature space
is relatively low-dimensional (often 2 or 3 in spatial statistics).

\paragraph{Sparse Cholesky factorization}

Our proposed sparse Cholesky factorization algorithm is most similar
to \cite{schafer2021sparse} and \cite{kang2021correlationbased}.
We note using \( k \)-nearest neighbors to select the sparsity set
instead of our proposed conditional selection algorithm essentially
reduces to \cite{schafer2021sparse} and that using the correlation
objective \cref{eq:obj_gp} but without conditioning on previously
selected points reduces to \cite{kang2021correlationbased}.

\section{Conclusions}
\label{sec:conclusion}

We have shown that accounting for conditioning while selecting sparsity
entries for sparse Cholesky factors significantly improves accuracy and
performance in downstream tasks compared to selection by Euclidean distance.
Not only is conditional selection computationally efficient,
it can be extended to the settings of multiple-target and
partial conditioning at no additional asymptotic time cost.
In addition, conditional selection gives a principled
way of deciding the number of nonzeros per column of the
Cholesky factor by greedy selection over all columns.
We support these claims by extensive numerical
experimentation for a variety of problems.

\section*{Acknowledgments}
\todo{add more funding information}
This research was supported in part through research cyberinfrastructure
resources and services provided by the Partnership for an Advanced Computing
Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia,
USA.

\bibliographystyle{siamplain}
\bibliography{references}

\newpage

\appendix

\todo{add proofs, if any, in appendix}

\section{Derivations in KL-minimization}

\subsection{Reduction for optimal factor}
\label{app:kl_L}

We wish to compute the KL divergence between \( \CM \) and the optimal
sparse approximate Cholesky factor \( L \) computed from \cref{eq:L_col}.
From \cref{eq:kl} and letting \( \Delta = 2
\KL*{\N(\vec{0}, \CM)}{\N(\vec{0}, (L L^{\top})^{-1})} \),
\begin{align}
  \label{eq:kl_L}
  \Delta &= \trace(L L^{\top} \CM) - \logdet(L L^{\top}) - \logdet(\CM) - N
  \shortintertext{
    Focusing on the term \( \trace(L L^{\top} \CM) = \trace(L^{\top} \CM L)
    \) by the cyclic property of trace and using the sparsity of \( L \) by
    plugging in the definition \cref{eq:L_col} for each column \( L_{s_i, i}
    \),
  }
  \trace(L^{\top} \CM L) &= \sum_{i = 1}^N
    L_{s_i, i}^{\top} \CM_{s_i, s_i} L_{s_i, i} \\
  &= \sum_{i = 1}^N
    \left (
      \frac{\left ( \CM_{s_i, s_i}^{-1} \vec{e}_1 \right )^{\top}}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right )
    \CM_{s_i, s_i}
    \left (
      \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
        {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}}
    \right ) \\
  &= \sum_{i = 1}^N
    \frac{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1}
          \CM_{s_i, s_i} \CM_{s_i, s_i}^{-1}
          \vec{e}_1}{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}
  = \sum_{i = 1}^N 1 = N
  \shortintertext{
    exactly the Jacobi scaling constraint \( \diag(L^{\top} \CM L) = 1 \).
    Substituting back into \cref{eq:kl_L},
  }
  \Delta &= -\logdet(L L^{\top}) - \logdet(\CM)
  \shortintertext{
    Computing the determinant of a triangular matrix as the
    product of its diagonal entries and plugging in the
    definition \cref{eq:L_col} for the diagonal entries,
  }
  &= -2 \sum_{i = 1}^N \left [ \log(L_{i, i}) \right ] - \logdet(\CM) \\
  &= -\sum_{i = 1}^N
    \left [ \log(\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1) \right ]
    - \logdet(\CM) \\
  &= \sum_{i = 1}^N
    \left [
      \log
      \left (
        (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1}
      \right )
    \right ]
    - \logdet(\CM)
  \shortintertext{
    Applying \cref{eq:L_cond_var} to turn each term \(
    (\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1)^{-1} \)
    into a variance \( \CM_{i, i \mid s_i \setminus \{ i \}} \),
  }
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
    \right ]
    - \logdet(\CM) \\
  \shortintertext{
    Expanding the log determinant by the chain rule \cref{eq:det_chain},
  }
  &= \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right ) -
    \sum_{i = 1}^N
      \log \left ( \CM_{i, i \mid i + 1:} \right ) \\
  &= \sum_{i = 1}^N
    \left [
      \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right ) -
      \log \left ( \CM_{i, i \mid i + 1:} \right )
    \right ]
\end{align}
We can interpret this sum as the accumulated \emph{difference} in error for
a series of independent prediction problems: each to predict the \( i \)th
variable given a subset of the variables after it in the ordering, \( i + 1,
i + 2, \dotsc, N \).
The left term \( \log \left ( \CM_{i, i \mid s_i \setminus \{ i \}} \right )
\) is restricted to the variables in the sparsity pattern \( s_i \), while the
right term \( \log \left ( \CM_{i, i \mid i + 1:} \right ) \) is unconstrained.
As a result, the left term will necessarily be greater than the
right, and the goal is to minimize the accumulated deviation.
Thus, the KL divergence measures the maintenance of predictive accuracy after
forcing conditional independence (sparsity) on the original distribution.
This independent regression problem interpretation of the KL
divergence is also given in equation 5 of \cite{katzfuss2022scalable}.

The asymmetry of KL divergence implies the order of the matrices matters as
well as whether both matrices have been inverted or not. This seems to imply
that there are four possible ways to compare two covariance matrices. However,
note that
\begin{align}
  \KL*{\N(\vec{0}, \CM_1)}{\N(\vec{0}, \CM_2)} &=
    \KL*{\N(\vec{0}, \CM_2^{-1})}{\N(\vec{0}, \CM_1^{-1})}
\end{align}
from \cref{eq:kl} and the cyclic property of trace, so inverting both
matrices implicitly reverses the order of the KL divergence.
There are therefore only two possible ways to
compare the two matrices, depending on their order.
From a statistical perspective the KL divergence can interpreted as the
likelihood-ratio test, so the asymmetry in order corresponds to the asymmetry
between the null and alternative hypotheses \cite{eguchi2006interpreting}.

\subsection{Partial updates in the selection algorithm}
\label{app:partial}

We have the collection of Gaussian random variables \( \vec{y} = [y_1, \dotsc,
y_m]^{\top} \) corresponding to the ordered column indices \( i_1 \succ
\dotsb \succ i_m \) in the context of aggregated Cholesky factorization.
We then select an index \( k \) that conditions the variables, but ignores
the first \( p \) variables (the indices are sorted in decreasing order,
so if \( k \) conditions a variable, it conditions everything afterwards).
After this partial conditioning we denote the updated variables as
\( \vec{y}_{\parallel k} \defeq y_1, \dotsc, y_p, y_{p + 1 \mid k},
\dotsc, y_{m \mid k} \) and wish to compute the covariance matrix \(
\Cov[\vec{y}_{\parallel k}] \).
We know the original variables \( \vec{y} \) have joint density multivariate
Gaussian according to some covariance matrix \( \CM \), \( \vec{y} \sim
\N(\vec{0}, \CM) \), and the fully conditional variables \( \vec{y}_{\mid k}
\) have closed-form posterior distribution according to \cref{eq:cond_cov},
\( \vec{y}_{\mid k} \sim \N(\vec{\mean}, \CM - \CM_{:, k} \CM_{k, k}^{-1}
\CM_{k, :}) \) for some posterior mean \( \vec{\mean} \).
For unconditioned \( y_i \) and \( y_j \),
their covariance is simply \( \CM_{i, j} \).
Similarly, for conditioned \( y_{i \mid k} \) and \( y_{j
\mid k} \), their covariance is \( \CM_{i, j \mid k} \).
The only unknown is the covariance between unconditioned
\( y_i \) and conditioned \( y_{j \mid k} \).
Taking the Cholesky factorization of both covariance matrices,
let \( L = \chol(\CM) \) and \( L' = \chol(\CM_{:, : \mid k}) \).
We can then view \( \vec{y} \) as \( L \vec{z} \), where \( \vec{z}
\) is distributed according to standard normal, \( \N(\vec{0}, \Id) \).
Similarly, \( \vec{y}_{\mid k} = L' \vec{z} + \vec{\mean} \).
By definition,
\begin{align}
  \Cov[y_i, y_{j \mid k}] &=
    \E[(y_i - \E[y_i])(y_{j \mid k} - \E[y_{j \mid k}])] \\
  &= \E[(L_i \vec{z}) ({L'}_j \vec{z} + \mu_j - \mu_j)] \\
  &= \E[(L_{i, 1} z_1 + \dotsb + L_{i, m} z_m)
        ({L'}_{j, 1} z_1 + \dotsb + {L'}_{j, m} z_m)]
  \shortintertext{
    For \( i \neq j \), \( \E[z_i z_j] = \E[z_i] \E[z_j] = 0 \)
    since \( z_i \) is independent of \( z_j \) and has mean 0.
  }
  &= \E[L_{i, 1} {L'}_{j, 1} z_1^2 + \dotsb +
        L_{i, m} {L'}_{j, m} z_m^2] \\
  &= L_{i, 1} {L'}_{j, 1} \E[z_1^2] + \dotsb +
     L_{i, m} {L'}_{j, m} \E[z_m^2]
  \shortintertext{
    For any \( i \), \( \E[z_i^2] = \Var[z_i] + \E[z_i]^2 = 1 + 0 = 1 \).
  }
  &= L_{i, 1} {L'}_{j, 1} + \dotsb + L_{i, m} {L'}_{j, m} \\
  &= L_i \cdot {L'}_j
  \shortintertext{Thus, the new covariance matrix can be written as}
  \Cov[\vec{y}_{\parallel k}] &=
  \begin{pmatrix}
    L_{:p} L_{:p}^{\top} &
    L_{:p} {L'}_{p + 1:}^{\top} \\
    {L'}_{p + 1:} L_{:p}^{\top} &
    {L'}_{p + 1:} {L'}_{p + 1:}^{\top}
  \end{pmatrix} =
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}
  \begin{pmatrix}
    L_{:p} \\
    {L'}_{p + 1:}
  \end{pmatrix}^{\top}
\end{align}

\subsection{Aggregated computation of sparsity entries}
\label{app:L_mult}

We wish to compute the entries of the sparse Cholesky factor
\( L_{s_i, i} \) according to \cref{eq:L_col} in the aggregated
sparsity setting of \cref{subsubsec:aggregated}.
Recall that we have aggregated the column indices \( i_1 \succ
\dotsb \succ i_m \) into \( \tilde{i} = \{ i_1, \dotsc, i_m \} \)
and \( s_{\tilde{i}} \) denotes the aggregated sparsity pattern.
The sparsity pattern for the \( i \)th column of the group is all
those entries that satisfy lower triangularity, \( s_i \defeq \{
j \in s_{\tilde{i}} : j \succeq i \} \subseteq s_{\tilde{i}} \).
Because each column's sparsity pattern is a subset of the
overall sparsity pattern, it is possible to compute an outer
approximation and specialize to each column efficiently.
Assuming the sparsity entries \( s_{\tilde{i}} \) are sorted according to
\( \prec \), let \( Q \defeq \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) be
the precision of the aggregated sparsity pattern and let \( k \) be the \(
i \)th column's index in \( s_{\tilde{i}} \).
Because \( s_{\tilde{i}} \) is sorted according to \(
\prec \), the sparsity pattern for the \( i \)th column
\( s_i \) is exactly the entries \( k \) and after.
\begin{align}
  L_{s_i, i} &= \frac{\CM_{s_i, s_i}^{-1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} \CM_{s_i, s_i}^{-1} \vec{e}_1}} \\
  \shortintertext{
    Since \( Q^{-1} = \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) and \(
    s_i \) is the \( k \)th index of \( s_{\tilde{i}} \) and after,
  }
  &= \frac{\left (Q^{-1} \right )_{k:, k:}^{-1} \vec{e}_1}
    {\sqrt{\vec{e}_1^{\top} \left (Q^{-1} \right )_{k:, k:} \vec{e}_1}} \\
  \shortintertext{
    By \cref{eq:inverse_cond}, we can turn marginalization
    in precision into conditioning in covariance.
  }
  \label{eq:L_precision}
             &= \frac{Q_{k:, k: \mid :k - 1} \vec{e}_1}
                     {\sqrt{\vec{e}_1^{\top} Q_{k:, k: \mid :k - 1} \vec{e}_1}}
\end{align}
So we want the \( k \)th column of \( Q
\), conditional on all columns before it.
From \cref{eq:chol}, this can be directly read off the \(
k \)th column of the Cholesky factor \( L = \chol(Q) \) to
compute \cref{eq:L_col} for each \( i \in \tilde{i} \).
However, computing \( Q = \CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} \) by
inverting \( \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) and then additionally
computing its Cholesky factor \( L = \chol(Q) \) is a bit wasteful.

Instead of computing a \emph{lower} triangular factor for the precision,
we can compute an \emph{upper} triangular factor for the covariance whose
inverse transpose will be a lower triangular factor for the precision.
Let \( U = P^{\Reverse} \chol(P^{\Reverse} \CM_{s_{\tilde{i}},
s_{\tilde{i}}} P^{\Reverse}) P^{\Reverse} \) where \( P^{\Reverse} \) is the
order-reversing permutation; \( U \) is upper triangular and satisfies \( U
U^{\top} = \CM_{s_{\tilde{i}}, s_{\tilde{i}}} \) so \( U^{-\top} U^{-1} =
\CM_{s_{\tilde{i}}, s_{\tilde{i}}}^{-1} = Q \) and we see that \( L = U^{-\top}
\) is a lower triangular factor satisfying \( L L^{\top} = Q \).
Thus we can compute \( U \) as a Cholesky factor of the covariance and
dynamically form \( L \) by solving the triangular system \( L_{:, k}
= U^{-T} \vec{e}_k \) for the desired column \( k \) of \( L \) (like
\cref{eq:obj_chol}, \( \vec{e}_k \) is the vector with \( k \)th entry
one and rest zero).

\section{Computation in sparse Gaussian process selection}

\subsection{Mutual information objective}
\label{app:mutual_info}

The mutual information or information gain between two collections of
random variables \( \vec{y}_\Pred \) and \( \vec{y}_\Train \) is defined as
\begin{align}
  \label{eq:info}
  \MI[\vec{y}_\Pred;\vec{y}_\Train] &= \entropy[\vec{y}_\Pred] -
    \entropy[\vec{y}_\Pred \mid \vec{y}_\Train]
\end{align}
Maximizing the mutual information is equivalent to minimizing the
conditional entropy since the entropy of \( \vec{y}_\Pred \) is constant.
Because the differential entropy of a multivariate Gaussian is monotonically
increasing with the log determinant of its covariance matrix, minimizing the
conditional entropy is equivalent to minimizing the log determinant of the
posterior covariance matrix.
For a single predictive point, its log determinant is simply its variance.
Thus, maximizing mutual information minimizes the
\emph{conditional variance} of the target point.
In particular, because our estimator is the conditional
expectation \cref{eq:cond_mean}, it is unbiased because \(
\E[\E[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[\vec{y}_\Pred] \).
Because it is unbiased, its expected mean squared error is simply the
conditional variance since \( \E[(\vec{y}_\Pred - \E[\vec{y}_\Pred
\mid \vec{y}_\Train])^2 \mid \vec{y}_\Train] = \Var[\vec{y}_\Pred
\mid \vec{y}_\Train] \) where the outer expectation is taken under
conditioning because of the assumption that \( \vec{y}_\Pred \) is
distributed according to the Gaussian process.
So maximizing the mutual information is equivalent to minimizing
the conditional variance which is in turn equivalent to
minimizing the expected mean squared error of the prediction.
Another perspective arises from comparing the definition of mutual
information \cref{eq:info} to the EV-VE identity \cref{eq:eve},
\begin{align}
  \label{eq:info_eve}
  \textcolor{darkorange}{\entropy[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\entropy[\vec{y}_\Pred \mid \vec{y}_\Train]} +
    \textcolor{rust}{\MI[\vec{y}_\Pred;\vec{y}_\Train]} \\
  \label{eq:eve}
  \textcolor{darkorange}{\Var[\vec{y}_\Pred]} &=
    \textcolor{lightblue}{\E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]} +
    \textcolor{rust}{     \Var[\E[\vec{y}_\Pred \mid \vec{y}_\Train]]}
\end{align}
On the left hand side, entropy is monotone with variance.
On the right hand side, the expectation of conditional
variance is monotone with conditional entropy.
Because the sum of the two terms on the right hand side is constant,
minimizing the expectation of conditional variance is equivalent to
maximizing the variance of conditional expectation, which corresponds
to the mutual information.

Supposing \( \vec{y}_\Pred \) was independent of \( \vec{y}_\Train
\), then the predictor \( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)
reduces to the constant \( \E[\vec{y}_\Pred] \) whose variance is 0.
Meanwhile, the error \( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]]
\) becomes \( \E[\Var[\vec{y}_\Pred]] = \Var[\vec{y}_\Pred] \).
On the other extreme, supposing \( \vec{y}_\Pred \) was a deterministic
function of \( \vec{y}_\Train \), then \( \Var[\E[\vec{y}_\Pred \mid
\vec{y}_\Train]] = \Var[\vec{y}_\Pred] \) while the error term becomes
\( \E[\Var[\vec{y}_\Pred \mid \vec{y}_\Train]] = \E[0] = 0 \).
Thus, the variance of conditional expectation is the information shared
between \( \vec{y}_\Pred \) and \( \vec{y}_\Train \), as it increases
when the predictor for \( \vec{y}_\Pred \) (the conditional expectation
\( \E[\vec{y}_\Pred \mid \vec{y}_\Train] \)) depends on the observed
results of \( \vec{y}_\Train \).

\subsection{Updating precision after insertion}
\label{app:prec_insert}

Assuming we have the precision matrix \( \CM_{1, 1}^{-1} \), we wish
to compute the precision of the covariance \( \CM_{1, 1} \) with a
new row and column added to it, that is, compute \( \CM^{-1} \) for
\(
  \CM =
    \begin{pmatrix}
      \CM_{1, 1} & \CM_{1, 2} \\
      \CM_{2, 1} & \CM_{2, 2}
    \end{pmatrix}
\)
where \( \CM_{1, 2} = \CM_{2, 1}^{\top} \) is a
column vector and \( \CM_{2, 2} \) is a scalar.
Using the same block \( L D L^{\top} \) factorization as \cref{eq:chol}
where the Schur complement is denoted \textcolor{lightblue}{\(
\CM_{2, 2 \mid 1} \)} from \cref{eq:cond_cov_notation},
\begin{align}
  \begin{pmatrix}
    \CM_{1, 1} & \CM_{1, 2} \\
    \CM_{2, 1} & \CM_{2, 2}
  \end{pmatrix} &=
  \begin{pmatrix}
    \Id & 0 \\
    \textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & \textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \shortintertext{Inverting both sides of the equation,}
  \CM^{-1} &=
  \begin{pmatrix}
    \Id & -\textcolor{darkorange}{\CM_{1, 1}^{-1} \CM_{1, 2}} \\
    0 & \Id
  \end{pmatrix}
  \begin{pmatrix}
    \CM_{1, 1}^{-1} & 0 \\
    0 & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \begin{pmatrix}
    \Id & 0 \\
    -\textcolor{darkorange}{\CM_{2, 1} \CM_{1, 1}^{-1}} & \Id
  \end{pmatrix} \\
  &=
  \begin{pmatrix}
    \CM_{1, 1}^{-1} +
    \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
    \textcolor{darkorange}{
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } &
    - \textcolor{darkorange}{
      \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right )
    } \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \\
    - \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}} \textcolor{darkorange} {
      \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
    } & \textcolor{lightblue}{\CM_{2, 2 \mid 1}^{-1}}
  \end{pmatrix}
  \shortintertext{In the context of adding a new entry \( k \) to the matrix,
    \( \CM_{1, 1} = \CM_{\I, \I} \), \( \CM_{1, 2} = \CM_{\I, k} \), and \(
    \CM_{2, 2} = \CM_{k, k} \). Also note that \( \textcolor{lightblue}{\CM_{k,
    k \mid \I}^{-1}} \) is the precision of \( k \) conditional on the entries
    in \( \I \), which has already been computed in \cref{alg:select_prec}. If
    we let \( \vec{v} = \textcolor{darkorange}{\CM_{\I, \I}^{-1} \CM_{\I, k}}
    \), then}
  &=
  \begin{pmatrix}
    \CM_{\I, \I}^{-1} + \CM_{k, k \mid \I}^{-1} \vec{v} \vec{v}^T &
    -\CM_{k, k \mid \I}^{-1} \vec{v} \\
    -\CM_{k, k \mid \I}^{-1} \vec{v}^{\top} & \CM_{k, k \mid \I}^{-1} \\
  \end{pmatrix}
\end{align}
which is precisely the update in line 13 of \cref{alg:select_prec}.
Note that the bulk of the update is a rank-one update to \( \CM_{1,
1}^{-1} \), which can be computed in \( \BigO(\card{\I}) = \BigO(s^2) \).

\subsection{Updating precision after conditioning}
\label{app:prec_cond}

Assuming we have the precision matrix of the prediction points conditional
on the selected entries, \( \CM_{\Pred, \Pred \mid \I}^{-1} \), we want to
take into account selecting an index \( k \), or to compute \( \CM_{\Pred,
\Pred \mid \I, k}^{-1} \), which is a rank-one update to the covariance (but
not necessarily the precision) from \cref{eq:obj_gp_mult}.
We can directly apply the ShermanMorrisonWoodbury
formula which states that:
\begin{align}
  \CM_{1, 1 \mid 2}^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{Expanding the conditioning from \cref{eq:cond_cov},}
  \left (
    \CM_{1, 1} - \CM_{1, 2} \CM_{2, 2}^{-1} \CM_{2, 1}
  \right )^{-1} &= \CM_{1, 1}^{-1} +
    \left (\CM_{1, 1}^{-1} \CM_{1, 2} \right ) \CM_{2, 2 \mid 1}^{-1}
    \left (\CM_{2, 1} \CM_{1, 1}^{-1} \right )
  \shortintertext{
    For brevity of notation, letting \( \vec{u} = \CM_{1, 2} \) and \(
    \vec{v} = \CM_{1, 1}^{-1} \CM_{1, 2} = \CM_{1, 1}^{-1} \vec{u} \),
  }
  (\CM_{1, 1} - \CM_{2, 2}^{-1} \vec{u} \vec{u}^{\top})^{-1} &=
    \CM_{1, 1}^{-1} + \CM_{2, 2 \mid 1}^{-1} \vec{v} \vec{v}^{\top}
  \shortintertext{
    So we see that a rank-one update to \( \CM_{1, 1} \) then
    inverting is a rank-one update to \( \CM_{1, 1}^{-1} \).
    In our context, \( \CM_{1, 1} = \CM_{\Pred, \Pred \mid \I}, \vec{u}
    = \CM_{\Pred, k \mid I}, \CM_{2, 2} = \CM_{k, k \mid \I} \) so \(
    \CM_{2, 2 \mid 1}^{-1} = \CM_{k, k \mid \Pred, I}^{-1} \) (from the
    quotient rule \cref{eq:quotient_rule}).
    \( \vec{v} \) can be computed according to definition
    as \( \CM_{\Pred, \Pred \mid \I}^{-1} \vec{u} \).
    Thus, we can write the update as
  }
  \left ( \CM_{\Pred, \Pred \mid \I} -
    \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
         {\CM_{kk \mid \I}}
  \right )^{-1} &=
    \CM_{1, 1}^{-1} +
    \CM_{k, k \mid \Pred, \I}^{-1} \vec{v} \vec{v}^{\top}
\end{align}
which is the update in line 18 of \cref{alg:select_mult_prec}.
Since the update is a rank-one update, it can be
computed in \( \BigO(\card{\Pred}^2) = \BigO(m^2) \).

\subsection{Updating the log determinant after a rank-one downdate}
\label{app:logdet_downdate}

Assuming we already have the log determinant of the covariance matrix of the
prediction points conditional on the selected entries, \( \logdet(\CM_{\Pred,
\Pred \mid \I}) \), we wish to compute the log determinant after we add an
index \( k \) to \( \I \), that is, to compute \( \logdet(\CM_{\Pred, \Pred
\mid \I, k}) \).
From \cref{eq:cond_select}, selecting a new point
is a rank-one downdate on the covariance matrix.
\begin{align}
  \nonumber
  \logdet(\CM_{\Pred, \Pred \mid \I, k})
  &= \logdet \left ( \CM_{\Pred, \Pred \mid \I} -
       \frac{\CM_{\Pred, k \mid \I} \CM_{\Pred, k \mid \I}^{\top}}
            {\CM_{k, k \mid \I}}
     \right ) \\
  \shortintertext{Using the matrix determinant lemma,}
  &= \logdet(\CM_{\Pred, \Pred \mid \I}) +
     \log \left ( 1 -
       \frac{\CM_{\Pred, k \mid \I}^{\top} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{
    Focusing on the second term, we can turn
    the quadratic form into condtioning.
  }
  &= \logdet(\CM_{\Pred, \Pred \mid \I}) +
     \log \left (
       \frac{\CM_{k, k \mid \I} -
             \CM_{k, \Pred \mid \I} \CM_{\Pred, \Pred \mid \I}^{-1}
             \CM_{\Pred, k \mid \I}
            }{\CM_{k, k \mid \I}}
     \right )
  \shortintertext{
    By the quotient rule \cref{eq:quotient_rule}, we combine the conditioning.
  }
  &= \logdet(\CM_{\Pred, \Pred \mid \I}) +
     \log \left ( \frac{\CM_{k, k \mid \I, \Pred}}{\CM_{k, k \mid \I}} \right )
\end{align}

\newpage

\section{Algorithms}

\begin{figure}[th!]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Point selection by \\ explicit precision}
      \label{alg:select_prec}
      \input{figures/algorithms/select_prec.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Point selection by \\ Cholesky factorization}
      \label{alg:select_chol}
      \input{figures/algorithms/select_chol.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for single-target selection.}
  \label{fig:alg_select}
\end{figure}

\begin{algorithm}[H]
  \caption{Direct sparse Gaussian process regression by selection}
  \label{alg:infer_select}
  \input{figures/algorithms/infer_select.tex}
\end{algorithm}

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Multiple-target selection by explicit precision}
      \label{alg:select_mult_prec}
      \input{figures/algorithms/select_mult_prec.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Multiple-target selection by Cholesky factorization}
      \label{alg:select_mult_chol}
      \input{figures/algorithms/select_mult_chol.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Update Cholesky factor}
      \label{alg:chol_update}
      \input{figures/algorithms/chol_update.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for multiple-target selection.}
  \label{fig:alg_mult_select}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Partial point selection}
      \label{alg:select_partial}
      \input{figures/algorithms/select_partial.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Find \( j \)'s index in \( \Order \)}
      \label{alg:insert_index}
      \input{figures/algorithms/insert_index.tex}
    \end{algorithm}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth}
    \begin{algorithm}[H]
      \caption{Compute \( j \)'s objective}
      \label{alg:partial_score}
      \input{figures/algorithms/partial_score.tex}
    \end{algorithm}

    \begin{algorithm}[H]
      \caption{Insert \( k \) into \( L \)}
      \label{alg:chol_insert}
      \input{figures/algorithms/chol_insert.tex}
    \end{algorithm}
  \end{minipage}
  \caption{Algorithms for partial selection.}
  \label{fig:alg_partial_select}
\end{figure}

\end{document}
